{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">ChatGPT LangChain Example for Chatbot Q&A</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: PhD.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">`ChatGPT`, `LangChain` e `FAISS` — um trio transformador que simplifica a criação de chatbots.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui estudamos o artigo de [Ivan Campos](https://medium.com/sopmac-ai/chatgpt-langchain-example-for-chatbot-q-a-a8b6ef40bbb6) na qual ele demonstra como `ChatGPT`, `LangChain` e `FAISS` permitem que os desenvolvedores criem chatbots inteligentes e sensíveis ao contexto com facilidade notável."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `ChatGPT` é um modelo de linguagem avançado desenvolvido pela OpenAI que pode gerar texto semelhante ao humano com base em determinados `prompts`, permitindo aplicativos versáteis como conversação, resumo de texto e `resposta a perguntas`.\n",
    "\n",
    "Para fins de nossa demonstração, vamos nos concentrar no modelo `\"gpt-3.5-turbo\"` da `OpenAI`, pois atualmente tem a combinação certa de velocidade e preço para `chatbots`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain](https://python.langchain.com/en/latest/index.html) é uma biblioteca (disponível em `Python`, `JavaScript` ou `TypeScript`) que fornece um conjunto de ferramentas e utilitários para trabalhar com modelos de linguagem, `Embeddings` de texto e tarefas de processamento de texto. Ele simplifica tarefas como criar `chatbots`, lidar com a recuperação de documentos e executar operações de `resposta a perguntas`, combinando vários componentes, como `modelos de linguagem`, `armazenamento de vetores` e loaders de documentos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*0PP6dH9-X2sgKd1tyaEcJA.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Vamos nos concentrar na criação de um chatbot de Q&A com um subconjunto dos componentes (`veja os itens verdes acima`) disponíveis na crescente biblioteca LangChain.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FAISS](https://faiss.ai/) (`Facebook AI Similarity Search`) é uma [biblioteca de código aberto](https://github.com/facebookresearch/faiss) desenvolvida pela `Facebook AI Research`. Ele foi projetado para pesquisar itens similares ([vetores](https://medium.com/sopmac-ai/vector-databases-as-memory-for-your-ai-agents-986288530443)) com eficiência em grandes coleções de dados de Alta Dimensão. O `FAISS` fornece métodos para indexar e pesquisar vetores, tornando mais fácil e rápido encontrar os itens mais similares dentro de um conjunto de dados.\n",
    "\n",
    "É particularmente útil em tarefas como:\n",
    "\n",
    "* Sistemas de recomendação\n",
    "\n",
    "* Recuperação (retrieval) de informações\n",
    "\n",
    "* Agrupamento (Clustering) — onde encontrar itens similares é importante\n",
    "\n",
    "\n",
    "O `FAISS` é uma escolha sólida de armazenamento de vetores se você tiver um `chatbot` básico e for:\n",
    "\n",
    "* Consultando um conjunto de dados `limitado` que pode ser alimentado por uma CPU\n",
    "\n",
    "* Buscando uma solução de armazenamento de vetores gratuita e de código aberto\n",
    "\n",
    "* Não pretende introduzir outro servidor ou API de nuvem em sua arquitetura\n",
    "\n",
    "\n",
    "<font color=\"orange\">Para saber mais sobre o `conceito de vetores e bancos de dados vetoriais`, confira: [Vector Databases as Memory for your AI Agents](https://medium.com/sopmac-ai/vector-databases-as-memory-for-your-ai-agents-986288530443).</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Salvar OpenAI Embeddings como um índice FAISS</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*gr2rWjAWSogHAUl8lRCZUg.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código carrega dados de um arquivo `CSV`, divide-o em partes e, em seguida, cria e salva um `índice FAISS` com `OpenAI Embeddings`. Isso permite uma pesquisa de similaridade eficiente e a recuperação de informações relevantes do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isto é quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "#print('Carregando a minha chave Key: ', load_dotenv())\n",
    "load_dotenv()\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] \n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV from https://gist.github.com/IvanCampos/94576c9746be280cf5b64083c8ea5b4d\n",
    "#loader = CSVLoader(\"./fisica_eddy.csv\", csv_args = {\"delimiter\": ','})\n",
    "loader = CSVLoader(\"./fisica_eddy.csv\", csv_args = {\"delimiter\": ','})\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "faissIndex = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "faissIndex.save_local(\"faiss_fisica_eddy\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importar as bibliotecas necessárias, incluindo a biblioteca `dotenv` para gerenciar variáveis ​​de ambiente e outros módulos da biblioteca LangChain para lidar com o processamento de texto e criar um `índice FAISS`.\n",
    "\n",
    "2. Carregando o arquivo `.env` que contém o uso `OPENAI_API_KEY` da função dotenv.load_dotenv(). Se seu código for hospedado em um repositório Git, adicione o arquivo .env ao seu .gitignore.\n",
    "\n",
    "3. Criando uma instância `CSVLoader` para carregar um arquivo `CSV` chamado `midjourney-20230505.csv`. O `CSV` pode ser baixado [aqui](https://gist.github.com/IvanCampos/94576c9746be280cf5b64083c8ea5b4d).\n",
    "\n",
    "4. Carregando os documentos do arquivo CSV usando a função `loader.load()`.\n",
    "\n",
    "5. Criar uma instância `CharacterTextSplitter` para dividir os documentos em partes menores com <font color=\"yellow\">tamanho máximo de 1.000 caracteres cada</font> e sem sobreposição entre as partes.\n",
    "\n",
    "6. Dividindo os documentos em chunks usando a função `text_splitter.split_documents(documents)`.\n",
    "\n",
    "7. Criando um `índice FAISS` a partir dos chunks do documento, usando as representações vetoriais `OpenAIEmbeddings()` dos chunks de texto.\n",
    "\n",
    "8. Salvando o `índice FAISS` criado em um arquivo local chamado `\"faiss_midjourney_docs\"`. <font color=\"yellow\">O índice pode então ser reutilizado para tarefas de pesquisa de similaridade eficientes no futuro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Carregue um índice FAISS e comece a conversar com seus documentos</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*jk5MgeuZi8Mj_nc27WALlw.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código importa as bibliotecas necessárias e inicializa um `chatbot` usando `LangChain`, `FAISS` e `ChatGPT` por meio do modelo `GPT-3.5-turbo`. Ele carrega um `índice FAISS` pré-construído para pesquisa de documentos e configura uma `RetrievalQAcadeia`. Um modelo de `prompt` é definido para solicitar respostas sucintas. Por fim, o chatbot é executado com uma query sobre a versão mais recente (que se supõe ser Midjourney) e a resposta é impressa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ação e reação são iguais e opostas.\n"
     ]
    }
   ],
   "source": [
    "import os, dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chatbot = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                   temperature=0,\n",
    "                   model_name=\"gpt-3.5-turbo\",\n",
    "                   max_tokens=50\n",
    "                  ), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=FAISS.load_local(\"./faiss_fisica_eddy\", OpenAIEmbeddings())\n",
    "        .as_retriever(search_type=\"similarity\", search_kwargs={\"k\":1})\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "Responda da forma mais sucinta possível. No máximo com 10 palavras! {query}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "print(chatbot.run(\n",
    "    prompt.format(query=\"Qual é a terceira lei de Newton?\")\n",
    "))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importe as bibliotecas e módulos necessários, incluindo `os`, `dotenv`, `OpenAIEmbeddings`, `FAISS`, `ChatOpenAI`, `RetrievalQA` e `PromptTemplate`.\n",
    "\n",
    "2. Carregue a variável de ambiente (ou seja, `OPENAI_API_KEY`) de seu arquivo `.env` usando `dotenv`.\n",
    "\n",
    "3. Inicialize uma instância `ChatOpenAI` com o modelo `GPT-3.5-turbo`, uma `temperatura` de `0`, um `máximo de 50 tokens para respostas` e a chave de API OpenAI. A `temperatura padrão é 0.7` — definir o valor como `0` reduzirá a aleatoriedade das conclusões do ChatGPT.\n",
    "\n",
    "4. Carregue o `índice FAISS` pré-criado `“faiss_midjourney_docs”` usando `OpenAIEmbeddings`.\n",
    "\n",
    "5. Configure uma cadeia `RetrievalQA` com a instância `ChatOpenAI`, o índice FAISS e o tipo de pesquisa e os parâmetros. É altamente recomendável definir `search_type` e `search_kwargs` - <font color=\"yellow\">não fazer isso seria ineficiente em termos de custo</font>, pois todos os chunks em seu armazenamento de vetores seriam enviados para o LLM. Também vale a pena notar que o `chain_type` é [stuff](https://docs.langchain.com/docs/components/chains/index_related_chains) que tenta stuff todos os chunks no prompt como contexto para o seu LLM (ou seja, `ChatGPT`).\n",
    "\n",
    "6. Defina um modelo de prompt que inclua a variável `\"query\"` e solicite respostas sucintas.\n",
    "\n",
    "7. Crie uma instância `PromptTemplate` usando o modelo definido.\n",
    "\n",
    "8. Formate o prompt com uma query sobre algo relacionado ao `Midjourney`.\n",
    "\n",
    "9. Execute o `chatbot` com a pergunta de prompt formatada.\n",
    "\n",
    "10. Imprima a resposta do chatbot ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar `ChatGPT`, `LangChain` e `FAISS` oferece vários benefícios:\n",
    "\n",
    "* `Processo de desenvolvimento simplificado:` a combinação dessas tecnologias torna mais fácil para os desenvolvedores criar, manter e otimizar `chatbots`, economizando tempo e dinheiro e facilitando uma implantação mais rápida.\n",
    "\n",
    "* `Maior adaptabilidade:` ao combinar essas tecnologias, os `chatbots` podem ser mais facilmente adaptados e estendidos para atender a novos domínios, idiomas e casos de uso, aumentando sua versatilidade e valor.\n",
    "\n",
    "* `Tratamento avançado de consultas:` ao aproveitar os pontos fortes do `ChatGPT`, `LangChain` e `FAISS`, os `chatbots` podem entender e lidar melhor com consultas complexas ou ambíguas, levando a respostas mais precisas, relevantes e satisfatórias para os usuários."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
