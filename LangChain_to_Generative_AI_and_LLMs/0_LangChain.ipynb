{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">LangChain 0: GPT-3 vs LLMs (c√≥digo aberto)</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste script seguiremos os tutoriais de [James Briggs](https://www.pinecone.io/learn/langchain-intro/) üòé."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LangChain` √© um Framework popular que permite aos usu√°rios criar rapidamente aplicativos e pipelines em torno de `Large Language Models` (LLMs). Ele se integra diretamente aos modelos `GPT-3` e `GPT-3.5` da `OpenAI` e √†s alternativas de c√≥digo aberto do `Hugging Face`, como os modelos `flan-t5 do Google`. Ele pode ser usado para `chatbots`, `perguntas-respostas generativas` (GQA), `resumos` e muito mais. A ideia central da biblioteca √© que podemos `\"encadear\"` (chain) diferentes componentes para criar casos de uso mais avan√ßados em torno de LLMs. As cadeias podem consistir em v√°rios componentes de v√°rios m√≥dulos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come√ßamos citando e descrevendo as quatro componentes b√°sicas:\n",
    "\n",
    "* `Prompt templates:` Os templates de prompt s√£o, bem, modelos para diferentes tipos de prompts. Como modelos de estilo `\"chatbot\"`, respostas a perguntas `ELI5`, etc.\n",
    "\n",
    "* `LLMs:` Modelos de Linguagem Grandes como `GPT-3`, `BLOOM`, etc.\n",
    "\n",
    "* `Agentes:` os agentes usam LLMs para decidir quais a√ß√µes devem ser executadas, ferramentas como pesquisa na Web ou calculadoras podem ser usadas e tudo empacotado em um loop l√≥gico de opera√ß√µes.\n",
    "\n",
    "* `Mem√≥ria:` mem√≥ria de curto prazo, mem√≥ria de longo prazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto √© quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] \n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando LLMs em LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">O `LangChain` oferece suporte a v√°rios provedores de LLM, como `Hugging Face` e `OpenAI`.\n",
    "\n",
    "Vamos come√ßar nossa explora√ß√£o do `LangChain` aprendendo como usar algumas dessas diferentes integra√ß√µes LLM.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Hugging Face</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU huggingface_hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ent√£o gerar texto usando um modelo HF Hub (usaremos `google/flan-t5-x1`) usando a `API de infer√™ncia` incorporada ao `Hugging Face Hub`.\n",
    "\n",
    "(A API de infer√™ncia padr√£o n√£o usa hardware especializado e, portanto, pode ser lenta e n√£o pode executar modelos maiores como `bigscience/bloom-560m` ou `google/flan-t5-xxl`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum mechanics\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "\n",
    "# Inicializar HF LLM\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xl\",\n",
    "    model_kwargs={\"temperature\":1e-10}\n",
    ")\n",
    "\n",
    "# Crie um modelo de prompt para responder a perguntas simples\n",
    "template = \"\"\"\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "question = \"O que √© f√≠sica qu√¢ntica?\"\n",
    "\n",
    "print(llm_chain.run(question))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Se quisermos fazer v√°rias perguntas, podemos passar uma lista de objetos de dicion√°rio, onde os dicion√°rios devem conter a vari√°vel de entrada definida em nosso modelo de `prompt` (`\"question\"`) que √© mapeada para a pergunta que gostar√≠amos de fazer.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='san francisco 49ers', generation_info=None)], [Generation(text='240', generation_info=None)], [Generation(text='jupiter', generation_info=None)], [Generation(text='2', generation_info=None)]], llm_output=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "qs = [\n",
    "    {'question': \"Qual time da NFL venceu o Super Bowl na temporada de 2010?\"},\n",
    "    {'question': \"Se eu tenho 6 p√©s e 4 polegadas, qual a minha altura em cent√≠metros?\"},\n",
    "    {'question': \"Quem foi a 12¬™ pessoa na lua?\"},\n",
    "    {'question': \"Quantos olhos tem uma l√¢mina de grama?\"}\n",
    "]\n",
    "res = llm_chain.generate(qs)\n",
    "res\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">√â um LLM, ent√£o podemos tentar alimentar todas as perguntas de uma s√≥ vez:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qual tempo da NFL venceu o Super Bowl na tempo\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Responda √†s seguintes perguntas, uma de cada vez.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "\n",
    "qs_str = (\n",
    "    \"Qual time da NFL venceu o Super Bowl na temporada de 2010?\\n\" +\n",
    "    \"Se eu tenho 6 p√©s e 4 polegadas, qual a minha altura em cent√≠metros?\\n\" +\n",
    "    \"Quem foi a 12¬™ pessoa na lua?\" +\n",
    "    \"Quantos olhos tem uma l√¢mina de grama?\"\n",
    ")\n",
    "\n",
    "\n",
    "print(llm_chain.run(qs_str))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Mas com este modelo n√£o funciona muito bem, veremos que esta abordagem funciona melhor com `modelos diferentes` em breve.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">OpenAI</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ent√£o, decidimos qual modelo gostar√≠amos de usar, existem v√°rias op√ß√µes, mas iremos com `text-davinci-003`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "davinci = OpenAI(model_name='text-davinci-003')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Como alternativa, se estiver usando o `Azure OpenAI`, fazemos:</font>\n",
    "\n",
    "```\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"your-azure-deployment\", \n",
    "    model_name=\"text-davinci-003\"\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "<font color=\"orange\">Usaremos o mesmo modelo de `prompt` de `pergunta-resposta simples` de antes com o exemplo de `Hugging Face`. A √∫nica mudan√ßa √© que agora passamos nosso `OpenAI LLM davinci`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Os New Orleans Saints venceram o Super Bowl na temporada de 2010.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo funciona novamente para `v√°rias perguntas` usando `generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' Os New Orleans Saints venceram o Super Bowl na temporada de 2010.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 193.04 cm', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' O astronauta Eugene Cernan foi a 12¬™ pessoa a pisar na Lua. Ele foi o √∫ltimo homem a pisar na Lua durante a miss√£o Apollo 17, em dezembro de 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Uma l√¢mina de grama n√£o tem olhos.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 192, 'completion_tokens': 91, 'prompt_tokens': 101}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Qual time da NFL venceu o Super Bowl na temporada de 2010?\"},\n",
    "    {'question': \"Se eu tenho 6 p√©s e 4 polegadas, qual a minha altura em cent√≠metros?\"},\n",
    "    {'question': \"Quem foi a 12¬™ pessoa na lua?\"},\n",
    "    {'question': \"Quantos olhos tem uma l√¢mina de grama?\"}\n",
    "]\n",
    "\n",
    "llm_chain.generate(qs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Observe que o `formato abaixo` n√£o alimenta as perguntas de forma `iterativa`, mas todas em um bloco.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. O New Orleans Saints foi o vencedor do Super Bowl na temporada de 2010.\n",
      "2. Sua altura em cent√≠metros √© 193.04.\n",
      "3. A 12¬™ pessoa na lua foi o astronauta Harrison Schmitt em 1972.\n",
      "4. Uma l√¢mina de grama n√£o tem olhos.\n"
     ]
    }
   ],
   "source": [
    "qs = [\n",
    "    \"Qual time da NFL venceu o Super Bowl na temporada de 2010?\",\n",
    "    \"Se eu tenho 6 p√©s e 4 polegadas, qual a minha altura em cent√≠metros?\",\n",
    "    \"Quem foi a 12¬™ pessoa na lua?\",\n",
    "    \"Quantos olhos tem uma l√¢mina de grama?\"\n",
    "     ]\n",
    "\n",
    "print(llm_chain.run(qs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Agora podemos tentar responder a todas as perguntas de uma s√≥ vez, como mencionado, `LLMs` mais poderosos como `text-davinci-003` ter√£o maior probabilidade de lidar com essas consultas mais complexas.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qual time da NFL venceu o Super Bowl na temporada de 2010? Os Green Bay Packers.\n",
      "\n",
      "Se eu tenho 6 p√©s e 4 polegadas, qual a minha altura em cent√≠metros? 193,04 cent√≠metros.\n",
      "\n",
      "Quem foi a 12¬™ pessoa na lua? O americano Harrison Schmitt.\n",
      "\n",
      "Quantos olhos tem uma l√¢mina de grama? Uma l√¢mina de grama n√£o tem olhos.\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Responda √†s seguintes perguntas, uma de cada vez.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "\n",
    "qs_str = (\n",
    "    \"Qual time da NFL venceu o Super Bowl na temporada de 2010?\\n\" +\n",
    "    \"Se eu tenho 6 p√©s e 4 polegadas, qual a minha altura em cent√≠metros?\\n\" +\n",
    "    \"Quem foi a 12¬™ pessoa na lua?\" +\n",
    "    \"Quantos olhos tem uma l√¢mina de grama?\"\n",
    ")\n",
    "\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
