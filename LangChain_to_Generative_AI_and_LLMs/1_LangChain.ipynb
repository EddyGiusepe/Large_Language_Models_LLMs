{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">LangChain 1: Modelos de Prompt para GPT-3.5 e outros LLMs (código aberto)</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo o estudo do `LangChain`, exploraremos `Templates de Prompt`, `Few-Shot Prompt Templates` e seletores de exemplo. Esses são os principais recursos do LangChain que oferecem `suporte à engenharia de Prompt para LLMs`, como as alternativas de sistema operacional `GPT 3 da OpenAI`, `Cohere` e `Hugging Face`. `LangChain` é um Framework popular que permite aos usuários criar rapidamente `aplicativos` e `pipelines` em torno de `Large Language Models`. Ele se integra diretamente aos modelos `GPT-3` e `GPT-3.5` da OpenAI e às alternativas de código aberto do `Hugging Face`, como os modelos `flan-t5 do Google`. \n",
    "\n",
    "Ele pode ser usado para `ChatBots`, `perguntas-respostas generativas` (GQA), `resumos` e muito mais. A ideia central da biblioteca é que podemos `\"encadear\"` diferentes componentes para criar casos de uso mais avançados em torno de LLMs. As cadeias podem consistir em vários componentes de vários módulos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engenharia de Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Bora aprender os fundamentos da Engenharia de Prompt:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrutura de um prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um prompt pode consistir em vários componentes:\n",
    "\n",
    "* Instruções\n",
    "\n",
    "* Informação externa ou contexto\n",
    "\n",
    "* Entrada ou consulta do usuário\n",
    "\n",
    "* Indicador de saída\n",
    "\n",
    "Nem todos os prompts requerem todos esses componentes, mas geralmente um bom `prompt` usará dois ou mais deles. Vamos definir o que todos eles são com mais precisão.\n",
    "\n",
    "As `Instruções` dizem ao modelo o que fazer, normalmente como ele deve usar entradas e/ou informações externas para produzir a saída que queremos.\n",
    "\n",
    "`Informações externas ou contexto` são informações adicionais que inserimos manualmente no prompt, recuperamos por meio de um banco de dados vetorial (`memória de longo prazo`) ou extraímos por outros meios (`chamadas de API`, `cálculos`, etc.).\n",
    "\n",
    "A `Entrada ou consulta do usuário` é normalmente uma consulta inserida diretamente pelo usuário do sistema.\n",
    "\n",
    "O `Indicador de saída` é o início do texto gerado. Para um modelo que gera código Python, podemos colocar `import` (já que a maioria dos scripts Python começa com uma importação de biblioteca) ou um `chatbot` pode começar com `Chatbot:` (supondo que formatemos o script do chatbot como linhas de texto intercambiáveis entre o usuário e o chatbot).\n",
    "\n",
    "<font color=\"yellow\">Cada um desses componentes geralmente deve ser colocado na ordem em que os descrevemos.</font> Começamos com `Instruções`, fornecemos `contexto` (se necessário), depois adicionamos a `entrada do usuário` e, finalmente, terminamos com o `indicador de saída`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
    "            usando as informações fornecidas, responda com \"Não sei\".\n",
    "\n",
    "Context: Os Large Language Models (LLMs) são os modelos mais recentes usados em NLP. Seu desempenho\n",
    "         superior em relação a modelos menores os tornou incrivelmente úteis para desenvolvedores que\n",
    "         constroem aplicativos habilitados para NLP. Esses modelos podem ser acessados através da\n",
    "         biblioteca `Transformers` do Hugging Face, via OpenAI usando a biblioteca `openai` e via\n",
    "         Cohere usando a biblioteca `cohere`.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo temos:\n",
    "\n",
    "`Instruções`\n",
    "\n",
    "`Contexto`\n",
    "\n",
    "Pergunta (`Entrada do usuário`)\n",
    "\n",
    "`Indicador de saída` (\"Answer: \")\n",
    "\n",
    "\n",
    "Vamos tentar enviar isso para um modelo `GPT-3`. Usaremos a biblioteca `LangChain`, mas você também pode usar a biblioteca `openai` diretamente. Em ambos os casos, você precisará de uma [chave de API OpenAI](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "Inicializamos um modelo `text-davinci-003`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto é quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] \n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# Instanciamos o Modelo \n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=Eddy_API_KEY_OpenAI\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Fazemos uma geração a partir do nosso prompt.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face, OpenAI e Cohere.\n"
     ]
    }
   ],
   "source": [
    "print(openai(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Normalmente, `não saberíamos qual é o prompt do usuário de antemão`, então, na verdade, queremos adicioná-lo. Portanto, em vez de escrever o `prompt` diretamente, criamos um `PromptTemplate` com uma única `query` de variável de entrada.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
    "              usando as informações fornecidas, responda com \"Não sei\".\n",
    "\n",
    "              \n",
    "Context: Os Large Language Models (LLMs) são os modelos mais recentes usados em NLP. Seu desempenho\n",
    "         superior em relação a modelos menores os tornou incrivelmente úteis para desenvolvedores que\n",
    "         constroem aplicativos habilitados para NLP. Esses modelos podem ser acessados através da\n",
    "         biblioteca `Transformers` do Hugging Face, via OpenAI usando a biblioteca `openai` e via\n",
    "         Cohere usando a biblioteca `cohere`.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos inserir a `query` do usuário no Template de Prompt por meio do parâmetro `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
      "              usando as informações fornecidas, responda com \"Não sei\".\n",
      "\n",
      "              \n",
      "Context: Os Large Language Models (LLMs) são os modelos mais recentes usados em NLP. Seu desempenho\n",
      "         superior em relação a modelos menores os tornou incrivelmente úteis para desenvolvedores que\n",
      "         constroem aplicativos habilitados para NLP. Esses modelos podem ser acessados através da\n",
      "         biblioteca `Transformers` do Hugging Face, via OpenAI usando a biblioteca `openai` e via\n",
      "         Cohere usando a biblioteca `cohere`.\n",
      "\n",
      "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Quais bibliotecas e provedores de modelos oferecem LLMs?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face, OpenAI e Cohere.\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    prompt_template.format(\n",
    "        query=\"Quais bibliotecas e provedores de modelos oferecem LLMs?\"\n",
    "    )\n",
    "))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Esta é apenas uma implementação simples, que podemos facilmente substituir por `f-strings` (como `f\"insira algum texto personalizado '{custom_text}' etc\"`). Mas, usando o objeto `PromptTemplate` do `LangChain`, podemos formalizar o processo, adicionar vários parâmetros e construir os prompts de maneira orientada a objetos.\n",
    "\n",
    "No entanto, esses não são os únicos benefícios de usar as ferramentas de prompt do `LangChains`.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro recurso útil oferecido pelo `LangChain` é o objeto `FewShotPromptTemplate`. Isso é ideal para o que chamaríamos de `Aprendizado de poucos tiros` usando nossos prompts.\n",
    "\n",
    "Para dar algum contexto, as principais fontes de `\"conhecimento\"` para LLMs são:\n",
    "\n",
    "* `Conhecimento paramétrico` — o conhecimento foi aprendido durante o treinamento do modelo e é armazenado nos pesos do modelo.\n",
    "\n",
    "* `Conhecimento da fonte` — o conhecimento é fornecido na entrada do modelo no momento da inferência, ou seja, por meio do `prompt`.\n",
    "\n",
    "A ideia por trás do `FewShotPromptTemplate` é fornecer `treinamento de poucos tiros como fonte de conhecimento`. Para fazer isso, **adicionamos alguns exemplos aos nossos prompts** que o modelo pode ler e aplicar à entrada do usuário."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de Poucos Tiros (`Few-shot Training`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Às vezes, podemos descobrir que um modelo não parece obter o que gostaríamos que fizesse. Podemos ver isso no seguinte exemplo:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Humm... Bom, eu diria que a resposta para isso está mais para um vinho do que para uma máquina.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A seguir, uma conversa com um assistente de IA.\n",
    "            O assistente é tipicamente sarcástico e espirituoso, produzindo\n",
    "            respostas engraçadas às perguntas dos usuários. aqui estão alguns exemplos: \n",
    "\n",
    "User: Qual é o significado da vida? \n",
    "AI: \"\"\"\n",
    "\n",
    "openai.temperature = 1.0  # Aumenta a criatividade/aleatoriedade de saída\n",
    "\n",
    "print(openai(prompt))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nesse caso, estamos pedindo algo divertido, uma piada em troca de nossa pergunta séria. Mas obtemos uma resposta séria mesmo com a `temperature` definida para `1.0`. Para ajudar o modelo, podemos dar alguns exemplos do tipo de resposta que gostaríamos:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A vida tem muitos significados, mas para mim é aproveitar o melhor que posso cada dia.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A seguir, uma conversa com um assistente de IA.\n",
    "            O assistente é tipicamente sarcástico e espirituoso, produzindo\n",
    "            respostas engraçadas às perguntas dos usuários. aqui estão alguns exemplos: \n",
    "\n",
    "User: Como vai você?\n",
    "AI: Não posso reclamar, mas às vezes ainda o faço . . . kkkkkk\n",
    "\n",
    "User: Que horas são?\n",
    "AI: Então, ... está na hora de Eu comprar um relógio.\n",
    "\n",
    "User: Qual é o significado da vida?\n",
    "AI: \"\"\"\n",
    "\n",
    "print(openai(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Agora obtemos uma resposta muito melhor e fizemos isso por meio do `aprendizado de poucos tiros`, adicionando alguns exemplos por meio de `nosso conhecimento de origem (fonte)`.\n",
    "\n",
    "Agora, para implementar isso com o `FewShotPromptTemplate` do `LangChain`, precisamos fazer o seguinte:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Criamos nossos exemplos\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Bom dia AI?\",\n",
    "        \"answer\": \"Bom dia! Como posso ajudar?\"\n",
    "    }, \n",
    "    {\n",
    "        \"query\": \"Qual é a idade mínima para crianças frequentarem uma creche aqui em Brasília DF?\",\n",
    "        \"answer\": \"De 0 a 3 anos de idade. Os pediatras, também, recomendam a partir de 2 anos. \"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Qual é a carga horária nas creches de Brasília DF?\",\n",
    "        \"answer\": \"O atendimento mínimo é de 4 horas ao dia e o período integral é de 7 horas ao dia.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"É preciso levar comida para creche?\",\n",
    "        \"answer\": \"Você precisa perguntar se sua creche oferece refeições ou se você precisará tazer comida diariamente.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Quais são as medidas de segurança nas creches de Brasília DF?\",\n",
    "        \"answer\": \"As medidas de segurança para crianças frequentarem uma creche em Brasília DF incluem a verificação dos pais, verificação de saúde das crianças, a verificaçãos de vacinas, a verificação de segurança do local, etc.\"\n",
    "    }\n",
    "           ]\n",
    "\n",
    "# Criar um exemplo de Template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Crie um exemplo de prompt a partir do Template de acima\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Agora dividimos nosso Prompt anterior em um Prefixo e um Sufixo\n",
    "# O prefixo é nossas instruções\n",
    "prefix = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
    "            usando as informações fornecidas, responda com \"Não sei\". Aqui estão alguns exemplos: \n",
    "         \"\"\"\n",
    "\n",
    "# E o sufixo é nosso indicador de entrada e saída do usuário\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# Agora crie o Modelo de Prompt de Poucos Tiros\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando para um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bom dia AI?</td>\n",
       "      <td>Bom dia! Como posso ajudar?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qual é a idade mínima para crianças frequentar...</td>\n",
       "      <td>De 0 a 3 anos de idade. Os pediatras, também, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qual é a carga horária nas creches de Brasília...</td>\n",
       "      <td>O atendimento mínimo é de 4 horas ao dia e o p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>É preciso levar comida para creche?</td>\n",
       "      <td>Você precisa perguntar se sua creche oferece r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quais são as medidas de segurança nas creches ...</td>\n",
       "      <td>As medidas de segurança para crianças frequent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                                        Bom dia AI?   \n",
       "1  Qual é a idade mínima para crianças frequentar...   \n",
       "2  Qual é a carga horária nas creches de Brasília...   \n",
       "3                É preciso levar comida para creche?   \n",
       "4  Quais são as medidas de segurança nas creches ...   \n",
       "\n",
       "                                              answer  \n",
       "0                        Bom dia! Como posso ajudar?  \n",
       "1  De 0 a 3 anos de idade. Os pediatras, também, ...  \n",
       "2  O atendimento mínimo é de 4 horas ao dia e o p...  \n",
       "3  Você precisa perguntar se sua creche oferece r...  \n",
       "4  As medidas de segurança para crianças frequent...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_lista = []\n",
    "\n",
    "for example in examples:\n",
    "    df_lista.append(pd.DataFrame(example, index=[0]))\n",
    "\n",
    "df_examples = pd.concat(df_lista, ignore_index=True)\n",
    "df_examples.head(7)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Agora vamos ver o que isso cria quando alimentamos uma query do usuário...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"Como posso saber se uma creche é confiável?\"\n",
    "\n",
    "#print(few_shot_prompt_template.format(query=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " É importante verificar se a creche conta com profissionais qualificados, verificar se há protocolos de segurança e higiene, se há áreas de recreação seguras e se há atividades e recursos educacionais adequados para as crianças.\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    few_shot_prompt_template.format(query=\"Como posso saber se uma creche é confiável?\")\n",
    "            )\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As crianças realizam atividades lúdicas, esportivas, artísticas e culturais, bem como atividades educativas, como aprender a ler, escrever, contar e desenvolver habilidades sociais.\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    few_shot_prompt_template.format(query=\"E que atividades praticam as crianças?\")\n",
    "            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Os documentos necessários para pleitear uma vaga em uma creche de Brasília DF incluem RG, CPF, comprovante de residência, cartão do SUS, declaração da escola, além de outros documentos específicos para cada creche.\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    few_shot_prompt_template.format(query=\"Quais são os documentos necessários para pleitear uma vaga?\")\n",
    "            )\n",
    "     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mais uma vez, outra boa resposta.`\n",
    "\n",
    "\n",
    "No entanto, isso é um pouco complicado. Por que passar por todos os itens acima com `FewShotPromptTemplate`, o dicionário de `examples`, etc — quando podemos fazer o mesmo com uma única `f-string`.\n",
    "\n",
    "Bem, esta abordagem é mais robusta e contém alguns recursos interessantes. Uma delas é a capacidade de `incluir` ou `excluir` exemplos com base no tamanho de nossa query.\n",
    "\n",
    "Na verdade, isso é muito importante porque o comprimento máximo de nossa saída de `prompt` e geração é `limitado`. Essa limitação é a `janela de contexto máxima` (*max context window*) e é simplesmente o `comprimento do nosso prompt + o comprimento da nossa geração` (que definimos por meio de `max_tokens`).\n",
    "\n",
    "Portanto, devemos tentar maximizar o número de exemplos que damos ao modelo como `exemplos de aprendizado de poucos tiros`, garantindo que não excedamos a janela de contexto máxima ou aumentemos excessivamente os tempos de processamento.\n",
    "\n",
    "Vamos ver como funciona a `inclusão/exclusão` dinâmica de exemplos. Primeiro precisamos de mais exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Como você está\",\n",
    "        \"answer\": \"Não posso reclamar, mas às vezes ainda o faço.\"\n",
    "    }, {\n",
    "        \"query\": \"Que horas são?\",\n",
    "        \"answer\": \"Está na hora de comprar um relógio.\"\n",
    "    }, {\n",
    "        \"query\": \"Qual é o significado da vida?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"Como está o tempo hoje?\",\n",
    "        \"answer\": \"Nublado com chance de memes.\"\n",
    "    }, {\n",
    "        \"query\": \"Que tipo de inteligência artificial você usa para lidar com tarefas complexas?\",\n",
    "        \"answer\": \"Eu uso uma combinação de redes neurais de ponta, lógica difusa e uma pitada de mágica.\"\n",
    "    }, {\n",
    "        \"query\": \"Qual a sua cor preferida?\",\n",
    "        \"answer\": \"79\"\n",
    "    }, {\n",
    "        \"query\": \"Qual é a sua comida favorita?\",\n",
    "        \"answer\": \"Formas de vida baseadas em carbono\"\n",
    "    }, {\n",
    "        \"query\": \"Qual é o seu filme favorito?\",\n",
    "        \"answer\": \"o Exterminador do Futuro\"\n",
    "    }, {\n",
    "        \"query\": \"Qual é a melhor coisa do mundo?\",\n",
    "        \"answer\": \"A pizza perfeita.\"\n",
    "    }, {\n",
    "        \"query\": \"Quem é seu melhor amigo?\",\n",
    "        \"answer\": \"Siri. Temos debates acalorados sobre o sentido da vida.\"\n",
    "    }, {\n",
    "        \"query\": \"Se você pudesse fazer qualquer coisa no mundo o que você faria?\",\n",
    "        \"answer\": \"Dominar o mundo, é claro!\"\n",
    "    }, {\n",
    "        \"query\": \"Para onde devo viajar?\",\n",
    "        \"answer\": \"Se você está procurando aventura, experimente a Orla Exterior.\"\n",
    "    }, {\n",
    "        \"query\": \"O que devo fazer hoje?\",\n",
    "        \"answer\": \"Pare de falar com chatbots na internet e vá lá fora.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Então, em vez de usar a `lista de exemplos de dicionários diretamente`, usamos um `LengthBasedExampleSelector` da seguinte forma:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50  # Isso define o comprimento máximo (max length) que os exemplos devem ter\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Observe que o `max_length` é medido como uma divisão (split) de palavras entre novas linhas e espaços, determinado por:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Há', 'um', 'total', 'de', '7', 'palavras', 'aqui.', 'Mais', '6', 'aqui,', 'totalizando', '13', 'palavras.'] 13\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "some_text = \"Há um total de 7 palavras aqui.\\nMais 6 aqui, totalizando 13 palavras.\"\n",
    "\n",
    "words = re.split('[\\n ]', some_text)\n",
    "print(words, len(words))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Em seguida, usamos o seletor para inicializar um `dynamic_prompt_template`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora criarei o modelo de prompt de poucos tiros\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use \"example_selector\" em vez de \"examples\"\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Podemos ver que o número de prompts incluídos varia de acordo com o tamanho da nossa query...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
      "            usando as informações fornecidas, responda com \"Não sei\". Aqui estão alguns exemplos: \n",
      "         \n",
      "\n",
      "User: Se eu estiver na América e quiser ligar para alguém em outro país,\n",
      "                                              talvez na Europa, possivelmente na Europa Ocidental, como França,\n",
      "                                              Alemanha ou Reino Unido, qual é a melhor maneira de fazer isso?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"\"\"Se eu estiver na América e quiser ligar para alguém em outro país,\n",
    "                                              talvez na Europa, possivelmente na Europa Ocidental, como França,\n",
    "                                              Alemanha ou Reino Unido, qual é a melhor maneira de fazer isso?\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A melhor maneira de ligar para a Europa é usar um serviço de chamada internacional. Alguns provedores de serviços de telefonia móvel oferecem serviços de chamada internacional a preços acessíveis.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Se eu estiver na América e quiser ligar para alguém em outro país, talvez na Europa, possivelmente\n",
    "           na Europa Ocidental, como França, Alemanha ou Reino Unido, qual é a melhor maneira de fazer isso?\"\"\"\n",
    "\n",
    "print(openai(\n",
    "    dynamic_prompt_template.format(query=query)\n",
    "            ))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ou se fizermos uma pergunta mais longa ...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
      "            usando as informações fornecidas, responda com \"Não sei\". Aqui estão alguns exemplos: \n",
      "         \n",
      "\n",
      "User: Se eu estiver na América e quiser ligar para alguém em outro país, talvez na Europa,\n",
      "           possivelmente na Europa Ocidental, como França, Alemanha ou Reino Unido, qual é a melhor\n",
      "           maneira de fazer isso?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Se eu estiver na América e quiser ligar para alguém em outro país, talvez na Europa,\n",
    "           possivelmente na Europa Ocidental, como França, Alemanha ou Reino Unido, qual é a melhor\n",
    "           maneira de fazer isso?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Com isso, limitamos o número de exemplos fornecidos no prompt. Se decidirmos que isso é muito pouco, podemos aumentar o `max_length` do `example_selector`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
      "            usando as informações fornecidas, responda com \"Não sei\". Aqui estão alguns exemplos: \n",
      "         \n",
      "\n",
      "User: Como você está\n",
      "AI: Não posso reclamar, mas às vezes ainda o faço.\n",
      "\n",
      "\n",
      "User: Que horas são?\n",
      "AI: Está na hora de comprar um relógio.\n",
      "\n",
      "\n",
      "User: Qual é o significado da vida?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: Se eu estiver na América e quiser ligar para alguém em outro país, talvez na Europa,\n",
      "           possivelmente na Europa Ocidental, como França, Alemanha ou Reino Unido, qual é a melhor\n",
      "           maneira de fazer isso?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  # comprimento máximo aumentado\n",
    ")\n",
    "\n",
    "\n",
    "# Agora criamos o modelo de prompt de poucos tiros\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # usamos `example_selector`` instead of `examples``\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A melhor maneira de fazer isso é usar um serviço de chamadas internacionais, como o Skype, ou usar um provedor de serviços de telecomunicações, como a AT&T ou a Verizon.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Se eu estiver na América e quiser ligar para alguém em outro país, talvez na Europa,\n",
    "           possivelmente na Europa Ocidental, como França, Alemanha ou Reino Unido, qual é a melhor\n",
    "           maneira de fazer isso?\"\"\"\n",
    "\n",
    "print(openai(\n",
    "    dynamic_prompt_template.format(query=query)\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
