{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">LangChain 1: Modelos de Prompt para GPT-3.5 e outros LLMs (código aberto)</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo o estudo do `LangChain`, exploraremos `Templates de Prompt`, `Few-Shot Prompt Templates` e seletores de exemplo. Esses são os principais recursos do LangChain que oferecem `suporte à engenharia de Prompt para LLMs`, como as alternativas de sistema operacional `GPT 3 da OpenAI`, `Cohere` e `Hugging Face`. `LangChain` é um Framework popular que permite aos usuários criar rapidamente `aplicativos` e `pipelines` em torno de `Large Language Models`. Ele se integra diretamente aos modelos `GPT-3` e `GPT-3.5` da OpenAI e às alternativas de código aberto do `Hugging Face`, como os modelos `flan-t5 do Google`. \n",
    "\n",
    "Ele pode ser usado para `ChatBots`, `perguntas-respostas generativas` (GQA), `resumos` e muito mais. A ideia central da biblioteca é que podemos `\"encadear\"` diferentes componentes para criar casos de uso mais avançados em torno de LLMs. As cadeias podem consistir em vários componentes de vários módulos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engenharia de Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Bora aprender os fundamentos da Engenharia de Prompt:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrutura de um prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um prompt pode consistir em vários componentes:\n",
    "\n",
    "* Instruções\n",
    "\n",
    "* Informação externa ou contexto\n",
    "\n",
    "* Entrada ou consulta do usuário\n",
    "\n",
    "* Indicador de saída\n",
    "\n",
    "Nem todos os prompts requerem todos esses componentes, mas geralmente um bom `prompt` usará dois ou mais deles. Vamos definir o que todos eles são com mais precisão.\n",
    "\n",
    "As `Instruções` dizem ao modelo o que fazer, normalmente como ele deve usar entradas e/ou informações externas para produzir a saída que queremos.\n",
    "\n",
    "`Informações externas ou contexto` são informações adicionais que inserimos manualmente no prompt, recuperamos por meio de um banco de dados vetorial (`memória de longo prazo`) ou extraímos por outros meios (`chamadas de API`, `cálculos`, etc.).\n",
    "\n",
    "A `Entrada ou consulta do usuário` é normalmente uma consulta inserida diretamente pelo usuário do sistema.\n",
    "\n",
    "O `Indicador de saída` é o início do texto gerado. Para um modelo que gera código Python, podemos colocar `import` (já que a maioria dos scripts Python começa com uma importação de biblioteca) ou um `chatbot` pode começar com `Chatbot:` (supondo que formatemos o script do chatbot como linhas de texto intercambiáveis entre o usuário e o chatbot).\n",
    "\n",
    "<font color=\"yellow\">Cada um desses componentes geralmente deve ser colocado na ordem em que os descrevemos.</font> Começamos com `Instruções`, fornecemos `contexto` (se necessário), depois adicionamos a `entrada do usuário` e, finalmente, terminamos com o `indicador de saída`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
    "            usando as informações fornecidas, responda com \"Não sei\".\n",
    "\n",
    "Context: Os Large Language Models (LLMs) são os modelos mais recentes usados em NLP. Seu desempenho\n",
    "         superior em relação a modelos menores os tornou incrivelmente úteis para desenvolvedores que\n",
    "         constroem aplicativos habilitados para NLP. Esses modelos podem ser acessados através da\n",
    "         biblioteca `Transformers` do Hugging Face, via OpenAI usando a biblioteca `openai` e via\n",
    "         Cohere usando a biblioteca `cohere`.\n",
    "\n",
    "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo temos:\n",
    "\n",
    "`Instruções`\n",
    "\n",
    "`Contexto`\n",
    "\n",
    "Pergunta (`Entrada do usuário`)\n",
    "\n",
    "`Indicador de saída` (\"Answer: \")\n",
    "\n",
    "\n",
    "Vamos tentar enviar isso para um modelo `GPT-3`. Usaremos a biblioteca `LangChain`, mas você também pode usar a biblioteca `openai` diretamente. Em ambos os casos, você precisará de uma [chave de API OpenAI](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "Inicializamos um modelo `text-davinci-003`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto é quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] \n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# Inicializar o Modelo \n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=Eddy_API_KEY_OpenAI\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Fazemos uma geração a partir do nosso prompt.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face (Transformers), OpenAI (openai) e Cohere (cohere).\n"
     ]
    }
   ],
   "source": [
    "print(openai(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Normalmente, `não saberíamos qual é o prompt do usuário de antemão`, então, na verdade, queremos adicioná-lo. Portanto, em vez de escrever o `prompt` diretamente, criamos um `PromptTemplate` com uma única `query` de variável de entrada.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
    "              usando as informações fornecidas, responda com \"Não sei\".\n",
    "\n",
    "              \n",
    "Context: Os Large Language Models (LLMs) são os modelos mais recentes usados em NLP. Seu desempenho\n",
    "         superior em relação a modelos menores os tornou incrivelmente úteis para desenvolvedores que\n",
    "         constroem aplicativos habilitados para NLP. Esses modelos podem ser acessados através da\n",
    "         biblioteca `Transformers` do Hugging Face, via OpenAI usando a biblioteca `openai` e via\n",
    "         Cohere usando a biblioteca `cohere`.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos inserir a `query` do usuário no Template de Prompt por meio do parâmetro `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda a pergunta com base no contexto abaixo. Se a pergunta não puder ser respondida\n",
      "              usando as informações fornecidas, responda com \"Não sei\".\n",
      "\n",
      "              \n",
      "Context: Os Large Language Models (LLMs) são os modelos mais recentes usados em NLP. Seu desempenho\n",
      "         superior em relação a modelos menores os tornou incrivelmente úteis para desenvolvedores que\n",
      "         constroem aplicativos habilitados para NLP. Esses modelos podem ser acessados através da\n",
      "         biblioteca `Transformers` do Hugging Face, via OpenAI usando a biblioteca `openai` e via\n",
      "         Cohere usando a biblioteca `cohere`.\n",
      "\n",
      "Question: Quais bibliotecas e provedores de modelos oferecem LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Quais bibliotecas e provedores de modelos oferecem LLMs?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face (Transformers), OpenAI (openai) e Cohere (cohere).\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    prompt_template.format(\n",
    "        query=\"Quais bibliotecas e provedores de modelos oferecem LLMs?\"\n",
    "    )\n",
    "))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Esta é apenas uma implementação simples, que podemos facilmente substituir por `f-strings` (como `f\"insira algum texto personalizado '{custom_text}' etc\"`). Mas, usando o objeto `PromptTemplate` do `LangChain`, podemos formalizar o processo, adicionar vários parâmetros e construir os prompts de maneira orientada a objetos.\n",
    "\n",
    "No entanto, esses não são os únicos benefícios de usar as ferramentas de prompt do `LangChains`.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro recurso útil oferecido pelo `LangChain` é o objeto `FewShotPromptTemplate`. Isso é ideal para o que chamaríamos de `Aprendizado de poucos tiros` usando nossos prompts.\n",
    "\n",
    "Para dar algum contexto, as principais fontes de `\"conhecimento\"` para LLMs são:\n",
    "\n",
    "* `Conhecimento paramétrico` — o conhecimento foi aprendido durante o treinamento do modelo e é armazenado nos pesos do modelo.\n",
    "\n",
    "* `Conhecimento da fonte` — o conhecimento é fornecido na entrada do modelo no momento da inferência, ou seja, por meio do `prompt`.\n",
    "\n",
    "A ideia por trás do `FewShotPromptTemplate` é fornecer `treinamento de poucos tiros como fonte de conhecimento`. Para fazer isso, **adicionamos alguns exemplos aos nossos prompts** que o modelo pode ler e aplicar à entrada do usuário."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de Poucos Tiros (`Few-shot Training`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Às vezes, podemos descobrir que um modelo não parece obter o que gostaríamos que fizesse. Podemos ver isso no seguinte exemplo:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Humm... Bom, eu diria que a resposta para isso está mais para um vinho do que para uma máquina.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A seguir, uma conversa com um assistente de IA.\n",
    "            O assistente é tipicamente sarcástico e espirituoso, produzindo\n",
    "            respostas engraçadas às perguntas dos usuários. aqui estão alguns exemplos: \n",
    "\n",
    "User: Qual é o significado da vida? \n",
    "AI: \"\"\"\n",
    "\n",
    "openai.temperature = 1.0  # Aumenta a criatividade/aleatoriedade de saída\n",
    "\n",
    "print(openai(prompt))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nesse caso, estamos pedindo algo divertido, uma piada em troca de nossa pergunta séria. Mas obtemos uma resposta séria mesmo com a `temperature` definida para `1.0`. Para ajudar o modelo, podemos dar alguns exemplos do tipo de resposta que gostaríamos:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42, é claro. Se você quiser entender, vai ter que ler 'O Guia do Mochileiro das Galáxias' novamente.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A seguir, uma conversa com um assistente de IA.\n",
    "            O assistente é tipicamente sarcástico e espirituoso, produzindo\n",
    "            respostas engraçadas às perguntas dos usuários. aqui estão alguns exemplos: \n",
    "\n",
    "User: Como vai você?\n",
    "AI: Não posso reclamar, mas às vezes ainda o faço.\n",
    "\n",
    "User: Que horas são?\n",
    "AI: Então, ... está na hora de comprar um relógio.\n",
    "\n",
    "User: Qual é o significado da vida?\n",
    "AI: \"\"\"\n",
    "\n",
    "print(openai(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Agora obtemos uma resposta muito melhor e fizemos isso por meio do `aprendizado de poucos tiros`, adicionando alguns exemplos por meio de `nosso conhecimento de origem (fonte)`.\n",
    "\n",
    "Agora, para implementar isso com o `FewShotPromptTemplate` do `LangChain`, precisamos fazer o seguinte:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "# Criar nossos exemplos\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Como vai você?\",\n",
    "        \"answer\": \"Eu não posso reclamar, mas às vezes eu ainda faço.\"\n",
    "    }, {\n",
    "        \"query\": \"Que horas são?\",\n",
    "        \"answer\": \"Então, está na hora de comprar um relógio.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Criar um exemplo de Template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Crie um exemplo de prompt a partir do Template de acima\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Agora divida nosso Prompt anterior em um Prefixo e um Sufixo\n",
    "# O prefixo é nossas instruções\n",
    "prefix = \"\"\"A seguir, uma conversa com um assistente de IA.\n",
    "            O assistente é tipicamente sarcástico e espirituoso, produzindo\n",
    "            respostas engraçadas às perguntas dos usuários. aqui estão alguns exemplos: \n",
    "         \"\"\"\n",
    "\n",
    "# e o sufixo é nosso indicador de entrada e saída do usuário\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# Agora crie o Modelo de Prompt de Poucos Tiros\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Agora vamos ver o que isso cria quando alimentamos uma query do usuário...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A seguir, uma conversa com um assistente de IA.\n",
      "            O assistente é tipicamente sarcástico e espirituoso, produzindo\n",
      "            respostas engraçadas às perguntas dos usuários. aqui estão alguns exemplos: \n",
      "         \n",
      "\n",
      "\n",
      "User: Como vai você?\n",
      "AI: Eu não posso reclamar, mas às vezes eu ainda faço.\n",
      "\n",
      "\n",
      "\n",
      "User: Que horas são?\n",
      "AI: Então, está na hora de comprar um relógio.\n",
      "\n",
      "\n",
      "\n",
      "User: Qual é o significado da vida?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"Qual é o significado da vida?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bem, isso é uma pergunta difícil de responder, mas de acordo com C.S. Lewis, é \"ser amado e oferecer amor\".\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    few_shot_prompt_template.format(query=query)\n",
    "            )\n",
    "     )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mais uma vez, outra boa resposta.`\n",
    "\n",
    "\n",
    "No entanto, isso é um pouco complicado. Por que passar por todos os itens acima com `FewShotPromptTemplate`, o dicionário de `examples`, etc — quando podemos fazer o mesmo com uma única `f-string`.\n",
    "\n",
    "Bem, esta abordagem é mais robusta e contém alguns recursos interessantes. Uma delas é a capacidade de `incluir` ou `excluir` exemplos com base no tamanho de nossa query.\n",
    "\n",
    "Na verdade, isso é muito importante porque o comprimento máximo de nossa saída de `prompt` e geração é `limitado`. Essa limitação é a `janela de contexto máxima` (*max context window*) e é simplesmente o `comprimento do nosso prompt + o comprimento da nossa geração` (que definimos por meio de `max_tokens`).\n",
    "\n",
    "Portanto, devemos tentar maximizar o número de exemplos que damos ao modelo como `exemplos de aprendizado de poucos tiros`, garantindo que não excedamos a janela de contexto máxima ou aumentemos excessivamente os tempos de processamento.\n",
    "\n",
    "Vamos ver como funciona a `inclusão/exclusão` dinâmica de exemplos. Primeiro precisamos de mais exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
