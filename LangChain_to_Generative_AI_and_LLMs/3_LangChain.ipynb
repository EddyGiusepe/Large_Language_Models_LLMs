{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">LangChain 3: Mem√≥ria ChatBot para ChatGPT, Davinci + outros LLMs</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste script seguimos aprimorando nossos conhecimentos com o Framework LangChain com o [James Briggs](https://www.pinecone.io/learn/langchain-conversational-memory/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `mem√≥ria conversacional` √© como um `chatbot` pode responder a v√°rias consultas de maneira semelhante a um bate-papo. Ele permite uma conversa coerente e, sem ele, cada consulta seria tratada como uma entrada totalmente independente, sem considerar as intera√ß√µes anteriores. A `mem√≥ria` permite que um Large Language Model (`LLM`) lembre-se de intera√ß√µes anteriores com o usu√°rio. <font color=\"orange\">Por padr√£o, os LLMs s√£o sem estado ‚Äî significando que cada consulta recebida √© processada independentemente de outras intera√ß√µes.</font> A √∫nica coisa que existe para um agente sem estado √© a entrada atual, nada mais. Existem muitos aplicativos em que lembrar as intera√ß√µes anteriores √© muito importante, como os `chatbots`. A `mem√≥ria conversacional` nos permite fazer isso. Existem v√°rias maneiras de implementar a mem√≥ria conversacional. No contexto do `LangChain`, todos eles s√£o constru√≠dos sobre o ``ConversationChain``."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mem√≥ria Conversacional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come√ßaremos importando todas as bibliotecas que usaremos neste exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
    "                                                  ConversationSummaryMemory, \n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto √© quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui instanciamos o Modelo que vamos usar:\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=Eddy_API_KEY_OpenAI,\n",
    "    model_name='text-davinci-003'  # Podemos usar o LLM como: 'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Posteriormente, faremos uso de uma fun√ß√£o utilit√°ria `count_tokens`. Isso nos permitir√° contar o n√∫mero de tokens que estamos usando para cada chamada. N√≥s o definimos assim:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Gastou um total de {cb.total_tokens} Tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Agora vamos mergulhar na `mem√≥ria conversacional`:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que √© mem√≥ria?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem√≥ria √© a capacidade de um agente de lembrar intera√ß√µes anteriores com o usu√°rio (`pense em chatbots`)\n",
    "\n",
    "A defini√ß√£o oficial de `mem√≥ria` √© a seguinte:\n",
    "\n",
    "<font color=\"orange\">Por padr√£o, `Chains` e `Agents` s√£o sem estado, o que significa que eles tratam cada consulta recebida de forma independente. Em algumas aplica√ß√µes (sendo os `chatbots` um √ìTIMO exemplo), √© muito importante lembrar as intera√ß√µes anteriores, tanto a curto prazo, quanto a longo prazo. O conceito de `‚ÄúMem√≥ria‚Äù` existe para fazer exatamente isso.</font>\n",
    "\n",
    "Como veremos, embora isso pare√ßa muito simples, existem v√°rias maneiras diferentes de implementar esse recurso de mem√≥ria.\n",
    "\n",
    "Antes de nos aprofundarmos nos diferentes m√≥dulos de mem√≥ria que a biblioteca oferece, vamos apresentar a cadeia que usaremos para esses exemplos: a `ConversationChain`.\n",
    "\n",
    "Como sempre, ao entender uma chain, √© interessante dar uma olhada em seu `prompt` primeiro e depois dar uma olhada em seu m√©todo `._call`. Como vimos no cap√≠tulo sobre `chains`, podemos verificar o `prompt` acessando o `template` dentro do atributo prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que se segue √© uma conversa amig√°vel entre um `humano` e uma `IA`. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
    "```\n",
    "Current conversaton:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\n",
    "```\n",
    "Interessante! Portanto, o prompt desta cadeia est√° dizendo para conversar com o usu√°rio e tentar dar respostas verdadeiras. Se olharmos de perto, h√° um novo componente no `prompt` que n√£o vimos quando est√°vamos mexendo no `LLMMathChain`: history. √â aqui que nossa mem√≥ria entrar√° em a√ß√£o.\n",
    "\n",
    "O que essa cadeia est√° fazendo com esse prompt? Vamos dar uma olhada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tudo junto:\n",
    "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation._call))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation.apply))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nada realmente m√°gico acontecendo aqui, apenas uma passagem direta por um `LLM`. Na verdade, essa cadeia herda esses m√©todos diretamente do `LLMChain` sem nenhuma modifica√ß√£o:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tudo junto:\n",
    "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LLMChain._call))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LLMChain.apply))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ent√£o, basicamente, essa cadeia combina uma `entrada do usu√°rio` com o `hist√≥rico da conversa` para gerar uma resposta significativa (e, com sorte, verdadeira).\n",
    "\n",
    "Agora que entendemos o b√°sico da cadeia que usaremos, podemos entrar na mem√≥ria. Vamos mergulhar!</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipos de Mem√≥ria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nesta se√ß√£o, revisaremos v√°rios tipos de mem√≥ria e analisaremos os `pr√≥s` e `contras` de cada um, para que voc√™ possa escolher o melhor para o seu caso de uso.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Tipo de mem√≥ria N¬∫ 1:</font> ConversationBufferMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `ConversationBufferMemory` faz exatamente o que seu nome sugere: ele mant√©m um buffer dos trechos da conversa anterior como parte do contexto no prompt.\n",
    "\n",
    "`Recurso principal:` a mem√≥ria do buffer de conversa√ß√£o mant√©m as partes anteriores da conversa completamente inalteradas, em sua forma bruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Passamos um `prompt` de `usu√°rio` para `ConversationBufferMemory` da seguinte forma:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Bom dia AI',\n",
       " 'history': 'Human: Bom dia AI\\nAI:  Bom dia! Como posso ajudar?\\nHuman: Bom dia AI\\nAI:  Bom dia novamente! O que posso fazer por voc√™?\\nHuman: Meu interesse aqui √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.\\nAI:  Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreens√£o de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Voc√™ pode me dar alguns exemplos de como isso pode ser feito?\\nHuman: Eu s√≥ quero analisar as diferentes possibilidades. O que voc√™ pode pensar?\\nAI:  Existem v√°rias maneiras de integrar Large Language Models com conhecimento externo. Uma maneira √© usar modelos de aprendizado profundo para incorporar conhecimento externo aos modelos de linguagem. Outra maneira √© usar modelos de mem√≥ria para armazenar e recuperar informa√ß√µes externas. Al√©m disso, tamb√©m √© poss√≠vel usar modelos de aten√ß√£o para direcionar a aten√ß√£o para informa√ß√µes externas relevantes.\\nHuman: Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\\nAI:  Existem v√°rias fontes de dados que podem ser usadas para fornecer contexto aos modelos de linguagem. Estas incluem bases de dados de conhecimento, como o Wikidata, fontes de dados estruturadas, como o Freebase, e fontes de dados n√£o estruturadas, como not√≠cias, documentos e conversas. Al√©m disso, tamb√©m √© poss√≠vel usar dados de aprendizado profundo, como imagens e v√≠deos, para fornecer contexto aos modelos.\\nHuman: Qual √© o meu objetivo novamente?\\nAI:  Seu objetivo √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.',\n",
       " 'response': '  Bom dia! Como posso ajudar?'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_buf(\"Bom dia AI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta chamada usou um total de \"n\" tokens, mas n√£o podemos ver isso acima. Se quisermos contar o n√∫mero de tokens sendo usados, basta passar nosso objeto da cadeia de conversa√ß√£o e a mensagem que gostar√≠amos de inserir por meio da fun√ß√£o `count_tokens` que definimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 105 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Bom dia novamente! O que posso fazer por voc√™?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf, \"Bom dia AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 235 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreens√£o de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Voc√™ pode me dar alguns exemplos de como isso pode ser feito?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Meu interesse aqui √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 414 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem v√°rias maneiras de integrar Large Language Models com conhecimento externo. Uma maneira √© usar modelos de aprendizado profundo para incorporar conhecimento externo aos modelos de linguagem. Outra maneira √© usar modelos de mem√≥ria para armazenar e recuperar informa√ß√µes externas. Al√©m disso, tamb√©m √© poss√≠vel usar modelos de aten√ß√£o para direcionar a aten√ß√£o para informa√ß√µes externas relevantes.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"Eu s√≥ quero analisar as diferentes possibilidades. O que voc√™ pode pensar?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 594 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem v√°rias fontes de dados que podem ser usadas para fornecer contexto aos modelos de linguagem. Estas incluem bases de dados de conhecimento, como o Wikidata, fontes de dados estruturadas, como o Freebase, e fontes de dados n√£o estruturadas, como not√≠cias, documentos e conversas. Al√©m disso, tamb√©m √© poss√≠vel usar dados de aprendizado profundo, como imagens e v√≠deos, para fornecer contexto aos modelos.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 645 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Seu objetivo √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Qual √© o meu objetivo novamente?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso `LLM` com `ConversationBufferMemory` pode lembrar claramente as intera√ß√µes anteriores na conversa. Vamos dar uma olhada em como o `LLM` est√° salvando nossa conversa anterior. Podemos fazer isso acessando o atributo `.buffer` para `.memory` em nossa cadeia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Bom dia AI\n",
      "AI:  Bom dia! Como posso ajudar?\n",
      "Human: Bom dia AI\n",
      "AI:  Bom dia novamente! O que posso fazer por voc√™?\n",
      "Human: Meu interesse aqui √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.\n",
      "AI:  Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreens√£o de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Voc√™ pode me dar alguns exemplos de como isso pode ser feito?\n",
      "Human: Eu s√≥ quero analisar as diferentes possibilidades. O que voc√™ pode pensar?\n",
      "AI:  Existem v√°rias maneiras de integrar Large Language Models com conhecimento externo. Uma maneira √© usar modelos de aprendizado profundo para incorporar conhecimento externo aos modelos de linguagem. Outra maneira √© usar modelos de mem√≥ria para armazenar e recuperar informa√ß√µes externas. Al√©m disso, tamb√©m √© poss√≠vel usar modelos de aten√ß√£o para direcionar a aten√ß√£o para informa√ß√µes externas relevantes.\n",
      "Human: Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\n",
      "AI:  Existem v√°rias fontes de dados que podem ser usadas para fornecer contexto aos modelos de linguagem. Estas incluem bases de dados de conhecimento, como o Wikidata, fontes de dados estruturadas, como o Freebase, e fontes de dados n√£o estruturadas, como not√≠cias, documentos e conversas. Al√©m disso, tamb√©m √© poss√≠vel usar dados de aprendizado profundo, como imagens e v√≠deos, para fornecer contexto aos modelos.\n",
      "Human: Qual √© o meu objetivo novamente?\n",
      "AI:  Seu objetivo √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Legal! Portanto, cada parte de nossa conversa foi explicitamente gravada e enviada ao `LLM` no `prompt`.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Tipo de mem√≥ria N¬∫ 2:</font> ConversationSummaryMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">O problema com `ConversationBufferMemory` √© que conforme a conversa progride, a contagem de tokens de nosso hist√≥rico de contexto aumenta. Isso √© problem√°tico porque podemos maximizar nosso `LLM` com um `prompt` muito grande para ser processado.</font>\n",
    "\n",
    "Digite `ConversationSummaryMemory`.\n",
    "\n",
    "Mais uma vez, podemos inferir a partir do nome o que est√° acontecendo. Manteremos um resumo de nossos trechos de conversas anteriores como nosso hist√≥rico. `Como vamos resumi-los?` LLM para o resgate.\n",
    "\n",
    "`Funcionalidade principal:` a mem√≥ria de resumo da conversa√ß√£o mant√©m os trechos anteriores da conversa de forma resumida, onde a sumariza√ß√£o √© realizada por um `LLM`.\n",
    "\n",
    "Nesse caso, precisamos enviar o llm para nosso construtor de mem√≥ria para ativar sua capacidade de resumo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_sum = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Quando temos um LLM, sempre temos um `prompt` üòé Vamos ver o que est√° acontecendo dentro da nossa mem√≥ria de resumo da conversa:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Legal! Assim, cada nova intera√ß√£o √© resumida e anexada a um resumo em execu√ß√£o como a mem√≥ria de nossa cadeia. Vamos ver como isso funciona na pr√°tica!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 281 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Bom dia! Como posso ajudar?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sem count_tokens, chamar√≠amos `conversation_sum(\"Bom dia AI!\")`\n",
    "# Mas vamos manter o controle de nossos Tokens:\n",
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Bom dia AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 629 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreens√£o de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Existe alguma abordagem espec√≠fica que voc√™ est√° considerando?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Meu interesse aqui √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 963 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Existem v√°rias abordagens para integrar modelos de linguagem grandes com conhecimento externo. Uma abordagem comum √© usar modelos de linguagem grandes para gerar representa√ß√µes de palavras que podem ser usadas para acessar conhecimento externo. Outra abordagem √© usar modelos de linguagem grandes para gerar representa√ß√µes de frases que podem ser usadas para acessar conhecimento externo. Al√©m disso, existem abordagens que combinam essas duas abordagens.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Eu s√≥ quero analisar as diferentes possibilidades. O que voc√™ pode pensar?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1113 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem v√°rias fontes de dados que podem ser usadas para fornecer contexto ao modelo de linguagem grande, como bases de dados de conhecimento, corpora de texto, bases de dados de imagens, bases de dados de √°udio e v√≠deo, entre outras. Al√©m disso, tamb√©m √© poss√≠vel usar t√©cnicas de aprendizado de m√°quina para extrair informa√ß√µes relevantes de fontes de dados n√£o estruturadas.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1088 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Seu objetivo √© analisar as diferentes possibilidades de integrar modelos de linguagem de grande porte com conhecimento externo para melhorar a compreens√£o de linguagem natural.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Qual √© o meu objetivo novamente?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greets the AI with \"Bom dia!\" and the AI responds with \"Bom dia! Como posso ajudar?\". The human then expresses interest in exploring the potential of integrating Large Language Models with external knowledge, to which the AI responds that it is familiar with Large Language Models and how they can be used to improve natural language understanding. The AI is interested in knowing how these models can be integrated with external knowledge and asks if there is a specific approach the human is considering. The human responds that they just want to analyze the different possibilities, and the AI explains that there are various approaches to integrating Large Language Models with external knowledge, such as using the models to generate word representations to access external knowledge, using the models to generate phrase representations to access external knowledge, or combining both approaches. The human then asks what types of data sources can be used to provide context to the model, and the AI explains that there are various data sources that can be used, such as knowledge databases, text corpora, image databases, audio and video databases, and others. Additionally, machine learning techniques can also be used to extract relevant information from unstructured data sources. The human then asks what their goal is again, and the AI responds that their goal\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Voc√™ deve estar se perguntando . . . se a contagem agregada de tokens √© maior em cada chamada aqui do que no exemplo do buffer, `por que dever√≠amos usar esse tipo de mem√≥ria?` Bem, se verificarmos o buffer, perceberemos que, embora estejamos usando mais tokens em cada inst√¢ncia de nossa conversa, nosso hist√≥rico final √© mais curto. Isso nos permitir√° ter muito mais intera√ß√µes antes de `atingirmos o tamanho m√°ximo do nosso prompt`, tornando nosso `chatbot` mais robusto para conversas mais longas.\n",
    "\n",
    "Podemos contar o n√∫mero de tokens sendo usados (`sem fazer uma chamada para OpenAI`) usando o tokenizer `tiktoken` da seguinte forma:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento da conversa√ß√£o da mem√≥ria do buffer (ConversationBufferMemory): 591\n",
      "Comprimento da conversa de mem√≥ria resumida (ConversationSummaryMemory): 256\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos o tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model('text-davinci-003')\n",
    "\n",
    "# Mostra o n√∫mero de TOKENS para a mem√≥ria usada por cada tipo de mem√≥ria\n",
    "print(\n",
    "    f'Comprimento da conversa√ß√£o da mem√≥ria do buffer (ConversationBufferMemory): {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Comprimento da conversa de mem√≥ria resumida (ConversationSummaryMemory): {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Nota pr√°tica:</font> \n",
    "\n",
    "os modelos `text-davinci-003` e `gpt-3.5-turbo` t√™m uma grande [contagem m√°xima de tokens](https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens) de `4096 tokens` entre `prompt` e `resposta`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Tipo de mem√≥ria N¬∫ 3:</font> ConversationBufferWindowMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra √≥tima op√ß√£o para esses casos √© a `ConversationBufferWindowMemory`, onde manteremos algumas das √∫ltimas intera√ß√µes em nossa mem√≥ria, mas descartaremos intencionalmente as mais antigas - `mem√≥ria de curto prazo`, se desejar. Aqui, a contagem agregada de tokens e a contagem de tokens por chamada cair√£o visivelmente. **Vamos controlar esta janela com o par√¢metro `k`**.\n",
    "\n",
    "`Recurso principal:` <font color=\"orange\">a mem√≥ria da janela do buffer de conversa√ß√£o mant√©m as partes mais recentes da conversa em formato bruto.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationBufferWindowMemory(k=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 78 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Bom dia! Como posso ajudar?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Bom dia AI!\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 270 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreens√£o de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Existem v√°rias abordagens para fazer isso, como a incorpora√ß√£o de informa√ß√µes externas em modelos de linguagem natural, a incorpora√ß√£o de conhecimento em modelos de linguagem natural e a incorpora√ß√£o de conhecimento em modelos de aprendizado profundo.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Meu interesse aqui √© explorar o potencial de integra√ß√£o de Large Language Models com conhecimento externo.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 538 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem v√°rias maneiras de integrar conhecimento externo com Large Language Models. Uma abordagem √© usar modelos de linguagem natural para incorporar informa√ß√µes externas. Isso pode ser feito adicionando informa√ß√µes externas ao modelo de linguagem natural, como dados de texto, imagens, v√≠deos ou outros dados. Outra abordagem √© usar modelos de aprendizado profundo para incorporar conhecimento externo. Isso pode ser feito adicionando informa√ß√µes externas ao modelo de aprendizado profundo, como dados de texto, imagens, v√≠deos ou outros dados. Al√©m disso, tamb√©m √© poss√≠vel usar modelos de linguagem natural para incorporar conhecimento externo. Isso pode ser feito adicionando informa√ß√µes externas ao modelo de linguagem natural, como'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Eu s√≥ quero analisar as diferentes possibilidades. O que voc√™ pode pensar?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 634 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Os dados externos usados para fornecer contexto ao modelo podem ser de v√°rios tipos, como dados de texto, imagens, v√≠deos, √°udio, dados de sensores, dados de localiza√ß√£o, dados de redes sociais, dados de jogos, dados de sa√∫de, dados de finan√ßas, dados de clima, dados de tr√°fego, dados de mercado, dados de not√≠cias, dados de blogs, dados de f√≥runs, dados de coment√°rios, dados de an√°lise de sentimentos, dados de an√°lise de tend√™ncias, dados de an√°lise de comportamento, dados de an√°lise de conte√∫do, dados de an√°lise de linguagem, dados de an√°lise de imagem, dados de an√°lise de v√≠deo, dados de an√°lise de √°udio, dados de an√°lise de dados e outros.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 387 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' O seu objetivo √© usar dados externos para fornecer contexto ao modelo.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Qual √© o meu objetivo novamente?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Como podemos ver, ele efetivamente `'esqueceu'` o que conversamos na primeira intera√ß√£o. Vamos ver o que ele `'lembra'`. <font color=\"yellow\">Dado que definimos k como 1, esperamos que ele se lembre apenas da √∫ltima intera√ß√£o.</font>\n",
    "\n",
    "Precisamos acessar um m√©todo especial aqui, pois neste tipo de mem√≥ria, o buffer √© passado primeiro por este m√©todo para ser enviado posteriormente para o LLM.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Qual √© o meu objetivo novamente?\n",
      "AI:  O seu objetivo √© usar dados externos para fornecer contexto ao modelo.\n"
     ]
    }
   ],
   "source": [
    "print(bufw_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Faz sentido.\n",
    "\n",
    "No lado positivo, estamos encurtando a dura√ß√£o da conversa em compara√ß√£o com o `buffer de mem√≥ria sem janela`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento da conversa√ß√£o da mem√≥ria do buffer (ConversationBufferMemory): 614\n",
      "Comprimento da conversa de mem√≥ria resumida (ConversationSummaryMemory): 256\n",
      "Comprimento da conversa√ß√£o da mem√≥ria da janela de buffer (ConversationBufferWindowMemory): 44\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Comprimento da conversa√ß√£o da mem√≥ria do buffer (ConversationBufferMemory): {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Comprimento da conversa de mem√≥ria resumida (ConversationSummaryMemory): {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
    "    f'Comprimento da conversa√ß√£o da mem√≥ria da janela de buffer (ConversationBufferWindowMemory): {len(tokenizer.encode(bufw_history))}'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Nota Pr√°tica:</font> \n",
    "\n",
    "\n",
    "Estamos usando `k=1` aqui para fins ilustrativos, na maioria das aplica√ß√µes do mundo real voc√™ precisaria de um valor mais alto para `k`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mais tipos de mem√≥ria!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que j√° entendemos a mem√≥ria, apresentaremos mais alguns tipos de mem√≥ria aqui e esperamos que uma breve descri√ß√£o seja suficiente para entender sua funcionalidade subjacente.\n",
    "\n",
    "\n",
    "### <font color=\"pink\">ConversationSummaryBufferMemory</font>\n",
    "\n",
    "`Recurso principal:` a mem√≥ria de resumo da conversa mant√©m um resumo das primeiras partes da conversa enquanto ret√©m uma lembran√ßa bruta das √∫ltimas intera√ß√µes.\n",
    "\n",
    "\n",
    "### <font color=\"pink\">ConversationKnowledgeGraphMemory</font>\n",
    "\n",
    "Este √© um tipo de mem√≥ria super legal que foi introduzido recentemente. Baseia-se no `conceito de grafo de conhecimento` que reconhece diferentes entidades e as conecta em pares com um predicado resultando em trig√™meos `(sujeito, predicado, objeto)`. Isso nos permite compactar muitas informa√ß√µes em trechos altamente significativos que podem ser inseridos no modelo como contexto. Se voc√™ quiser entender esse tipo de mem√≥ria com mais profundidade, confira esta postagem no [blog](https://apex974.com/articles/explore-langchain-support-for-knowledge-graph).\n",
    "\n",
    "\n",
    "`Caracter√≠stica chave:` <font color=\"red\">a mem√≥ria do `grafo de conhecimento` da conversa√ß√£o mant√©m um `grafo de conhecimento` de todas as ENTIDADES que foram mencionadas nas intera√ß√µes junto com suas `rela√ß√µes SEM√ÇNTICAS`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Voc√™ tem que instalar esta Biblioteca:\n",
    "\n",
    "#%pip install -qU networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1242 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ol√° Eddy! √â um prazer conhec√™-lo. Eu sou um AI e estou aqui para ajudar. Voc√™ gosta de melancia? √â uma fruta muito saborosa!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_kg, \n",
    "    \"Meu nome √© Eddy e gosto muito de melancia!\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A mem√≥ria mant√©m um `grafo de conhecimento` de tudo o que aprendeu at√© agora.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Eddy', 'Eddy', 'is named'), ('Eddy', 'melancia', 'likes')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_kg.memory.kg.get_triples()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"pink\">ConversationEntityMemory</font>\n",
    "\n",
    "\n",
    "`Caracter√≠stica chave:` a mem√≥ria das entidades de conversa√ß√£o guarda uma recorda√ß√£o das principais ENTIDADES MENCIONADAS, juntamente com os seus *atributos espec√≠ficos*.\n",
    "\n",
    "A maneira como isso funciona √© bastante semelhante ao `ConversationKnowledgeGraphMemory`, voc√™ pode consultar os documentos se quiser v√™-lo em a√ß√£o."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"green\">O que mais podemos fazer com a mem√≥ria?</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem v√°rias coisas legais que podemos fazer com a mem√≥ria no `LangChain`. Pudermos:\n",
    "\n",
    "ü•∏ implementar nosso pr√≥prio m√≥dulo de mem√≥ria customizada\n",
    "\n",
    "ü•∏ usar v√°rios m√≥dulos de mem√≥ria na mesma cadeia\n",
    "\n",
    "ü•∏ combinar agentes com mem√≥ria e outras ferramentas\n",
    "\n",
    "Para mais detalhes d√° uma olhada [AQUI](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
