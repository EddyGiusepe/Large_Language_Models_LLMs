{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">LangChain 3: Memória ChatBot para ChatGPT, Davinci + outros LLMs</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste script seguimos aprimorando nossos conhecimentos com o Framework LangChain com o [James Briggs](https://www.pinecone.io/learn/langchain-conversational-memory/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `memória conversacional` é como um `chatbot` pode responder a várias consultas de maneira semelhante a um bate-papo. Ele permite uma conversa coerente e, sem ele, cada consulta seria tratada como uma entrada totalmente independente, sem considerar as interações anteriores. A `memória` permite que um Large Language Model (`LLM`) lembre-se de interações anteriores com o usuário. <font color=\"orange\">Por padrão, os LLMs são sem estado — significando que cada consulta recebida é processada independentemente de outras interações.</font> A única coisa que existe para um agente sem estado é a entrada atual, nada mais. Existem muitos aplicativos em que lembrar as interações anteriores é muito importante, como os `chatbots`. A `memória conversacional` nos permite fazer isso. Existem várias maneiras de implementar a memória conversacional. No contexto do `LangChain`, todos eles são construídos sobre o ``ConversationChain``."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memória Conversacional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começaremos importando todas as bibliotecas que usaremos neste exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
    "                                                  ConversationSummaryMemory, \n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto é quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui instanciamos o Modelo que vamos usar:\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=Eddy_API_KEY_OpenAI,\n",
    "    model_name='text-davinci-003'  # Podemos usar o LLM como: 'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Posteriormente, faremos uso de uma função utilitária `count_tokens`. Isso nos permitirá contar o número de tokens que estamos usando para cada chamada. Nós o definimos assim:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Gastou um total de {cb.total_tokens} Tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Agora vamos mergulhar na `memória conversacional`:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que é memória?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memória é a capacidade de um agente de lembrar interações anteriores com o usuário (`pense em chatbots`)\n",
    "\n",
    "A definição oficial de `memória` é a seguinte:\n",
    "\n",
    "<font color=\"orange\">Por padrão, `Chains` e `Agents` são sem estado, o que significa que eles tratam cada consulta recebida de forma independente. Em algumas aplicações (sendo os `chatbots` um ÓTIMO exemplo), é muito importante lembrar as interações anteriores, tanto a curto prazo, quanto a longo prazo. O conceito de `“Memória”` existe para fazer exatamente isso.</font>\n",
    "\n",
    "Como veremos, embora isso pareça muito simples, existem várias maneiras diferentes de implementar esse recurso de memória.\n",
    "\n",
    "Antes de nos aprofundarmos nos diferentes módulos de memória que a biblioteca oferece, vamos apresentar a cadeia que usaremos para esses exemplos: a `ConversationChain`.\n",
    "\n",
    "Como sempre, ao entender uma chain, é interessante dar uma olhada em seu `prompt` primeiro e depois dar uma olhada em seu método `._call`. Como vimos no capítulo sobre `chains`, podemos verificar o `prompt` acessando o `template` dentro do atributo prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que se segue é uma conversa amigável entre um `humano` e uma `IA`. A IA é falante e fornece muitos detalhes específicos de seu contexto. Se a IA não souber a resposta para uma pergunta, ela diz com sinceridade que não sabe.\n",
    "```\n",
    "Current conversaton:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\n",
    "```\n",
    "Interessante! Portanto, o prompt desta cadeia está dizendo para conversar com o usuário e tentar dar respostas verdadeiras. Se olharmos de perto, há um novo componente no `prompt` que não vimos quando estávamos mexendo no `LLMMathChain`: history. É aqui que nossa memória entrará em ação.\n",
    "\n",
    "O que essa cadeia está fazendo com esse prompt? Vamos dar uma olhada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tudo junto:\n",
    "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation._call))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation.apply))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nada realmente mágico acontecendo aqui, apenas uma passagem direta por um `LLM`. Na verdade, essa cadeia herda esses métodos diretamente do `LLMChain` sem nenhuma modificação:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tudo junto:\n",
    "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LLMChain._call))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LLMChain.apply))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Então, basicamente, essa cadeia combina uma `entrada do usuário` com o `histórico da conversa` para gerar uma resposta significativa (e, com sorte, verdadeira).\n",
    "\n",
    "Agora que entendemos o básico da cadeia que usaremos, podemos entrar na memória. Vamos mergulhar!</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipos de Memória"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Nesta seção, revisaremos vários tipos de memória e analisaremos os `prós` e `contras` de cada um, para que você possa escolher o melhor para o seu caso de uso.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Tipo de memória Nº 1:</font> ConversationBufferMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `ConversationBufferMemory` faz exatamente o que seu nome sugere: ele mantém um buffer dos trechos da conversa anterior como parte do contexto no prompt.\n",
    "\n",
    "`Recurso principal:` a memória do buffer de conversação mantém as partes anteriores da conversa completamente inalteradas, em sua forma bruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Passamos um `prompt` de `usuário` para `ConversationBufferMemory` da seguinte forma:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Bom dia AI',\n",
       " 'history': 'Human: Bom dia AI\\nAI:  Bom dia! Como posso ajudar?\\nHuman: Bom dia AI\\nAI:  Bom dia novamente! O que posso fazer por você?\\nHuman: Meu interesse aqui é explorar o potencial de integração de Large Language Models com conhecimento externo.\\nAI:  Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreensão de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Você pode me dar alguns exemplos de como isso pode ser feito?\\nHuman: Eu só quero analisar as diferentes possibilidades. O que você pode pensar?\\nAI:  Existem várias maneiras de integrar Large Language Models com conhecimento externo. Uma maneira é usar modelos de aprendizado profundo para incorporar conhecimento externo aos modelos de linguagem. Outra maneira é usar modelos de memória para armazenar e recuperar informações externas. Além disso, também é possível usar modelos de atenção para direcionar a atenção para informações externas relevantes.\\nHuman: Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\\nAI:  Existem várias fontes de dados que podem ser usadas para fornecer contexto aos modelos de linguagem. Estas incluem bases de dados de conhecimento, como o Wikidata, fontes de dados estruturadas, como o Freebase, e fontes de dados não estruturadas, como notícias, documentos e conversas. Além disso, também é possível usar dados de aprendizado profundo, como imagens e vídeos, para fornecer contexto aos modelos.\\nHuman: Qual é o meu objetivo novamente?\\nAI:  Seu objetivo é explorar o potencial de integração de Large Language Models com conhecimento externo.',\n",
       " 'response': '  Bom dia! Como posso ajudar?'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_buf(\"Bom dia AI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta chamada usou um total de \"n\" tokens, mas não podemos ver isso acima. Se quisermos contar o número de tokens sendo usados, basta passar nosso objeto da cadeia de conversação e a mensagem que gostaríamos de inserir por meio da função `count_tokens` que definimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 105 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Bom dia novamente! O que posso fazer por você?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(conversation_buf, \"Bom dia AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 235 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreensão de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Você pode me dar alguns exemplos de como isso pode ser feito?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Meu interesse aqui é explorar o potencial de integração de Large Language Models com conhecimento externo.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 414 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem várias maneiras de integrar Large Language Models com conhecimento externo. Uma maneira é usar modelos de aprendizado profundo para incorporar conhecimento externo aos modelos de linguagem. Outra maneira é usar modelos de memória para armazenar e recuperar informações externas. Além disso, também é possível usar modelos de atenção para direcionar a atenção para informações externas relevantes.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"Eu só quero analisar as diferentes possibilidades. O que você pode pensar?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 594 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem várias fontes de dados que podem ser usadas para fornecer contexto aos modelos de linguagem. Estas incluem bases de dados de conhecimento, como o Wikidata, fontes de dados estruturadas, como o Freebase, e fontes de dados não estruturadas, como notícias, documentos e conversas. Além disso, também é possível usar dados de aprendizado profundo, como imagens e vídeos, para fornecer contexto aos modelos.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 645 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Seu objetivo é explorar o potencial de integração de Large Language Models com conhecimento externo.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Qual é o meu objetivo novamente?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso `LLM` com `ConversationBufferMemory` pode lembrar claramente as interações anteriores na conversa. Vamos dar uma olhada em como o `LLM` está salvando nossa conversa anterior. Podemos fazer isso acessando o atributo `.buffer` para `.memory` em nossa cadeia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Bom dia AI\n",
      "AI:  Bom dia! Como posso ajudar?\n",
      "Human: Bom dia AI\n",
      "AI:  Bom dia novamente! O que posso fazer por você?\n",
      "Human: Meu interesse aqui é explorar o potencial de integração de Large Language Models com conhecimento externo.\n",
      "AI:  Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreensão de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Você pode me dar alguns exemplos de como isso pode ser feito?\n",
      "Human: Eu só quero analisar as diferentes possibilidades. O que você pode pensar?\n",
      "AI:  Existem várias maneiras de integrar Large Language Models com conhecimento externo. Uma maneira é usar modelos de aprendizado profundo para incorporar conhecimento externo aos modelos de linguagem. Outra maneira é usar modelos de memória para armazenar e recuperar informações externas. Além disso, também é possível usar modelos de atenção para direcionar a atenção para informações externas relevantes.\n",
      "Human: Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\n",
      "AI:  Existem várias fontes de dados que podem ser usadas para fornecer contexto aos modelos de linguagem. Estas incluem bases de dados de conhecimento, como o Wikidata, fontes de dados estruturadas, como o Freebase, e fontes de dados não estruturadas, como notícias, documentos e conversas. Além disso, também é possível usar dados de aprendizado profundo, como imagens e vídeos, para fornecer contexto aos modelos.\n",
      "Human: Qual é o meu objetivo novamente?\n",
      "AI:  Seu objetivo é explorar o potencial de integração de Large Language Models com conhecimento externo.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Legal! Portanto, cada parte de nossa conversa foi explicitamente gravada e enviada ao `LLM` no `prompt`.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Tipo de memória Nº 2:</font> ConversationSummaryMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">O problema com `ConversationBufferMemory` é que conforme a conversa progride, a contagem de tokens de nosso histórico de contexto aumenta. Isso é problemático porque podemos maximizar nosso `LLM` com um `prompt` muito grande para ser processado.</font>\n",
    "\n",
    "Digite `ConversationSummaryMemory`.\n",
    "\n",
    "Mais uma vez, podemos inferir a partir do nome o que está acontecendo. Manteremos um resumo de nossos trechos de conversas anteriores como nosso histórico. `Como vamos resumi-los?` LLM para o resgate.\n",
    "\n",
    "`Funcionalidade principal:` a memória de resumo da conversação mantém os trechos anteriores da conversa de forma resumida, onde a sumarização é realizada por um `LLM`.\n",
    "\n",
    "Nesse caso, precisamos enviar o llm para nosso construtor de memória para ativar sua capacidade de resumo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_sum = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Quando temos um LLM, sempre temos um `prompt` 😎 Vamos ver o que está acontecendo dentro da nossa memória de resumo da conversa:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Legal! Assim, cada nova interação é resumida e anexada a um resumo em execução como a memória de nossa cadeia. Vamos ver como isso funciona na prática!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 281 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Bom dia! Como posso ajudar?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sem count_tokens, chamaríamos `conversation_sum(\"Bom dia AI!\")`\n",
    "# Mas vamos manter o controle de nossos Tokens:\n",
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Bom dia AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 629 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreensão de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Existe alguma abordagem específica que você está considerando?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Meu interesse aqui é explorar o potencial de integração de Large Language Models com conhecimento externo.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 963 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Existem várias abordagens para integrar modelos de linguagem grandes com conhecimento externo. Uma abordagem comum é usar modelos de linguagem grandes para gerar representações de palavras que podem ser usadas para acessar conhecimento externo. Outra abordagem é usar modelos de linguagem grandes para gerar representações de frases que podem ser usadas para acessar conhecimento externo. Além disso, existem abordagens que combinam essas duas abordagens.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Eu só quero analisar as diferentes possibilidades. O que você pode pensar?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1113 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem várias fontes de dados que podem ser usadas para fornecer contexto ao modelo de linguagem grande, como bases de dados de conhecimento, corpora de texto, bases de dados de imagens, bases de dados de áudio e vídeo, entre outras. Além disso, também é possível usar técnicas de aprendizado de máquina para extrair informações relevantes de fontes de dados não estruturadas.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1088 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Seu objetivo é analisar as diferentes possibilidades de integrar modelos de linguagem de grande porte com conhecimento externo para melhorar a compreensão de linguagem natural.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Qual é o meu objetivo novamente?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greets the AI with \"Bom dia!\" and the AI responds with \"Bom dia! Como posso ajudar?\". The human then expresses interest in exploring the potential of integrating Large Language Models with external knowledge, to which the AI responds that it is familiar with Large Language Models and how they can be used to improve natural language understanding. The AI is interested in knowing how these models can be integrated with external knowledge and asks if there is a specific approach the human is considering. The human responds that they just want to analyze the different possibilities, and the AI explains that there are various approaches to integrating Large Language Models with external knowledge, such as using the models to generate word representations to access external knowledge, using the models to generate phrase representations to access external knowledge, or combining both approaches. The human then asks what types of data sources can be used to provide context to the model, and the AI explains that there are various data sources that can be used, such as knowledge databases, text corpora, image databases, audio and video databases, and others. Additionally, machine learning techniques can also be used to extract relevant information from unstructured data sources. The human then asks what their goal is again, and the AI responds that their goal\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Você deve estar se perguntando . . . se a contagem agregada de tokens é maior em cada chamada aqui do que no exemplo do buffer, `por que deveríamos usar esse tipo de memória?` Bem, se verificarmos o buffer, perceberemos que, embora estejamos usando mais tokens em cada instância de nossa conversa, nosso histórico final é mais curto. Isso nos permitirá ter muito mais interações antes de `atingirmos o tamanho máximo do nosso prompt`, tornando nosso `chatbot` mais robusto para conversas mais longas.\n",
    "\n",
    "Podemos contar o número de tokens sendo usados (`sem fazer uma chamada para OpenAI`) usando o tokenizer `tiktoken` da seguinte forma:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento da conversação da memória do buffer (ConversationBufferMemory): 591\n",
      "Comprimento da conversa de memória resumida (ConversationSummaryMemory): 256\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos o tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model('text-davinci-003')\n",
    "\n",
    "# Mostra o número de TOKENS para a memória usada por cada tipo de memória\n",
    "print(\n",
    "    f'Comprimento da conversação da memória do buffer (ConversationBufferMemory): {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Comprimento da conversa de memória resumida (ConversationSummaryMemory): {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Nota prática:</font> \n",
    "\n",
    "os modelos `text-davinci-003` e `gpt-3.5-turbo` têm uma grande [contagem máxima de tokens](https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens) de `4096 tokens` entre `prompt` e `resposta`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Tipo de memória Nº 3:</font> ConversationBufferWindowMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra ótima opção para esses casos é a `ConversationBufferWindowMemory`, onde manteremos algumas das últimas interações em nossa memória, mas descartaremos intencionalmente as mais antigas - `memória de curto prazo`, se desejar. Aqui, a contagem agregada de tokens e a contagem de tokens por chamada cairão visivelmente. **Vamos controlar esta janela com o parâmetro `k`**.\n",
    "\n",
    "`Recurso principal:` <font color=\"orange\">a memória da janela do buffer de conversação mantém as partes mais recentes da conversa em formato bruto.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationBufferWindowMemory(k=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 78 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Bom dia! Como posso ajudar?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Bom dia AI!\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 270 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Entendo. Estou familiarizado com Large Language Models e como eles podem ser usados para melhorar a compreensão de linguagem natural. Estou interessado em saber como esses modelos podem ser integrados com conhecimento externo. Existem várias abordagens para fazer isso, como a incorporação de informações externas em modelos de linguagem natural, a incorporação de conhecimento em modelos de linguagem natural e a incorporação de conhecimento em modelos de aprendizado profundo.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Meu interesse aqui é explorar o potencial de integração de Large Language Models com conhecimento externo.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 538 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Existem várias maneiras de integrar conhecimento externo com Large Language Models. Uma abordagem é usar modelos de linguagem natural para incorporar informações externas. Isso pode ser feito adicionando informações externas ao modelo de linguagem natural, como dados de texto, imagens, vídeos ou outros dados. Outra abordagem é usar modelos de aprendizado profundo para incorporar conhecimento externo. Isso pode ser feito adicionando informações externas ao modelo de aprendizado profundo, como dados de texto, imagens, vídeos ou outros dados. Além disso, também é possível usar modelos de linguagem natural para incorporar conhecimento externo. Isso pode ser feito adicionando informações externas ao modelo de linguagem natural, como'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Eu só quero analisar as diferentes possibilidades. O que você pode pensar?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 634 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Os dados externos usados para fornecer contexto ao modelo podem ser de vários tipos, como dados de texto, imagens, vídeos, áudio, dados de sensores, dados de localização, dados de redes sociais, dados de jogos, dados de saúde, dados de finanças, dados de clima, dados de tráfego, dados de mercado, dados de notícias, dados de blogs, dados de fóruns, dados de comentários, dados de análise de sentimentos, dados de análise de tendências, dados de análise de comportamento, dados de análise de conteúdo, dados de análise de linguagem, dados de análise de imagem, dados de análise de vídeo, dados de análise de áudio, dados de análise de dados e outros.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Quais tipos de fonte de dados podem ser usados para fornecer contexto ao modelo?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 387 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' O seu objetivo é usar dados externos para fornecer contexto ao modelo.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Qual é o meu objetivo novamente?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Como podemos ver, ele efetivamente `'esqueceu'` o que conversamos na primeira interação. Vamos ver o que ele `'lembra'`. <font color=\"yellow\">Dado que definimos k como 1, esperamos que ele se lembre apenas da última interação.</font>\n",
    "\n",
    "Precisamos acessar um método especial aqui, pois neste tipo de memória, o buffer é passado primeiro por este método para ser enviado posteriormente para o LLM.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Qual é o meu objetivo novamente?\n",
      "AI:  O seu objetivo é usar dados externos para fornecer contexto ao modelo.\n"
     ]
    }
   ],
   "source": [
    "print(bufw_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Faz sentido.\n",
    "\n",
    "No lado positivo, estamos encurtando a duração da conversa em comparação com o `buffer de memória sem janela`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprimento da conversação da memória do buffer (ConversationBufferMemory): 614\n",
      "Comprimento da conversa de memória resumida (ConversationSummaryMemory): 256\n",
      "Comprimento da conversação da memória da janela de buffer (ConversationBufferWindowMemory): 44\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Comprimento da conversação da memória do buffer (ConversationBufferMemory): {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Comprimento da conversa de memória resumida (ConversationSummaryMemory): {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
    "    f'Comprimento da conversação da memória da janela de buffer (ConversationBufferWindowMemory): {len(tokenizer.encode(bufw_history))}'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Nota Prática:</font> \n",
    "\n",
    "\n",
    "Estamos usando `k=1` aqui para fins ilustrativos, na maioria das aplicações do mundo real você precisaria de um valor mais alto para `k`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mais tipos de memória!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que já entendemos a memória, apresentaremos mais alguns tipos de memória aqui e esperamos que uma breve descrição seja suficiente para entender sua funcionalidade subjacente.\n",
    "\n",
    "\n",
    "### <font color=\"pink\">ConversationSummaryBufferMemory</font>\n",
    "\n",
    "`Recurso principal:` a memória de resumo da conversa mantém um resumo das primeiras partes da conversa enquanto retém uma lembrança bruta das últimas interações.\n",
    "\n",
    "\n",
    "### <font color=\"pink\">ConversationKnowledgeGraphMemory</font>\n",
    "\n",
    "Este é um tipo de memória super legal que foi introduzido recentemente. Baseia-se no `conceito de grafo de conhecimento` que reconhece diferentes entidades e as conecta em pares com um predicado resultando em trigêmeos `(sujeito, predicado, objeto)`. Isso nos permite compactar muitas informações em trechos altamente significativos que podem ser inseridos no modelo como contexto. Se você quiser entender esse tipo de memória com mais profundidade, confira esta postagem no [blog](https://apex974.com/articles/explore-langchain-support-for-knowledge-graph).\n",
    "\n",
    "\n",
    "`Característica chave:` <font color=\"red\">a memória do `grafo de conhecimento` da conversação mantém um `grafo de conhecimento` de todas as ENTIDADES que foram mencionadas nas interações junto com suas `relações SEMÂNTICAS`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Você tem que instalar esta Biblioteca:\n",
    "\n",
    "#%pip install -qU networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1242 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Olá Eddy! É um prazer conhecê-lo. Eu sou um AI e estou aqui para ajudar. Você gosta de melancia? É uma fruta muito saborosa!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_kg, \n",
    "    \"Meu nome é Eddy e gosto muito de melancia!\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A memória mantém um `grafo de conhecimento` de tudo o que aprendeu até agora.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Eddy', 'Eddy', 'is named'), ('Eddy', 'melancia', 'likes')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_kg.memory.kg.get_triples()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"pink\">ConversationEntityMemory</font>\n",
    "\n",
    "\n",
    "`Característica chave:` a memória das entidades de conversação guarda uma recordação das principais ENTIDADES MENCIONADAS, juntamente com os seus *atributos específicos*.\n",
    "\n",
    "A maneira como isso funciona é bastante semelhante ao `ConversationKnowledgeGraphMemory`, você pode consultar os documentos se quiser vê-lo em ação."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"green\">O que mais podemos fazer com a memória?</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem várias coisas legais que podemos fazer com a memória no `LangChain`. Pudermos:\n",
    "\n",
    "🥸 implementar nosso próprio módulo de memória customizada\n",
    "\n",
    "🥸 usar vários módulos de memória na mesma cadeia\n",
    "\n",
    "🥸 combinar agentes com memória e outras ferramentas\n",
    "\n",
    "Para mais detalhes dá uma olhada [AQUI](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
