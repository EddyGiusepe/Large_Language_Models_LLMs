{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">LangChain 2: Chains de LLM usando GPT-3.5 e outros LLMs</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste script sobre `LangChain`, exploraremos as cadeias (`Chains`), com foco nas cadeias `gen√©ricas` e `utilit√°rias`, como a `LLMChain`. Esses s√£o os principais recursos do `LangChain` que atuam como a base por tr√°s de usos mais avan√ßados do langchain, como `IA conversacional` (`chatbots`), `recupera√ß√£o de ML aumentado` (retrieval augmented ML) e muito mais. \n",
    "\n",
    "LangChain √© um Framework popular que permite aos usu√°rios criar rapidamente aplicativos e pipelines em torno de Large Language Models. Ele se integra diretamente aos modelos `GPT-3` e `GPT-3.5` da OpenAI e √†s alternativas de c√≥digo aberto do `Hugging Face`, como os modelos `flan-t5 do Google`. Ele pode ser usado para `chatbots`, `perguntas-respostas generativas` (GQA), `resumos` e muito mais. A ideia central da biblioteca √© que podemos \"encadear\" diferentes componentes para criar casos de uso mais avan√ßados em torno de LLMs. As cadeias podem consistir em v√°rios componentes de v√°rios m√≥dulos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">As `Chains` s√£o o n√∫cleo do `LangChain`. Eles s√£o simplesmente uma cadeia de componentes, executados em uma ordem espec√≠fica.\n",
    "\n",
    "A mais simples dessas cadeias √© a `LLMChain`. Ele funciona recebendo a entrada de um usu√°rio, passando para o primeiro elemento da cadeia ‚Äî um `PromptTemplate` ‚Äî para formatar a entrada em um prompt espec√≠fico. O prompt formatado √© ent√£o passado para o pr√≥ximo (e √∫ltimo) elemento da cadeia ‚Äî um `LLM`.\n",
    "\n",
    "Come√ßaremos importando todas as bibliotecas que usaremos neste exemplo.</font>\n",
    "\n",
    "Antes disso vamos inserir a nossa Chave API OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto √© quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] \n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import re\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=Eddy_API_KEY_OpenAI\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um utilit√°rio extra que usaremos √© esta fun√ß√£o que nos dir√° quantos `Tokens` estamos usando em cada chamada. Esta √© uma boa pr√°tica cada vez mais importante √† medida que usamos ferramentas mais complexas que podem fazer v√°rias chamadas √† `API` (como `agentes`). `√â muito importante ter um controle de quantos tokens estamos gastando para evitar gastos inesperados`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Gastou um total de {cb.total_tokens} tokens')\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que s√£o cadeias afinal?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As cadeias s√£o um dos blocos de constru√ß√£o fundamentais desta biblioteca (como voc√™ pode imaginar!).\n",
    "\n",
    "A defini√ß√£o oficial de `Chains` √© a seguinte:\n",
    "\n",
    "\n",
    "<font color=\"pink\">Uma cadeia (chain) √© composta de elos, que podem ser primitivos ou outras cadeias. Primitivos podem ser `prompts`, `llms`, `utils` ou outras `cadeias`.</font>\n",
    "\n",
    "\n",
    "Portanto, uma cadeia √© basicamente um `pipeline` que processa uma entrada usando uma combina√ß√£o espec√≠fica de primitivas. Intuitivamente, pode ser pensado como uma `'etapa'` que executa um determinado conjunto de opera√ß√µes em uma entrada e retorna o resultado. Eles podem ser qualquer coisa, desde uma passagem baseada em `prompt` por meio de um `LLM` at√© a aplica√ß√£o de uma fun√ß√£o `Python` a um texto.\n",
    "\n",
    "As cadeias s√£o divididas em tr√™s tipos: `cadeias utilit√°rias`, `cadeias gen√©ricas` e `cadeias de combinar documentos`. Neste script, vamos nos concentrar nos *dois primeiros*, j√° que o terceiro √© muito espec√≠fico (ser√° abordado oportunamente).\n",
    "\n",
    "* <font color=\"red\">Cadeias utilit√°rias:</font> cadeias que geralmente s√£o usadas para extrair uma resposta espec√≠fica de um LLM com um prop√≥sito muito restrito e est√£o prontas para serem usadas fora da caixa.\n",
    "\n",
    "* <font color=\"red\">Cadeias Gen√©ricas:</font> cadeias que s√£o usadas como blocos de constru√ß√£o para outras chains, mas n√£o podem ser usadas fora da caixa por conta pr√≥pria.\n",
    "\n",
    "\n",
    "Vamos dar uma olhada no que essas redes t√™m a oferecer!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Cadeias de utilit√°rias</font> (`Utility Chains`)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos come√ßar com uma cadeia de utilidades simples. A `LLMMathChain` d√° aos LLMs a capacidade de fazer matem√°tica. Vamos ver como isso funciona!\n",
    "\n",
    "`Dica profissional:` use `verbose=True` para ver quais s√£o as diferentes etapas da cadeia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "Quanto √© 13 elevado √† pot√™ncia de 0.3432?\u001b[32;1m\u001b[1;3m\n",
      "```python\n",
      "print(13**0.3432)\n",
      "```\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Gastou um total de 271 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math = LLMMathChain(\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    "                       )\n",
    "\n",
    "\n",
    "count_tokens(llm_math, \"Quanto √© 13 elevado √† pot√™ncia de 0.3432?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Vamos ver o que est√° acontecendo aqui. A rede recebeu uma pergunta em linguagem natural e a enviou ao LLM. O LLM retornou um c√≥digo `Python` que a cadeia compilou para nos dar uma resposta. Algumas perguntas surgem. Como o llm sabia que quer√≠amos que ele retornasse o c√≥digo `Python`?</font>\n",
    "\n",
    "\n",
    "## Enter Prompts\n",
    "\n",
    "A pergunta que enviamos como entrada para a cadeia n√£o √© a √∫nica entrada que o llm recebe üòâ. O `Input` √© inserida em um contexto mais amplo, que fornece instru√ß√µes precisas sobre como interpretar a entrada que enviamos. `Isso √© chamado de prompt`. Vamos ver qual √© o prompt dessa cadeia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are GPT-3, and you can't do math.\n",
      "\n",
      "You can do basic math, and your memorization abilities are impressive, but you can't do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\n",
      "\n",
      "So we hooked you up to a Python 3 kernel, and now you can execute code. If anyone gives you a hard math problem, just use this format and we‚Äôll take care of the rest:\n",
      "\n",
      "Question: ${{Question with hard calculation.}}\n",
      "```python\n",
      "${{Code that prints what you need to know}}\n",
      "```\n",
      "```output\n",
      "${{Output of your code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Otherwise, use this simpler format:\n",
      "\n",
      "Question: ${{Question without hard calculation}}\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "\n",
      "```python\n",
      "print(37593 * 67)\n",
      "```\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok .. vamos ver o que temos aqui. Portanto, estamos literalmente dizendo ao LLM que, para problemas matem√°ticos complexos, ele n√£o deve tentar fazer matem√°tica sozinho, mas sim imprimir um c√≥digo `Python` que calcular√° o problema matem√°tico. Provavelmente, se apenas envi√°ssemos a query sem nenhum contexto, o LLM tentaria (`e falharia`) calcular isso por conta pr√≥pria. \n",
    "\n",
    "`Espere! Isso √© test√°vel.. vamos experimentar!` üßê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gastou um total de 35 tokens\n",
      "\n",
      "\n",
      "A resposta √© aproximadamente 2,068.\n"
     ]
    }
   ],
   "source": [
    "# Definimos o prompt para ter apenas a pergunta que fazemos\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['question'],\n",
    "    template='{question}'\n",
    "                       )\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Pedimos ao LLM a resposta sem contexto:\n",
    "print(count_tokens(llm_chain, \"Quanto √© 13 elevado √† pot√™ncia de 0.3432?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Resposta errada!`\n",
    "\n",
    "Aqui reside o poder do `prompting` e um dos nossos insights mais importantes at√© agora:\n",
    "\n",
    "`Insight:` ao usar prompts de forma inteligente, podemos `for√ßar o LLM` a evitar armadilhas comuns, programando-o expl√≠cita e intencionalmente para se comportar de uma determinada maneira.\n",
    "\n",
    "\n",
    "Outro ponto interessante sobre essa cadeia √© que ela n√£o apenas executa uma entrada por meio do LLM, mas tamb√©m compila o c√≥digo Python posteriormente. Vamos ver exatamente como isso funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
      "        llm_executor = LLMChain(\n",
      "            prompt=self.prompt, llm=self.llm, callback_manager=self.callback_manager\n",
      "        )\n",
      "        self.callback_manager.on_text(inputs[self.input_key], verbose=self.verbose)\n",
      "        t = llm_executor.predict(question=inputs[self.input_key], stop=[\"```output\"])\n",
      "        return self._process_llm_result(t)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(llm_math._call))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, podemos ver aqui que, se o LLM retornar o c√≥digo Python, iremos compil√°-lo com um simulador `Python REPL*`. Agora temos a imagem completa da cadeia: ou o LLM retorna uma resposta (`para problemas matem√°ticos simples`) ou retorna o `c√≥digo Python` que compilamos para obter uma resposta exata para problemas mais dif√≠ceis. `Inteligente!`\n",
    "\n",
    "\n",
    "Observe tamb√©m que aqui temos nosso primeiro exemplo de `composi√ß√£o de cadeia`, um conceito-chave por tr√°s do que torna o `LangChain` especial. Estamos usando o `LLMMathChain` que, por sua vez, inicializa e usa um `LLMChain` (uma `'cadeia gen√©rica'`) quando chamado. Podemos fazer qualquer n√∫mero arbitr√°rio dessas composi√ß√µes, efetivamente `'encadeando'` muitas dessas cadeias para obter um comportamento altamente complexo e personaliz√°vel.\n",
    "\n",
    "\n",
    "As <font color=\"yellow\">cadeias de utilit√°rios</font> geralmente seguem essa mesma estrutura b√°sica: h√° um `prompt` para restringir o LLM a retornar um tipo muito espec√≠fico de resposta de uma determinada query. Podemos pedir ao LLM para criar `queries SQL`, chamadas de API e at√© mesmo criar `comandos Bash` em tempo real üî•\n",
    "\n",
    "\n",
    "A lista continua a crescer √† medida que o `LangChain` se torna cada vez mais flex√≠vel e poderoso, ent√£o encorajamos voc√™ a dar uma olhada e mexer mais nos [Notebooks](https://python.langchain.com/en/latest/gallery.html?highlight=notebooks#misc-colab-notebooks) e ver se voc√™ pode achar eles interessantes.\n",
    "\n",
    "* Um `Python REPL` (Read-Eval-Print Loop) √© um shell interativo para executar o c√≥digo Python linha por linha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Cadeias gen√©ricas</font> (`Generic Chains`)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem apenas `tr√™s Cadeias Gen√©ricas` no `LangChain` e iremos mostr√°-las todas no mesmo exemplo. Vamos!\n",
    "\n",
    "Digamos que tivemos a experi√™ncia de obter textos de entrada sujos. Especificamente, como sabemos, os LLMs nos cobram pelo n√∫mero de `Tokens` que usamos e n√£o ficamos felizes em pagar a mais quando a entrada tem caracteres extras. Al√©m disso, n√£o √© legal üòâ\n",
    "\n",
    "`Primeiro`, construiremos uma fun√ß√£o de transforma√ß√£o personalizada para limpar o espa√ßamento de nossos textos. Em seguida, usaremos essa fun√ß√£o para construir uma cadeia onde inserimos nosso texto e esperamos um texto limpo como sa√≠da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # Substitu√≠mos v√°rias novas linhas e v√°rios espa√ßos por um √∫nico\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√â importante ressaltar que, quando inicializamos a cadeia, n√£o enviamos um `LLM` como argumento. Como voc√™ pode imaginar, n√£o ter um `LLM` torna as habilidades dessa cadeia muito mais fracas do que no exemplo que vimos anteriormente. No entanto, como veremos a seguir, combinar essa cadeia com outras cadeias pode nos dar resultados altamente desej√°veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"],\n",
    "                                          output_variables=[\"output_text\"],\n",
    "                                          transform=transform_func\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um texto aleat√≥rio com algum espa√ßamento irregular.\n",
      " Outro aqui tamb√©m.\n"
     ]
    }
   ],
   "source": [
    "# Vejamos como funciona:\n",
    "print(clean_extra_spaces_chain.run('Um texto aleat√≥rio   com    algum espa√ßamento irregular.\\n\\n\\n    Outro aqui    tamb√©m.'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">√ìtimo! Agora as coisas v√£o ficar interessantes.\n",
    "\n",
    "Digamos que queremos usar nossa cadeia para limpar um texto de entrada e, em seguida, parafrasear a entrada em um estilo espec√≠fico, digamos um poeta ou um policial. Como sabemos agora, o `TransformChain` <font color=\"red\">n√£o usa um LLM</font>, ent√£o o estilo ter√° que ser feito em outro lugar. √â a√≠ que entra o nosso `LLMChain`. J√° sabemos sobre essa cadeia e sabemos que podemos fazer coisas legais com `prompts inteligentes`, ent√£o vamos arriscar!\n",
    "\n",
    "\n",
    "`Primeiro` vamos construir o `Template Prompt`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Parafraseie este texto:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "No estilo de um {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"],\n",
    "                        template=template\n",
    "                       )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a seguir, inicializamos nossa Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=llm,\n",
    "                                  prompt=prompt,\n",
    "                                  output_key='final_output'\n",
    "                                 )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√ìtimo! Observe que o texto de entrada no `Template` √© chamado `'output_text'`. Voc√™ consegue adivinhar por qu√™?\n",
    "\n",
    "Vamos passar a sa√≠da do `TransformChain` para o `LLMChain`!\n",
    "\n",
    "Finalmente, precisamos combin√°-los para funcionar como uma cadeia integrada. Para isso, usaremos `SequentialChain`, que √© nosso terceiro bloco de constru√ß√£o de `cadeia gen√©rica`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain],\n",
    "                                   input_variables=['text', 'style'],\n",
    "                                   output_variables=['final_output']\n",
    "                                  )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso input √© a descri√ß√£o dos documentos LangChain de quais chains est√£o sujas com alguns espa√ßos extras ao redor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "As cadeias nos permitem combinar v√°rios\n",
    "\n",
    "\n",
    "componentes juntos para criar um aplicativo √∫nico e coerente.\n",
    "\n",
    "Por exemplo, podemos criar uma cadeia que recebe entrada do usu√°rio,    format√°-la com um PromptTemplate,\n",
    "\n",
    "e ent√£o passa a resposta formatada para um LLM. Podemos construir cadeias mais complexas combinando     v√°rias cadeias juntas ou\n",
    "\n",
    "\n",
    "combinando cadeias com outros componentes.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos prontos. Hora de ser criativo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gastou um total de 419 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAs cadeias nos d√£o a habilidade de juntar v√°rios elementos e criar um aplicativo √∫nico e consistente. Por exemplo, podemos montar uma cadeia que pegue a entrada do usu√°rio, a formate com um PromptTemplate, e ent√£o passe a resposta formatada para um LLM. N√≥s podemos construir cadeias mais complexas juntando v√°rias cadeias ou combinando cadeias com outros componentes.\\n\\nNo estilo de um Um rapper dos anos 90.\\n\\nParaphrase:\\nN√≥s temos a capacidade de juntar v√°rios elementos e criar um aplicativo √∫nico e s√≥lido. Por exemplo, podemos montar uma cadeia que pegue a entrada do usu√°rio, a formate com um PromptTemplate, e ent√£o passe a resposta formatada para um LLM. N√≥s podemos construir cadeias mais complexas j'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain,\n",
    "             {'text': input_text, 'style': 'Um rapper dos anos 90'}\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma nota sobre `LangChain-hub`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LangChain-hub` √© uma biblioteca irm√£ do `LangChain`, onde todas as `chains`, `agentes` e `prompts` s√£o serializados para nosso uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar do `langchain-hub` √© t√£o f√°cil quanto encontrar a cadeia que voc√™ deseja carregar no reposit√≥rio e, em seguida, usar `load_chain` com o caminho (path) correspondente. Tamb√©m temos `load_prompt` e `initialize_agent`, mas falaremos mais sobre isso depois. Vamos ver como podemos fazer isso com nosso `LLMMathChain` que vimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">E se quisermos alterar alguns dos par√¢metros de configura√ß√£o?</font> Podemos simplesmente substitu√≠-lo ap√≥s o carregamento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
