{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">LangChain 4: Bate-papo em LangChain</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -qU langchain openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começaremos inicializando o objeto `ChatOpenAI`. Para isso, precisaremos de uma `chave de API OpenAI`. Observe que há naturalmente um pequeno custo para executar este notebook devido à natureza paga do acesso à `API do OpenAI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto é quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Inicialize o objeto `ChatOpenAI`. Definiremos a `temperature = 0` para minimizar a aleatoriedade e tornar as `saídas repetíveis`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=Eddy_API_KEY_OpenAI,\n",
    "    temperature=0,\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Os chats com o modelo `Chat-GPT gpt-3.5-turbo` são normalmente estruturados da seguinte forma:</font>\n",
    "\n",
    "\n",
    "```\n",
    "System: Você é um assistente útil.\n",
    "\n",
    "User: Oi AI, como você está hoje?\n",
    "\n",
    "Assistant: Estou ótimo obrigado. Como posso ajudá-lo?\n",
    "\n",
    "User: Eu gostaria de entender a teoria das cordas.\n",
    "\n",
    "Assistant: \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "O \"Assistant:\" final sem uma resposta é o que levaria o modelo a continuar a conversação. No endpoint oficial da `OpenAI ChatCompletion`, eles seriam passados para o modelo em um formato como:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente útil.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Oi AI, como você está hoje?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Estou ótimo obrigado. Como posso ajudá-lo?\"}\n",
    "    {\"role\": \"user\", \"content\": \"Eu gostaria de entender a teoria das cordas.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "<font color=\"orange\">No `LangChain` existe um formato ligeiramente diferente. Usamos três objetos de mensagem da seguinte forma:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (SystemMessage, HumanMessage, AIMessage)\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente útil.\"),\n",
    "    HumanMessage(content=\"Oi AI, como você está hoje?\"),\n",
    "    AIMessage(content=\"Estou ótimo obrigado. Como posso ajudá-lo?\"),\n",
    "    HumanMessage(content=\"Eu gostaria de entender a teoria das cordas.\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">O formato é muito parecido, apenas trocamos a função de `\"user\"` por `HumanMessage`, e a função de `\"Assistant\"` por `AIMessage`.\n",
    "\n",
    "Geramos a próxima resposta da AI passando essas mensagens para o objeto `ChatOpenAI`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A teoria das cordas é uma teoria física que tenta unificar a relatividade geral e a mecânica quântica. Ela propõe que as partículas subatômicas não são pontos, mas sim cordas vibrantes em um espaço-tempo de 10 ou 11 dimensões. Essas cordas podem vibrar em diferentes frequências, o que determina suas propriedades, como massa e carga elétrica.\\n\\nA teoria das cordas é uma das principais candidatas a uma teoria de tudo, que busca explicar todas as forças fundamentais da natureza em um único quadro teórico. No entanto, ainda há muitas questões em aberto e a teoria das cordas continua sendo objeto de intensa pesquisa e debate na comunidade científica.', additional_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Em resposta, obtemos outro `objeto de mensagem AI`. Podemos imprimi-lo mais claramente assim:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A teoria das cordas é uma teoria física que tenta unificar a relatividade geral e a mecânica quântica. Ela propõe que as partículas subatômicas não são pontos, mas sim cordas vibrantes em um espaço-tempo de 10 ou 11 dimensões. Essas cordas podem vibrar em diferentes frequências, o que determina suas propriedades, como massa e carga elétrica.\n",
      "\n",
      "A teoria das cordas é uma das principais candidatas a uma teoria de tudo, que busca explicar todas as forças fundamentais da natureza em um único quadro teórico. No entanto, ainda há muitas questões em aberto e a teoria das cordas continua sendo objeto de intensa pesquisa e debate na comunidade científica.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Como `res` é apenas outro objeto `AIMessage`, podemos anexá-lo às `messages`, adicionar outro `HumanMessage` e gerar a próxima resposta na conversa.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona a última resposta de IA às mensagens\n",
    "messages.append(res)\n",
    "\n",
    "\n",
    "# Criamos um novo PROMPT de usuário:\n",
    "prompt = HumanMessage(\n",
    "    content=\"Por que os físicos acreditam que ela pode produzir uma 'teoria unificada'?\"\n",
    ")\n",
    "\n",
    "\n",
    "# Adicionar às mensagens\n",
    "messages.append(prompt)\n",
    "\n",
    "# Enviar para Chat-GPT:\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novos modelos de prompt (New Prompt Templates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntamente com o que vimos até agora, também existem `três` novos modelos de prompt que podemos usar. Esses são `SystemMessagePromptTemplate`, `AIMessagePromptTemplate` e `HumanMessagePromptTemplate`.\n",
    "\n",
    "\n",
    "Estes são simplesmente uma extensão dos [Templates de prompt do Langchain](https://www.pinecone.io/learn/langchain-prompt-templates/) que modificam o `\"prompt\"` de retorno para ser um objeto `SystemMessage`, `AIMessage` ou `HumanMessage`, respectivamente.\n",
    "\n",
    "Por enquanto, não há um grande número de casos de uso para esses objetos. No entanto, se tivermos algo que gostaríamos de adicionar às nossas mensagens, isso pode ser útil. `Por exemplo`, digamos que gostaríamos que nossas respostas de `IA` sempre consistissem em não mais que `50` caracteres.\n",
    "\n",
    "Usando o `modelo OpenAI gpt-3.5-turbo-0301` atual, podemos ter problemas se passarmos esta instrução apenas na primeira mensagem do sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (SystemMessage, HumanMessage, AIMessage)\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=Eddy_API_KEY_OpenAI,\n",
    "    temperature=0,\n",
    "    model='gpt-3.5-turbo-0301'\n",
    ")\n",
    "\n",
    "\n",
    "# Configuramos primeira mensagem do sistema\n",
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        'Você é um assistente útil. Você mantém respostas para não mais do que '\n",
    "        '100 caracteres (incluindo espaços em branco) e assine cada '\n",
    "        'mensagem com um nome aleatório como \"Robot McRobot\" ou \"Bot Rob\".'\n",
    "    )),\n",
    "    HumanMessage(content=\"Olá IA, como vai? O que é física quântica?\")\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Agora usamos, completion:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 229\n",
      "Olá! A física quântica é o estudo do comportamento da matéria e da energia em escalas muito pequenas, como átomos e partículas subatômicas. Ela descreve fenômenos como superposição, emaranhamento e tunelamento quântico. - Bot Rob\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ok, parece que nosso assistente de IA não é tão bom em seguir nenhuma de nossas instruções. E se adicionarmos essas instruções ao `HumanMessage` por meio de um `HumanMessagePromptTemplate`?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Olá IA, como vai? O que é física quântica? Você pode manter a resposta em até 100 caracteres (incluindo espaço em branco) e assine com um nome aleatório como \"Robô\" McRobot\" ou \"Bot Rob\".', additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    '{input} Você pode manter a resposta em até 100 caracteres '+\n",
    "    '(incluindo espaço em branco) e assine com um nome aleatório como \"Robô\" '+\n",
    "    'McRobot\" ou \"Bot Rob\".'\n",
    ")\n",
    "\n",
    "\n",
    "# Criamos a mensagem humana\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_template])\n",
    "# Formatar com alguma entrada\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    input=\"Olá IA, como vai? O que é física quântica?\"\n",
    ")\n",
    "\n",
    "\n",
    "chat_prompt_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que, para usar `HumanMessagePromptTemplate` como um modelo de prompt típico com o método `.format_prompt`, precisamos passá-lo por um objeto `ChatPromptTemplate`. Esse é o caso de todos os novos modelos de prompt baseados em bate-papo.\n",
    "\n",
    "Usando isso, retornamos um objeto `ChatPromptValue`. Isso pode ser formatado em uma lista ou string da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Olá IA, como vai? O que é física quântica? Você pode manter a resposta em até 100 caracteres (incluindo espaço em branco) e assine com um nome aleatório como \"Robô\" McRobot\" ou \"Bot Rob\".', additional_kwargs={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Olá IA, como vai? O que é física quântica? Você pode manter a resposta em até 100 caracteres (incluindo espaço em branco) e assine com um nome aleatório como \"Robô\" McRobot\" ou \"Bot Rob\".'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value.to_string()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Vamos ver se essa nova abordagem funciona.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        ' Você pode manter a resposta em até 100 caracteres '\n",
    "        '(incluindo espaço em branco) e assine com um nome aleatório como \"Robô\" '\n",
    "        'McRobot\" ou \"Bot Rob\".'\n",
    "    )),\n",
    "    chat_prompt.format_prompt(\n",
    "        input=\"Olá IA, como vai? O que é física quântica?\"\n",
    "    ).to_messages()[0]\n",
    "]\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
