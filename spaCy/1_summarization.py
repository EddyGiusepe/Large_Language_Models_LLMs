"""
Data Scientist.: PhD.Eddy Giusepe Chirinos Isidro

ü§ó Baseado no tutorial de jcharistech.com ü§ó

Resumo de texto com SpaCy
-------------------------
* A sumariza√ß√£o de texto √© o processo de destilar as informa√ß√µes mais importantes de uma fonte (ou fontes)
  para produzir uma vers√£o resumida para um determinado usu√°rio (ou usu√°rios) e tarefa (ou tarefas).

* A ideia da sumariza√ß√£o √© encontrar um subconjunto de dados que contenha a ‚Äúinforma√ß√£o‚Äù de todo o conjunto

Id√©ia principal:
================
    * Pr√©-processamento de texto (remover stopwords, pontua√ß√µes).
    * Tabela de frequ√™ncia de palavras/Distribui√ß√£o de frequ√™ncia de palavras - quantas vezes cada palavra aparece no documento
    * Pontue cada frase dependendo das palavras que ela cont√©m e da tabela de frequ√™ncia
    * Crie um resumo juntando todas as frases acima de um determinado limite de pontua√ß√£o   
"""
import spacy
# Pacote de pr√©-processamento de texto:
from spacy.lang.pt.stop_words import STOP_WORDS
from string import punctuation
# Criando uma list de stopwords:
stopwords = list(STOP_WORDS)
#print(stopwords)
print(len(stopwords))

document1 ="""O aprendizado de m√°quina (ML) √© o estudo cient√≠fico de algoritmos e modelos estat√≠sticos que os sistemas de computador
usam para melhorar progressivamente seu desempenho em uma tarefa espec√≠fica. Os algoritmos de aprendizado de m√°quina constroem um modelo
matem√°tico de dados de amostra, conhecidos como "dados de treinamento", para fazer previs√µes ou decis√µes sem serem explicitamente
programados para executar a tarefa. Algoritmos de aprendizado de m√°quina s√£o utilizados nas aplica√ß√µes de filtragem de e-mail, detec√ß√£o
de intrusos de rede e vis√£o computacional, onde √© invi√°vel desenvolver um algoritmo de instru√ß√µes espec√≠ficas para a execu√ß√£o da tarefa.
O aprendizado de m√°quina est√° intimamente relacionado √† estat√≠stica computacional, que se concentra em fazer previs√µes usando computadores.
O estudo da otimiza√ß√£o matem√°tica fornece m√©todos, teoria e dom√≠nios de aplica√ß√£o para o campo do aprendizado de m√°quina. A minera√ß√£o de
dados √© um campo de estudo dentro do aprendizado de m√°quina e se concentra na an√°lise explorat√≥ria de dados por meio do aprendizado n√£o supervisionado.
Em sua aplica√ß√£o em problemas de neg√≥cios, o aprendizado de m√°quina tamb√©m √© conhecido como an√°lise preditiva.
"""



nlp = spacy.load('pt_core_news_lg')


# Construindo um objeto NLP:
docx = nlp(document1)

# Tokenization do texto:
mytokens = [token.text for token in docx]

"""
Tabela de Frequ√™ncia de Palavras
--------------------------------

* Dicion√°rio de palavras e suas contagens
* Quantas vezes cada palavra aparece no documento
* Usando non-stopwords

"""
# Construir Frequ√™ncia de Palavras
# word.text √© tokenization no spaCy:
word_frequencies = {}
for word in docx:
    if word.text not in stopwords:
            if word.text not in word_frequencies.keys():
                word_frequencies[word.text] = 1
            else:
                word_frequencies[word.text] += 1


#print(word_frequencies)

"""
Frequ√™ncia M√°xima de Palavras
-----------------------------

* Encontre a frequ√™ncia ponderada
* Cada palavra sobre a palavra que mais ocorre
* Frase longa sobre frase curta

"""
# Frequ√™ncia M√°xima de Palavras
maximum_frequency = max(word_frequencies.values())

for word in word_frequencies.keys():  
        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

"""Distribui√ß√£o de Frequ√™ncia de Palavras"""
# Tabela de frequ√™ncia:
print("")
#print(word_frequencies)


"""
Pontua√ß√£o da frase e classifica√ß√£o das palavras em cada frase
=============================================================

* Tokens de senten√ßa
* Pontuando cada frase com base no n√∫mero de palavras
* Sem stopwords em nossa tabela de frequ√™ncia de palavras
"""
# Sentence Tokens
sentence_list = [ sentence for sentence in docx.sents ]

# Example of Sentence Tokenization,Word Tokenization and Lowering All Text
# for t in sentence_list:
#     for w in t:
#         print(w.text.lower())
[w.text.lower() for t in sentence_list for w in t ]

# Pontua√ß√£o da senten√ßa comparando cada palavra com a senten√ßa:
sentence_scores = {}  
for sent in sentence_list:  
        for word in sent:
            if word.text.lower() in word_frequencies.keys():
                if len(sent.text.split(' ')) < 30:
                    if sent not in sentence_scores.keys():
                        sentence_scores[sent] = word_frequencies[word.text.lower()]
                    else:
                        sentence_scores[sent] += word_frequencies[word.text.lower()]


# Sentence Score via comparrng each word with sentence
# Alternative Method
lowered_sentence_list = [w.text.lower() for t in sentence_list for w in t ]
lowered_sentence_scores = {}  
for sent in lowered_sentence_list:  
        for word in sent.split():
            if word in word_frequencies.keys():
                if len(sent.split(' ')) < 30:
                    if sent not in sentence_scores.keys():
                        lowered_sentence_scores[sent] = word_frequencies[word]
                    else:
                        lowered_sentence_scores[sent] += word_frequencies[word]

"""Obter pontua√ß√£o da frase"""
# Sentence Score Table
#print(sentence_scores)

"""
Encontrando a Senten√ßa Top N com maior pontua√ß√£o
================================================
* Usando heapq
"""
# Import Heapq 
from heapq import nlargest

summarized_sentences = nlargest(7, sentence_scores, key=sentence_scores.get)

#print(summarized_sentences)

# Converta senten√ßas de Spacy Span em Strings para unir a frase inteira:
for w in summarized_sentences:
     w.text
    #print(w.text)

# List comprehension of senten√ßas convertidas de spacy.span para strings:
final_sentences = [ w.text for w in summarized_sentences ]


"""Juntando as SENTEN√áAS"""

summary = ' '.join(final_sentences)
print(summary)

# Comprimento do resumo:
print(len(summary))


# Comprimento do texto original:
print(len(document1))
