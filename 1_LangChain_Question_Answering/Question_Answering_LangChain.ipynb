{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Quatro maneiras de Question-Answering in LangChain</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script está baseado nos trabalhos da [Sophia Yang]()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converse com seus longos documentos `PDF`: load_qa_chain, RetrievalQA, VectorstoreIndexCreator, ConversationalRetrievalChain.\n",
    "\n",
    "\n",
    "Você está interessado em conversar com seus próprios documentos, seja um `arquivo de texto`, um `PDF` ou um `site`? O `LangChain` torna mais fácil para você responder a perguntas com seus documentos. Mas você sabia que existem pelo menos `4` maneiras de responder a perguntas no LangChain? Nesta postagem do blog, exploraremos quatro maneiras diferentes de responder a perguntas e as várias opções que você pode considerar para seus casos de uso.\n",
    "\n",
    "Só lembrando que é `LangChain`, na opinião de `Sophia Yang`, é a maneira mais fácil de interagir com modelos de linguagem e criar aplicativos. É uma ferramenta de código aberto que envolve muitos `LLMs` e ferramentas. \n",
    "\n",
    "Ok, agora vamos começar a responder perguntas em documentos externos.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando a API da OpenAI e importando as Bibliotecas necessárias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a [API OpenAI](https://platform.openai.com/account) não é gratuita. Você precisará configurar as informações de cobrança para poder usar a `API OpenAI`. Como alternativa, você pode usar modelos do `HuggingFace Hub` ou de outros lugares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain==0.0.137 openai chromadb tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto é quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA # Tem que atualizar --> pip install langchai==0.0.137\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando Documentos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `LangChain` suporta muitos [carregadores de documentos](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html), como `Notion`, `YouTube` e `Figma`. Neste exemplo, gostaria de conversar com meu arquivo `PDF`. Assim, usei o `PyPDFLoader` para carregar meu arquivo.\n",
    "\n",
    "Eu recomendo usar um PDF pequeno. Eu usei, só para demonstração mesmo, um boleto de pagamento da minha conta de energia e um boleto de pagamento de mensalidade da minha faculdade que contém uma página cada boleto (você pode usar qualquer outro documento). Meu boleto contém certas entidades e em base a elas farei certas perguntas! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "O que o oceano disse para o outro oceano? Nada, ele só estava ondulando.\n"
     ]
    }
   ],
   "source": [
    "# Verificamos nosso Modelo Instanciado:\n",
    "\n",
    "llm = OpenAI()\n",
    "print(llm(\"Conte-me uma piada.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando meu documento (Boleto de energia):\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# loader = PyPDFLoader(\"./Karina_boleto_FACULDADE.pdf\")  # PDF apenas com uma página\n",
    "loader = PyPDFLoader(\"./Eddy_Running_the_Vale_API.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='UFES - L abVISIO - V aleS.A.\\nInstanciando a API de Análise de\\nComunicação de Voz (ACV)\\nData Scientist.: Dr.Eddy Giusepe Chirinos Isidro\\n9 de março de 2023\\n1', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 0}),\n",
       " Document(page_content='Resumo\\nNeste conciso tutorial aprenderemos a instanciar a nossa API de Análise\\nde Comunicação de Voz (ACV) da Vale. Mostrarei os passos necessários\\ne suficientes para poder executar a nossa ferramenta, comandos úteis de\\nDockerização, a como expor um serviço local na Internet com ngrok e o\\nuso de outras principais funcionalidades.\\n2', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 1}),\n",
       " Document(page_content='Conte ´udo\\n1 Introdução 4\\n2 Objetivo 5\\n3 Principais passos para instanciar a API da Vale 6\\n3.1 Ngrok . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n3.1.1 Instalando o Ngrok . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n3.2 docker compose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n3', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 2}),\n",
       " Document(page_content='1 Introdu ¸c˜ao\\nSabemos que instanciar uma aplicação num servidor não é uma tarefa simples, já\\nque envolve diversos passos e requer um conhecimento aprofundado sobre as tec-\\nnologias e ferramentas envolvidas no processo. Para isso é fundamental levar em\\nconta alguns passos gerais, como por exemplo: escolher a infraestrutura onde será\\nhospedada a aplicação, escolher o Sistema Operacional (SO), o uso de Docker, o uso\\nde ngrok, instalar as dependências, configurar o ambiente (variáveis de ambiente,\\nconfiguração de Banco de Dados, etc), políticas de segurança, etc. Das mencionadas\\ndetalharei as que usaremos continuamente em nossa aplicação:\\nUsaremos a Dockerização porque nos ajudará a garantir a portabilidade e escalabili-\\ndade da aplicação. Este processo, de Dockerização, envolve a criação de um contêiner\\nque inclui todos os componentes necessários para executar a nossa aplicação. Isso\\npermitirá que a nossa aplicação seja executada em qualquer ambiente que suporte\\ncontêineres Docker.\\nUtilizaremos o aplicativo ngrok para acessar a nossa aplicação em execução em nossa\\nmáquina local (localhost) por meio de uma URL pública. Isso permitirá que outros\\nacessem em nossa aplicação por meio da internet.\\nEntão, com esses passos básicos conseguiremos rodar nossa aplicação e conseguire-\\nmos realizar as inferências que a aplicação envolve. Basicamente, a nossa aplicação\\nrealiza a segmentação de um áudio (Diarization), realiza a transcrição de um áudio\\n(speech-to-text) e identifica quem fala num determinado trecho de áudio (Speaker\\nIdentification).\\nO desenvolvimento de nossa aplicação foi baseada em duas bibliotecas de código\\naberto escritas na linguagem de Programação Python1para processamento de sinais\\nde áudio e fala. A primeira biblioteca é o Pyannote e a outra é o SpeechBrain. Ambas\\nbibliotecas foram projetadas para tarefas de Diarização, separação de fontes e síntese\\nde fala, reconhecimento de fala, etc.\\n1https: //www.python.org /\\n4', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 3}),\n",
       " Document(page_content='2 Objetivo\\n•Conhecer e aplicar os comandos necessários para instanciar a aplicação da Vale.\\n5', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 4}),\n",
       " Document(page_content='3 Principais passos para instanciar a API daVale\\nA seguir farei uma explanação dos principais passos para executar a nossa aplicação.\\n3.1 N grok\\nBasicamente, Ngrok é uma ferramenta CLI (Comand Line Interface) que te permite\\ncriar um túnel seguro, atrás de Network Address Translation (NAT) e Firewalls, que\\nexpõem serviços locais (do nosso computador) para a Internet, tudo isso de forma\\nfácil e segura. A seguinte imagem, Figura 3.1, descreve o uso do ngrok [1]:\\nFigura 3.1: Ngrok expondo um serviço local para a Internet.\\n3.1.1 I nstalando o Ngrok\\nO Ngrok é multiplataforma, ou seja, pode ser instalado no Linux ,Mac OS ou no\\nWindows . Então, para usar o Ngrok o primeiro a fazer é criar uma conta no ngrok.com\\ne seguidamente baixar o arquivo de instalação conforme seu sistema operacional (em\\nnossa aplicação estamos usando o Linux). Assim:\\n6', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 5}),\n",
       " Document(page_content='•Descompacte o arquivo .tgz para instalar\\nNo Linux ou Mac OS, você pode descompactar o ngrok de um Terminal com o\\nseguinte comando:\\n$ tar xvzf ngrok-v3-stable-linux-amd64.tgz\\n•Conecte sua conta\\nA execução desse comando adicionará seu token de autenticação ao arquivo de\\nconfiguração padrão. Isso concederá a você mais recursos e tempos de sessão\\nmais longos. Você copia seu token e cola na linha de comando a seguir (o Token\\nabaixo é fictício)\\n$ ngrok config add-authtoken ##olaDr.EddyUFESgdfodf#_22343aSSDFT_GOD!BMB\\n•Criando um Túnel\\nAgora, o próximo passo é criar um túnel na porta 1337 ( porta na qual nossa\\naplicação está sendo executada, localmente ) expondo esse servidor web via ngrok,\\npara isso executamos em outro terminal o seguinte comando:\\n$ ngrok http 1337\\nA seguinte Figura 3.2, mostra onde baixar o arquivo de instalação do ngrok e mostra\\nno lado esquerdo superior de onde você pode pegar seu Token ( Your Authtoken ).\\nFigura 3.2: Download e Token para o uso do ngrok.\\n7', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 6}),\n",
       " Document(page_content='3.2docker compose\\nDocker Compose é uma ferramenta do Docker [2,3] que permite definir e executar\\naplicativos multi-container Docker. Ele é usado para definir serviços, redes e volumes\\npara vários contêineres como um único aplicativo. O Docker Compose permite que\\nvocê configure e inicie rapidamente vários contêineres Docker juntos, em um ambiente\\nisolado e definido por você.\\nA seguinte Figura 3.3, mostra os serviços da nossa aplicação.\\nFigura 3.3: Arquivo docker-compose.yml contendo nossos serviços.\\nEntão, para poder instanciar os três serviços2da nossa aplicação precisamos executar,\\nna seguinte ordem e no Terminal, os seguintes comandos:\\n2Nossos serviços são: acv-backend ,acv-frontend eacv-models . Obviamente cada serviço tem sua\\nprópria Imagem (Dockerfile), a qual instancia seu próprio contêiner.\\n8', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 7}),\n",
       " Document(page_content='•Para executar o seguinte comando devemos estar no diretório onde se encontra\\no arquivo docker-compose.yml . Ver a Figura 3.3, acima.\\n$ docker compose build\\nEste comando e basicamente usado para construir ou reconstruir as imagens\\nDocker definidas no arquivo docker-compose.yml.\\n•O seguinte comando é usado para construir, criar ou recriar e executar con-\\ntêineres para os serviços definidos no arquivo docker-compose.yml , também\\nconfigura as redes e os volumes especificados em dito arquivo.\\n$ docker compose up\\nEste comando além de iniciar os contêineres em um modo interativo e exibir\\na saída do console para cada contêiner, permite que você monitore o status do\\ncontêiner, visualize eventuais erros e saiba quando o serviço estiver pronto para\\nuso.\\nÉ importante aprender, também, como parar nossa aplicação. Então, para interromper\\nos contêineres de maneira segura, você pode executar (em outro terminal) o seguinte\\ncomando:\\n$ docker compose down\\nE para interromper abruptamente (ou em emergência) todos os contêineres em exe-\\ncução para os serviços definidos no arquivo docker-compose.yml , você pode usar o\\nseguinte comando:\\n$ docker compose kill\\nUm último comando, muito útil, é o uso do docker system prune . Este comando\\né usado para remover recursos não utilizados e liberar espaço em disco no sistema\\nDocker. Ele remove contêineres parados, redes não utilizadas, imagens sem tag e\\nvolumes sem anexos. Ao executar o comando docker system prune , o Docker irá\\nexibir uma mensagem de aviso informando que a ação irá remover permanentemente\\ntodos os recursos não utilizados. Se você deseja executar o comando sem confirmação,\\npode usar a opção -fou–force . Tenha cuidado no seu uso, já que isso significa que é\\npossível que você perca dados ou informações se remover recursos sem querer. Seu\\nuso na linha comando é:\\n$ docker system prune\\n9', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 8}),\n",
       " Document(page_content='Refer ˆencias\\n[1] J. R. da Paixão, “Ngrok: do localhost para o mundo.” https: //medium.com /\\ndesenvolvendo-com-paixao /ngrok-do-localhost-para-o-mundo-5445ad08419,\\n2021. Cited in page: 6.\\n[2] “Docker engine.” https: //docs.docker.com /engine /install /ubuntu /, 2023. Cited in\\npage: 8.\\n[3] E. G. C. Isidro, “Guia para iniciantes em docker.” https: //drive.google.com /file /d/\\n1S-azDR2jFvzDKMuPZJH4uGjefZEwM5Km /view, 2022. Cited in page: 8.\\n10', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 1: <font color=\"red\">load_qa_chain</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_qa_chain` fornece a interface mais genérica para responder a perguntas. Ele carrega uma cadeia que você pode fazer `QA` para seus documentos de entrada e usa `TODO o texto` nos documentos.\n",
    "\n",
    "`chain_type=\"stuff\"` não funcionará porque o número de tokens excede o limite (isso para o caso do PDF da Sophia). Podemos tentar outros tipos de cadeia como `\"map_reduce\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A pesquisa foi feita na UFES.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "\n",
    "chain = load_qa_chain(llm=OpenAI(),\n",
    "                      chain_type=\"stuff\"\n",
    "                     )\n",
    "\n",
    "\n",
    "query = \"A pesquisa foi feita em que universidade?\"\n",
    "\n",
    "chain.run(input_documents=documents,\n",
    "          question=query\n",
    "         )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ele também permite que você faça QA em um conjunto de documentos:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Para vários Documentos:\n",
    "# loaders = [....]\n",
    "# documents = []\n",
    "# for loader in loaders:\n",
    "#     documents.extend(loader.load())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">🤔 Mas e se meu documento for muito longo e ultrapassar o limite de token?</font>\n",
    "\n",
    "Existem duas maneiras de corrigi-lo:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Solução 1: Tipo de Cadeia (chain)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O padrão `chain_type=\"stuff\"` usa `TODO` o texto dos documentos no prompt. Se você tiver um arquivo muito grande (`tal como fez Sophia Yang, com um PDF de `50` páginas`) não funcionará porque excede o `limite de token` e causa erros de limitação de taxa. É por isso que para o exemplo da Sophia Yang, ela teve que usar outro tipo de cadeia, por exemplo `\"map_reduce\"`. \n",
    "\n",
    "A seguir vamos estudar outros tipos de cadeias:\n",
    "\n",
    "🤩 `map_reduce:` Separa os textos em lotes (<font color=\"red\">por exemplo</font>, você pode definir o tamanho do lote em `llm=OpenAI(batch_size=5)`), alimenta cada lote com a pergunta para o `LLM` separadamente e apresenta a resposta final com base nas respostas de cada lote.\n",
    "\n",
    "🤩 `refine:` Separa os textos em lotes, alimenta o primeiro lote para o LLM e alimenta a resposta e o segundo lote para o LLM. Ele refina a resposta passando por todos os lotes.\n",
    "\n",
    "🤩 `map_rerank:` ele separa os textos em lotes, alimenta cada lote para o LLM, retorna uma pontuação de como responde completamente à pergunta e apresenta a resposta final com base nas respostas com pontuação mais alta de cada lote."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Solução 2: Recuperação QA (`RetrievalQA`)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um problema com o uso de `TODO` o texto é que pode ser `muito caro` porque você está alimentando todos os textos para a `API OpenAI` e a `API` é cobrada pelo número de tokens. Uma solução melhor é recuperar os blocos (`chunks`) de texto relevantes primeiro e usar apenas os blocos de texto relevantes no modelo de linguagem. A seguir, examinarei os detalhes do `RetrievalQA`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 2: <font color=\"red\">RetrievalQA</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A cadeia `RetrievalQA` realmente usa `load_qa_chain` sob o capô. Recuperamos o pedaço de texto mais relevante e o alimentamos no modelo de linguagem.</font>\n",
    "\n",
    "Opções (posteriormente descreveremos cada um delas):\n",
    "\n",
    "* [Embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html)\n",
    "\n",
    "* [TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)\n",
    "\n",
    "* [VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
    "\n",
    "* [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)\n",
    "\n",
    "  * [search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr): \"similarity\" or \"mmr\"\n",
    "\n",
    "\n",
    "* [Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html): \"stuff\", \"map reduce\", \"refine\", \"map_rerank\"   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font color=\"orange\">A seguir vamos ver o como funciona, RetrievalQA:</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# Dividir os documentos em chunks:\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Selecione qual EMBEDDINGS quer usar:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Crie o vectorestore para usar como índice (index):\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "\n",
    "# Exponha este índice em uma interface de recuperação:\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":1})\n",
    "\n",
    "\n",
    "# Crie uma cadeia para responder perguntas:\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True)\n",
    "\n",
    "query = \"Para que usam ngrok?\"\n",
    "result = qa({\"query\": query})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">No resultado, podemos ver a resposta e dois documentos de origem porque definimos `k como 2`, o que significa que estamos interessados ​​apenas em obter dois chunks de texto relevantes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Para que usam ngrok?',\n",
       " 'result': ' Ngrok é usado para criar um túnel seguro, atrás de Network Address Translation (NAT) e Firewalls, que expõem serviços locais (do computador) para a Internet, de forma fácil e segura.',\n",
       " 'source_documents': [Document(page_content='3 Principais passos para instanciar a API daVale\\nA seguir farei uma explanação dos principais passos para executar a nossa aplicação.\\n3.1 N grok\\nBasicamente, Ngrok é uma ferramenta CLI (Comand Line Interface) que te permite\\ncriar um túnel seguro, atrás de Network Address Translation (NAT) e Firewalls, que\\nexpõem serviços locais (do nosso computador) para a Internet, tudo isso de forma\\nfácil e segura. A seguinte imagem, Figura 3.1, descreve o uso do ngrok [1]:\\nFigura 3.1: Ngrok expondo um serviço local para a Internet.\\n3.1.1 I nstalando o Ngrok\\nO Ngrok é multiplataforma, ou seja, pode ser instalado no Linux ,Mac OS ou no\\nWindows . Então, para usar o Ngrok o primeiro a fazer é criar uma conta no ngrok.com\\ne seguidamente baixar o arquivo de instalação conforme seu sistema operacional (em\\nnossa aplicação estamos usando o Linux). Assim:\\n6', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 5})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ngrok é usado para criar um túnel seguro, atrás de Network Address Translation (NAT) e Firewalls, que expõem serviços locais (do computador) para a Internet, de forma fácil e segura.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Usando similaridade com score no CHROMA</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='•Para executar o seguinte comando devemos estar no diretório onde se encontra\\no arquivo docker-compose.yml . Ver a Figura 3.3, acima.\\n$ docker compose build\\nEste comando e basicamente usado para construir ou reconstruir as imagens\\nDocker definidas no arquivo docker-compose.yml.\\n•O seguinte comando é usado para construir, criar ou recriar e executar con-\\ntêineres para os serviços definidos no arquivo docker-compose.yml , também\\nconfigura as redes e os volumes especificados em dito arquivo.\\n$ docker compose up\\nEste comando além de iniciar os contêineres em um modo interativo e exibir\\na saída do console para cada contêiner, permite que você monitore o status do\\ncontêiner, visualize eventuais erros e saiba quando o serviço estiver pronto para\\nuso.\\nÉ importante aprender, também, como parar nossa aplicação. Então, para interromper\\nos contêineres de maneira segura, você pode executar (em outro terminal) o seguinte\\ncomando:\\n$ docker compose down\\nE para interromper abruptamente (ou em emergência) todos os contêineres em exe-\\ncução para os serviços definidos no arquivo docker-compose.yml , você pode usar o\\nseguinte comando:\\n$ docker compose kill\\nUm último comando, muito útil, é o uso do docker system prune . Este comando\\né usado para remover recursos não utilizados e liberar espaço em disco no sistema\\nDocker. Ele remove contêineres parados, redes não utilizadas, imagens sem tag e\\nvolumes sem anexos. Ao executar o comando docker system prune , o Docker irá\\nexibir uma mensagem de aviso informando que a ação irá remover permanentemente\\ntodos os recursos não utilizados. Se você deseja executar o comando sem confirmação,\\npode usar a opção -fou–force . Tenha cuidado no seu uso, já que isso significa que é\\npossível que você perca dados ou informações se remover recursos sem querer. Seu\\nuso na linha comando é:\\n$ docker system prune\\n9', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 8}),\n",
       "  0.4172041118144989),\n",
       " (Document(page_content='1 Introdu ¸c˜ao\\nSabemos que instanciar uma aplicação num servidor não é uma tarefa simples, já\\nque envolve diversos passos e requer um conhecimento aprofundado sobre as tec-\\nnologias e ferramentas envolvidas no processo. Para isso é fundamental levar em\\nconta alguns passos gerais, como por exemplo: escolher a infraestrutura onde será\\nhospedada a aplicação, escolher o Sistema Operacional (SO), o uso de Docker, o uso\\nde ngrok, instalar as dependências, configurar o ambiente (variáveis de ambiente,\\nconfiguração de Banco de Dados, etc), políticas de segurança, etc. Das mencionadas\\ndetalharei as que usaremos continuamente em nossa aplicação:\\nUsaremos a Dockerização porque nos ajudará a garantir a portabilidade e escalabili-\\ndade da aplicação. Este processo, de Dockerização, envolve a criação de um contêiner\\nque inclui todos os componentes necessários para executar a nossa aplicação. Isso\\npermitirá que a nossa aplicação seja executada em qualquer ambiente que suporte\\ncontêineres Docker.\\nUtilizaremos o aplicativo ngrok para acessar a nossa aplicação em execução em nossa\\nmáquina local (localhost) por meio de uma URL pública. Isso permitirá que outros\\nacessem em nossa aplicação por meio da internet.\\nEntão, com esses passos básicos conseguiremos rodar nossa aplicação e conseguire-\\nmos realizar as inferências que a aplicação envolve. Basicamente, a nossa aplicação\\nrealiza a segmentação de um áudio (Diarization), realiza a transcrição de um áudio\\n(speech-to-text) e identifica quem fala num determinado trecho de áudio (Speaker\\nIdentification).\\nO desenvolvimento de nossa aplicação foi baseada em duas bibliotecas de código\\naberto escritas na linguagem de Programação Python1para processamento de sinais\\nde áudio e fala. A primeira biblioteca é o Pyannote e a outra é o SpeechBrain. Ambas\\nbibliotecas foram projetadas para tarefas de Diarização, separação de fontes e síntese\\nde fala, reconhecimento de fala, etc.\\n1https: //www.python.org /\\n4', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 3}),\n",
       "  0.48988890647888184),\n",
       " (Document(page_content='3 Principais passos para instanciar a API daVale\\nA seguir farei uma explanação dos principais passos para executar a nossa aplicação.\\n3.1 N grok\\nBasicamente, Ngrok é uma ferramenta CLI (Comand Line Interface) que te permite\\ncriar um túnel seguro, atrás de Network Address Translation (NAT) e Firewalls, que\\nexpõem serviços locais (do nosso computador) para a Internet, tudo isso de forma\\nfácil e segura. A seguinte imagem, Figura 3.1, descreve o uso do ngrok [1]:\\nFigura 3.1: Ngrok expondo um serviço local para a Internet.\\n3.1.1 I nstalando o Ngrok\\nO Ngrok é multiplataforma, ou seja, pode ser instalado no Linux ,Mac OS ou no\\nWindows . Então, para usar o Ngrok o primeiro a fazer é criar uma conta no ngrok.com\\ne seguidamente baixar o arquivo de instalação conforme seu sistema operacional (em\\nnossa aplicação estamos usando o Linux). Assim:\\n6', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 5}),\n",
       "  0.492635041475296),\n",
       " (Document(page_content='Resumo\\nNeste conciso tutorial aprenderemos a instanciar a nossa API de Análise\\nde Comunicação de Voz (ACV) da Vale. Mostrarei os passos necessários\\ne suficientes para poder executar a nossa ferramenta, comandos úteis de\\nDockerização, a como expor um serviço local na Internet com ngrok e o\\nuso de outras principais funcionalidades.\\n2', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 1}),\n",
       "  0.4965440034866333)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividir os documentos em chunks:\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Selecione qual EMBEDDINGS quer usar:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Crie o vectorestore para usar como índece (index):\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "docs_score = db.similarity_search_with_score(\"O que é system prune?\")\n",
    "\n",
    "docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='Resumo\\nNeste conciso tutorial aprenderemos a instanciar a nossa API de Análise\\nde Comunicação de Voz (ACV) da Vale. Mostrarei os passos necessários\\ne suficientes para poder executar a nossa ferramenta, comandos úteis de\\nDockerização, a como expor um serviço local na Internet com ngrok e o\\nuso de outras principais funcionalidades.\\n2', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 1}),\n",
       " 0.4965440034866333)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value = max(docs_score, key=lambda x: x[1])\n",
    "max_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem várias opções para você escolher neste processo:\n",
    "\n",
    "\n",
    "🤗 [embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html): No exemplo, usamos `OpenAI Embeddings`. Mas existem muitas outras opções de Embeddings, como `Cohere Embeddings` e `HuggingFaceEmbeddings` de modelos específicos.\n",
    "\n",
    "🤗 [TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html): Usamos o `Character Text Splitter` no exemplo em que o texto é dividido por um único caractere. Você também pode usar diferentes divisores de texto e diferentes `tokens` mencionados neste documento.\n",
    "\n",
    "🤗 [VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html): Usamos o `Chroma` como nosso `banco de dados de vetores`, onde armazenamos nossos vetores de texto incorporados. Outras opções populares são `FAISS`, `Mulvus` e `Pinecone`.\n",
    "\n",
    "🤗 [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html): usamos um `VectoreStoreRetriver`, que é apoiado por um `VectorStore`. Para recuperar o texto, existem dois tipos de pesquisa que você pode escolher: [search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr):\"similarity\" ou “mmr”. `search_type=\"similarity\"` usa pesquisa de similaridade no objeto retriever, onde seleciona vetores de blocos (chunk) de texto que são mais similares ao vetor de pergunta. `search_type=\"mmr\"` usa a pesquisa de `relevância marginal máxima` (mmr - maximum marginal relevance) onde otimiza a similaridade para query `AND` a diversidade entre os documentos selecionados.\n",
    "\n",
    "🤗 [Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html): igual ao `método 1`. Você também pode definir o tipo de cadeia como uma das quatro opções: `“stuff”`, `“map reduce”`, `“refine”`, `“map_rerank”`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 3: <font color=\"red\">VectorstoreIndexCreator</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorstoreIndexCreator` é um `wrapper` em torno da funcionalidade acima. É exatamente o mesmo sob o capô, mas apenas expõe uma interface de nível superior para permitir que você comece em três linhas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' O objetivo do trabalho de pesquisa foi conhecer e aplicar os comandos necessários para instanciar a aplicação da Vale, que realiza a segmentação de um áudio (Diarization), a transcrição de um áudio (speech-to-text) e a identificação de quem fala num determinado trecho de áudio (Speaker Identification).'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "query = \"Qual foi o objetivo do trabalho de pesquisa?\"\n",
    "\n",
    "index.query(llm=OpenAI(),\n",
    "            question=query,\n",
    "            chain_type=\"map_reduce\"\n",
    "           )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Claro, você também pode especificar diferentes opções neste `wrapper`:\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Dr.Eddy Giusepe Chirinos Isidro'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    # Dividir os documentos em chunks\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),\n",
    "    # Selecione os Embeddings que quer usar:\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    # Use Chroma como o vectorestore para indexar e pesquisar Embeddings:\n",
    "    vectorstore_cls=Chroma\n",
    ").from_loaders([loader])\n",
    "\n",
    "\n",
    "query = \"Quem foi o Cientista de Dados?\"\n",
    "\n",
    "index.query(llm=OpenAI(),\n",
    "            question=query,\n",
    "            chain_type= \"map_rerank\" #\"stuff\" # Usando `map_reduce` não trouxe resposta a minha pergunta.` \n",
    "           )\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 4: <font color=\"red\">ConversationalRetrievalChain</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConversationalRetrievalChain` é muito semelhante ao `método 2 RetrievalQA`. Ele adicionou um parâmetro adicional `chat_history` para passar no histórico do bate-papo, que pode ser usado para perguntas de acompanhamento.\n",
    "\n",
    "```\n",
    "ConversationalRetrievalChain = memória de conversação + RetrievalQAChain\n",
    "```\n",
    "\n",
    "<font color=\"yellow\">Se você quiser que seu modelo de linguagem tenha uma memória da conversa anterior, use este método.</font> \n",
    "\n",
    "\n",
    "No meu exemplo a seguir, perguntei para que foi importante aprender certos comandos no trabalho de pesquisa?. Como ele tem todo o histórico do chat, me trouxe uma boa resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# Carregamos nosso documento:\n",
    "loader = PyPDFLoader(\"./Eddy_Running_the_Vale_API.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Dividir os Documentosem Chunks:\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000,\n",
    "                                      chunk_overlap=0\n",
    "                                     )\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Seleciono os Embeddings que queremos usar:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Criamos o vectorestore para usar como índice:\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Exponha este índice em uma interface de recuperação:\n",
    "retriever = db.as_retriever(search_type=\"similarity\",\n",
    "                            search_kwargs={\"k\":2}\n",
    "                           )\n",
    "\n",
    "# Criamos uma cadeia (chain) para responder a perguntas:  \n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(),\n",
    "                                           retriever\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"Qual foi o objetivo do trabalho de pesquisa?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Qual foi o objetivo do trabalho de pesquisa?',\n",
       " 'chat_history': [],\n",
       " 'answer': ' Conhecer e aplicar os comandos necessários para instanciar a aplicação da Vale.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Conhecer e aplicar os comandos necessários para instanciar a aplicação da Vale.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"Com os comandos aprendidos se instanciou o que?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Qual foi o objetivo do trabalho de pesquisa?',\n",
       "  ' Conhecer e aplicar os comandos necessários para instanciar a aplicação da Vale.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A API de Análise de Comunicação de Voz (ACV) da Vale.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Agora você conhece quatro maneiras de responder a perguntas com `LLMs` no `LangChain`. Em resumo, `load_qa_chain` usa todos os textos e aceita vários documentos; O `RetrievalQA` usa **load_qa_chain** sob o capô, mas recupera primeiro os blocos de texto relevantes; `VectorstoreIndexCreator` é o mesmo que **RetrievalQA** com uma interface de nível superior; `ConversationalRetrievalChain` é útil quando você deseja passar seu `histórico de bate-papo` para o modelo.</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks God!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
