{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Quatro maneiras de Question-Answering in LangChain</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script est√° baseado nos trabalhos da [Sophia Yang]()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converse com seus longos documentos `PDF`: load_qa_chain, RetrievalQA, VectorstoreIndexCreator, ConversationalRetrievalChain.\n",
    "\n",
    "\n",
    "Voc√™ est√° interessado em conversar com seus pr√≥prios documentos, seja um `arquivo de texto`, um `PDF` ou um `site`? O `LangChain` torna mais f√°cil para voc√™ responder a perguntas com seus documentos. Mas voc√™ sabia que existem pelo menos `4` maneiras de responder a perguntas no LangChain? Nesta postagem do blog, exploraremos quatro maneiras diferentes de responder a perguntas e as v√°rias op√ß√µes que voc√™ pode considerar para seus casos de uso.\n",
    "\n",
    "S√≥ lembrando que √© `LangChain`, na opini√£o de `Sophia Yang`, √© a maneira mais f√°cil de interagir com modelos de linguagem e criar aplicativos. √â uma ferramenta de c√≥digo aberto que envolve muitos `LLMs` e ferramentas. \n",
    "\n",
    "Ok, agora vamos come√ßar a responder perguntas em documentos externos.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando a API da OpenAI e importando as Bibliotecas necess√°rias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a [API OpenAI](https://platform.openai.com/account) n√£o √© gratuita. Voc√™ precisar√° configurar as informa√ß√µes de cobran√ßa para poder usar a `API OpenAI`. Como alternativa, voc√™ pode usar modelos do `HuggingFace Hub` ou de outros lugares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain==0.0.137 openai chromadb tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto √© quando usas o arquivo .env:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA # Tem que atualizar --> pip install langchai==0.0.137\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando Documentos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `LangChain` suporta muitos [carregadores de documentos](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html), como `Notion`, `YouTube` e `Figma`. Neste exemplo, gostaria de conversar com meu arquivo `PDF`. Assim, usei o `PyPDFLoader` para carregar meu arquivo.\n",
    "\n",
    "Eu recomendo usar um PDF pequeno. Eu usei, s√≥ para demonstra√ß√£o mesmo, um boleto de pagamento da minha conta de energia e um boleto de pagamento de mensalidade da minha faculdade que cont√©m uma p√°gina cada boleto (voc√™ pode usar qualquer outro documento). Meu boleto cont√©m certas entidades e em base a elas farei certas perguntas! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "O que o oceano disse para o outro oceano? Nada, ele s√≥ estava ondulando.\n"
     ]
    }
   ],
   "source": [
    "# Verificamos nosso Modelo Instanciado:\n",
    "\n",
    "llm = OpenAI()\n",
    "print(llm(\"Conte-me uma piada.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando meu documento (Boleto de energia):\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# loader = PyPDFLoader(\"./Karina_boleto_FACULDADE.pdf\")  # PDF apenas com uma p√°gina\n",
    "loader = PyPDFLoader(\"./Eddy_Running_the_Vale_API.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='UFES - L abVISIO - V aleS.A.\\nInstanciando a API de An√°lise de\\nComunica√ß√£o de Voz (ACV)\\nData Scientist.: Dr.Eddy Giusepe Chirinos Isidro\\n9 de mar√ßo de 2023\\n1', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 0}),\n",
       " Document(page_content='Resumo\\nNeste conciso tutorial aprenderemos a instanciar a nossa API de An√°lise\\nde Comunica√ß√£o de Voz (ACV) da Vale. Mostrarei os passos necess√°rios\\ne suficientes para poder executar a nossa ferramenta, comandos √∫teis de\\nDockeriza√ß√£o, a como expor um servi√ßo local na Internet com ngrok e o\\nuso de outras principais funcionalidades.\\n2', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 1}),\n",
       " Document(page_content='Conte ¬¥udo\\n1 Introdu√ß√£o 4\\n2 Objetivo 5\\n3 Principais passos para instanciar a API da Vale 6\\n3.1 Ngrok . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n3.1.1 Instalando o Ngrok . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n3.2 docker compose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n3', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 2}),\n",
       " Document(page_content='1 Introdu ¬∏cÀúao\\nSabemos que instanciar uma aplica√ß√£o num servidor n√£o √© uma tarefa simples, j√°\\nque envolve diversos passos e requer um conhecimento aprofundado sobre as tec-\\nnologias e ferramentas envolvidas no processo. Para isso √© fundamental levar em\\nconta alguns passos gerais, como por exemplo: escolher a infraestrutura onde ser√°\\nhospedada a aplica√ß√£o, escolher o Sistema Operacional (SO), o uso de Docker, o uso\\nde ngrok, instalar as depend√™ncias, configurar o ambiente (vari√°veis de ambiente,\\nconfigura√ß√£o de Banco de Dados, etc), pol√≠ticas de seguran√ßa, etc. Das mencionadas\\ndetalharei as que usaremos continuamente em nossa aplica√ß√£o:\\nUsaremos a Dockeriza√ß√£o porque nos ajudar√° a garantir a portabilidade e escalabili-\\ndade da aplica√ß√£o. Este processo, de Dockeriza√ß√£o, envolve a cria√ß√£o de um cont√™iner\\nque inclui todos os componentes necess√°rios para executar a nossa aplica√ß√£o. Isso\\npermitir√° que a nossa aplica√ß√£o seja executada em qualquer ambiente que suporte\\ncont√™ineres Docker.\\nUtilizaremos o aplicativo ngrok para acessar a nossa aplica√ß√£o em execu√ß√£o em nossa\\nm√°quina local (localhost) por meio de uma URL p√∫blica. Isso permitir√° que outros\\nacessem em nossa aplica√ß√£o por meio da internet.\\nEnt√£o, com esses passos b√°sicos conseguiremos rodar nossa aplica√ß√£o e conseguire-\\nmos realizar as infer√™ncias que a aplica√ß√£o envolve. Basicamente, a nossa aplica√ß√£o\\nrealiza a segmenta√ß√£o de um √°udio (Diarization), realiza a transcri√ß√£o de um √°udio\\n(speech-to-text) e identifica quem fala num determinado trecho de √°udio (Speaker\\nIdentification).\\nO desenvolvimento de nossa aplica√ß√£o foi baseada em duas bibliotecas de c√≥digo\\naberto escritas na linguagem de Programa√ß√£o Python1para processamento de sinais\\nde √°udio e fala. A primeira biblioteca √© o Pyannote e a outra √© o SpeechBrain. Ambas\\nbibliotecas foram projetadas para tarefas de Diariza√ß√£o, separa√ß√£o de fontes e s√≠ntese\\nde fala, reconhecimento de fala, etc.\\n1https: //www.python.org /\\n4', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 3}),\n",
       " Document(page_content='2 Objetivo\\n‚Ä¢Conhecer e aplicar os comandos necess√°rios para instanciar a aplica√ß√£o da Vale.\\n5', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 4}),\n",
       " Document(page_content='3 Principais passos para instanciar a API daVale\\nA seguir farei uma explana√ß√£o dos principais passos para executar a nossa aplica√ß√£o.\\n3.1 N grok\\nBasicamente, Ngrok √© uma ferramenta CLI (Comand Line Interface) que te permite\\ncriar um t√∫nel seguro, atr√°s de Network Address Translation (NAT) e Firewalls, que\\nexp√µem servi√ßos locais (do nosso computador) para a Internet, tudo isso de forma\\nf√°cil e segura. A seguinte imagem, Figura 3.1, descreve o uso do ngrok [1]:\\nFigura 3.1: Ngrok expondo um servi√ßo local para a Internet.\\n3.1.1 I nstalando o Ngrok\\nO Ngrok √© multiplataforma, ou seja, pode ser instalado no Linux ,Mac OS ou no\\nWindows . Ent√£o, para usar o Ngrok o primeiro a fazer √© criar uma conta no ngrok.com\\ne seguidamente baixar o arquivo de instala√ß√£o conforme seu sistema operacional (em\\nnossa aplica√ß√£o estamos usando o Linux). Assim:\\n6', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 5}),\n",
       " Document(page_content='‚Ä¢Descompacte o arquivo .tgz para instalar\\nNo Linux ou Mac OS, voc√™ pode descompactar o ngrok de um Terminal com o\\nseguinte comando:\\n$ tar xvzf ngrok-v3-stable-linux-amd64.tgz\\n‚Ä¢Conecte sua conta\\nA execu√ß√£o desse comando adicionar√° seu token de autentica√ß√£o ao arquivo de\\nconfigura√ß√£o padr√£o. Isso conceder√° a voc√™ mais recursos e tempos de sess√£o\\nmais longos. Voc√™ copia seu token e cola na linha de comando a seguir (o Token\\nabaixo √© fict√≠cio)\\n$ ngrok config add-authtoken ##olaDr.EddyUFESgdfodf#_22343aSSDFT_GOD!BMB\\n‚Ä¢Criando um T√∫nel\\nAgora, o pr√≥ximo passo √© criar um t√∫nel na porta 1337 ( porta na qual nossa\\naplica√ß√£o est√° sendo executada, localmente ) expondo esse servidor web via ngrok,\\npara isso executamos em outro terminal o seguinte comando:\\n$ ngrok http 1337\\nA seguinte Figura 3.2, mostra onde baixar o arquivo de instala√ß√£o do ngrok e mostra\\nno lado esquerdo superior de onde voc√™ pode pegar seu Token ( Your Authtoken ).\\nFigura 3.2: Download e Token para o uso do ngrok.\\n7', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 6}),\n",
       " Document(page_content='3.2docker compose\\nDocker Compose √© uma ferramenta do Docker [2,3] que permite definir e executar\\naplicativos multi-container Docker. Ele √© usado para definir servi√ßos, redes e volumes\\npara v√°rios cont√™ineres como um √∫nico aplicativo. O Docker Compose permite que\\nvoc√™ configure e inicie rapidamente v√°rios cont√™ineres Docker juntos, em um ambiente\\nisolado e definido por voc√™.\\nA seguinte Figura 3.3, mostra os servi√ßos da nossa aplica√ß√£o.\\nFigura 3.3: Arquivo docker-compose.yml contendo nossos servi√ßos.\\nEnt√£o, para poder instanciar os tr√™s servi√ßos2da nossa aplica√ß√£o precisamos executar,\\nna seguinte ordem e no Terminal, os seguintes comandos:\\n2Nossos servi√ßos s√£o: acv-backend ,acv-frontend eacv-models . Obviamente cada servi√ßo tem sua\\npr√≥pria Imagem (Dockerfile), a qual instancia seu pr√≥prio cont√™iner.\\n8', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 7}),\n",
       " Document(page_content='‚Ä¢Para executar o seguinte comando devemos estar no diret√≥rio onde se encontra\\no arquivo docker-compose.yml . Ver a Figura 3.3, acima.\\n$ docker compose build\\nEste comando e basicamente usado para construir ou reconstruir as imagens\\nDocker definidas no arquivo docker-compose.yml.\\n‚Ä¢O seguinte comando √© usado para construir, criar ou recriar e executar con-\\nt√™ineres para os servi√ßos definidos no arquivo docker-compose.yml , tamb√©m\\nconfigura as redes e os volumes especificados em dito arquivo.\\n$ docker compose up\\nEste comando al√©m de iniciar os cont√™ineres em um modo interativo e exibir\\na sa√≠da do console para cada cont√™iner, permite que voc√™ monitore o status do\\ncont√™iner, visualize eventuais erros e saiba quando o servi√ßo estiver pronto para\\nuso.\\n√â importante aprender, tamb√©m, como parar nossa aplica√ß√£o. Ent√£o, para interromper\\nos cont√™ineres de maneira segura, voc√™ pode executar (em outro terminal) o seguinte\\ncomando:\\n$ docker compose down\\nE para interromper abruptamente (ou em emerg√™ncia) todos os cont√™ineres em exe-\\ncu√ß√£o para os servi√ßos definidos no arquivo docker-compose.yml , voc√™ pode usar o\\nseguinte comando:\\n$ docker compose kill\\nUm √∫ltimo comando, muito √∫til, √© o uso do docker system prune . Este comando\\n√© usado para remover recursos n√£o utilizados e liberar espa√ßo em disco no sistema\\nDocker. Ele remove cont√™ineres parados, redes n√£o utilizadas, imagens sem tag e\\nvolumes sem anexos. Ao executar o comando docker system prune , o Docker ir√°\\nexibir uma mensagem de aviso informando que a a√ß√£o ir√° remover permanentemente\\ntodos os recursos n√£o utilizados. Se voc√™ deseja executar o comando sem confirma√ß√£o,\\npode usar a op√ß√£o -fou‚Äìforce . Tenha cuidado no seu uso, j√° que isso significa que √©\\nposs√≠vel que voc√™ perca dados ou informa√ß√µes se remover recursos sem querer. Seu\\nuso na linha comando √©:\\n$ docker system prune\\n9', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 8}),\n",
       " Document(page_content='Refer ÀÜencias\\n[1] J. R. da Paix√£o, ‚ÄúNgrok: do localhost para o mundo.‚Äù https: //medium.com /\\ndesenvolvendo-com-paixao /ngrok-do-localhost-para-o-mundo-5445ad08419,\\n2021. Cited in page: 6.\\n[2] ‚ÄúDocker engine.‚Äù https: //docs.docker.com /engine /install /ubuntu /, 2023. Cited in\\npage: 8.\\n[3] E. G. C. Isidro, ‚ÄúGuia para iniciantes em docker.‚Äù https: //drive.google.com /file /d/\\n1S-azDR2jFvzDKMuPZJH4uGjefZEwM5Km /view, 2022. Cited in page: 8.\\n10', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©todo 1: <font color=\"red\">load_qa_chain</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_qa_chain` fornece a interface mais gen√©rica para responder a perguntas. Ele carrega uma cadeia que voc√™ pode fazer `QA` para seus documentos de entrada e usa `TODO o texto` nos documentos.\n",
    "\n",
    "`chain_type=\"stuff\"` n√£o funcionar√° porque o n√∫mero de tokens excede o limite (isso para o caso do PDF da Sophia). Podemos tentar outros tipos de cadeia como `\"map_reduce\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A pesquisa foi feita na UFES.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "\n",
    "chain = load_qa_chain(llm=OpenAI(),\n",
    "                      chain_type=\"stuff\"\n",
    "                     )\n",
    "\n",
    "\n",
    "query = \"A pesquisa foi feita em que universidade?\"\n",
    "\n",
    "chain.run(input_documents=documents,\n",
    "          question=query\n",
    "         )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ele tamb√©m permite que voc√™ fa√ßa QA em um conjunto de documentos:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Para v√°rios Documentos:\n",
    "# loaders = [....]\n",
    "# documents = []\n",
    "# for loader in loaders:\n",
    "#     documents.extend(loader.load())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">ü§î Mas e se meu documento for muito longo e ultrapassar o limite de token?</font>\n",
    "\n",
    "Existem duas maneiras de corrigi-lo:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Solu√ß√£o 1: Tipo de Cadeia (chain)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O padr√£o `chain_type=\"stuff\"` usa `TODO` o texto dos documentos no prompt. Se voc√™ tiver um arquivo muito grande (`tal como fez Sophia Yang, com um PDF de `50` p√°ginas`) n√£o funcionar√° porque excede o `limite de token` e causa erros de limita√ß√£o de taxa. √â por isso que para o exemplo da Sophia Yang, ela teve que usar outro tipo de cadeia, por exemplo `\"map_reduce\"`. \n",
    "\n",
    "A seguir vamos estudar outros tipos de cadeias:\n",
    "\n",
    "ü§© `map_reduce:` Separa os textos em lotes (<font color=\"red\">por exemplo</font>, voc√™ pode definir o tamanho do lote em `llm=OpenAI(batch_size=5)`), alimenta cada lote com a pergunta para o `LLM` separadamente e apresenta a resposta final com base nas respostas de cada lote.\n",
    "\n",
    "ü§© `refine:` Separa os textos em lotes, alimenta o primeiro lote para o LLM e alimenta a resposta e o segundo lote para o LLM. Ele refina a resposta passando por todos os lotes.\n",
    "\n",
    "ü§© `map_rerank:` ele separa os textos em lotes, alimenta cada lote para o LLM, retorna uma pontua√ß√£o de como responde completamente √† pergunta e apresenta a resposta final com base nas respostas com pontua√ß√£o mais alta de cada lote."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Solu√ß√£o 2: Recupera√ß√£o QA (`RetrievalQA`)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um problema com o uso de `TODO` o texto √© que pode ser `muito caro` porque voc√™ est√° alimentando todos os textos para a `API OpenAI` e a `API` √© cobrada pelo n√∫mero de tokens. Uma solu√ß√£o melhor √© recuperar os blocos (`chunks`) de texto relevantes primeiro e usar apenas os blocos de texto relevantes no modelo de linguagem. A seguir, examinarei os detalhes do `RetrievalQA`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©todo 2: <font color=\"red\">RetrievalQA</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A cadeia `RetrievalQA` realmente usa `load_qa_chain` sob o cap√¥. Recuperamos o peda√ßo de texto mais relevante e o alimentamos no modelo de linguagem.</font>\n",
    "\n",
    "Op√ß√µes (posteriormente descreveremos cada um delas):\n",
    "\n",
    "* [Embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html)\n",
    "\n",
    "* [TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)\n",
    "\n",
    "* [VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
    "\n",
    "* [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)\n",
    "\n",
    "  * [search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr): \"similarity\" or \"mmr\"\n",
    "\n",
    "\n",
    "* [Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html): \"stuff\", \"map reduce\", \"refine\", \"map_rerank\"   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font color=\"orange\">A seguir vamos ver o como funciona, RetrievalQA:</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# Dividir os documentos em chunks:\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Selecione qual EMBEDDINGS quer usar:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Crie o vectorestore para usar como √≠ndice (index):\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "\n",
    "# Exponha este √≠ndice em uma interface de recupera√ß√£o:\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":1})\n",
    "\n",
    "\n",
    "# Crie uma cadeia para responder perguntas:\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True)\n",
    "\n",
    "query = \"Para que usam ngrok?\"\n",
    "result = qa({\"query\": query})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">No resultado, podemos ver a resposta e dois documentos de origem porque definimos `k como 2`, o que significa que estamos interessados ‚Äã‚Äãapenas em obter dois chunks de texto relevantes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Para que usam ngrok?',\n",
       " 'result': ' Ngrok √© usado para criar um t√∫nel seguro, atr√°s de Network Address Translation (NAT) e Firewalls, que exp√µem servi√ßos locais (do computador) para a Internet, de forma f√°cil e segura.',\n",
       " 'source_documents': [Document(page_content='3 Principais passos para instanciar a API daVale\\nA seguir farei uma explana√ß√£o dos principais passos para executar a nossa aplica√ß√£o.\\n3.1 N grok\\nBasicamente, Ngrok √© uma ferramenta CLI (Comand Line Interface) que te permite\\ncriar um t√∫nel seguro, atr√°s de Network Address Translation (NAT) e Firewalls, que\\nexp√µem servi√ßos locais (do nosso computador) para a Internet, tudo isso de forma\\nf√°cil e segura. A seguinte imagem, Figura 3.1, descreve o uso do ngrok [1]:\\nFigura 3.1: Ngrok expondo um servi√ßo local para a Internet.\\n3.1.1 I nstalando o Ngrok\\nO Ngrok √© multiplataforma, ou seja, pode ser instalado no Linux ,Mac OS ou no\\nWindows . Ent√£o, para usar o Ngrok o primeiro a fazer √© criar uma conta no ngrok.com\\ne seguidamente baixar o arquivo de instala√ß√£o conforme seu sistema operacional (em\\nnossa aplica√ß√£o estamos usando o Linux). Assim:\\n6', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 5})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ngrok √© usado para criar um t√∫nel seguro, atr√°s de Network Address Translation (NAT) e Firewalls, que exp√µem servi√ßos locais (do computador) para a Internet, de forma f√°cil e segura.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Usando similaridade com score no CHROMA</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='‚Ä¢Para executar o seguinte comando devemos estar no diret√≥rio onde se encontra\\no arquivo docker-compose.yml . Ver a Figura 3.3, acima.\\n$ docker compose build\\nEste comando e basicamente usado para construir ou reconstruir as imagens\\nDocker definidas no arquivo docker-compose.yml.\\n‚Ä¢O seguinte comando √© usado para construir, criar ou recriar e executar con-\\nt√™ineres para os servi√ßos definidos no arquivo docker-compose.yml , tamb√©m\\nconfigura as redes e os volumes especificados em dito arquivo.\\n$ docker compose up\\nEste comando al√©m de iniciar os cont√™ineres em um modo interativo e exibir\\na sa√≠da do console para cada cont√™iner, permite que voc√™ monitore o status do\\ncont√™iner, visualize eventuais erros e saiba quando o servi√ßo estiver pronto para\\nuso.\\n√â importante aprender, tamb√©m, como parar nossa aplica√ß√£o. Ent√£o, para interromper\\nos cont√™ineres de maneira segura, voc√™ pode executar (em outro terminal) o seguinte\\ncomando:\\n$ docker compose down\\nE para interromper abruptamente (ou em emerg√™ncia) todos os cont√™ineres em exe-\\ncu√ß√£o para os servi√ßos definidos no arquivo docker-compose.yml , voc√™ pode usar o\\nseguinte comando:\\n$ docker compose kill\\nUm √∫ltimo comando, muito √∫til, √© o uso do docker system prune . Este comando\\n√© usado para remover recursos n√£o utilizados e liberar espa√ßo em disco no sistema\\nDocker. Ele remove cont√™ineres parados, redes n√£o utilizadas, imagens sem tag e\\nvolumes sem anexos. Ao executar o comando docker system prune , o Docker ir√°\\nexibir uma mensagem de aviso informando que a a√ß√£o ir√° remover permanentemente\\ntodos os recursos n√£o utilizados. Se voc√™ deseja executar o comando sem confirma√ß√£o,\\npode usar a op√ß√£o -fou‚Äìforce . Tenha cuidado no seu uso, j√° que isso significa que √©\\nposs√≠vel que voc√™ perca dados ou informa√ß√µes se remover recursos sem querer. Seu\\nuso na linha comando √©:\\n$ docker system prune\\n9', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 8}),\n",
       "  0.4172041118144989),\n",
       " (Document(page_content='1 Introdu ¬∏cÀúao\\nSabemos que instanciar uma aplica√ß√£o num servidor n√£o √© uma tarefa simples, j√°\\nque envolve diversos passos e requer um conhecimento aprofundado sobre as tec-\\nnologias e ferramentas envolvidas no processo. Para isso √© fundamental levar em\\nconta alguns passos gerais, como por exemplo: escolher a infraestrutura onde ser√°\\nhospedada a aplica√ß√£o, escolher o Sistema Operacional (SO), o uso de Docker, o uso\\nde ngrok, instalar as depend√™ncias, configurar o ambiente (vari√°veis de ambiente,\\nconfigura√ß√£o de Banco de Dados, etc), pol√≠ticas de seguran√ßa, etc. Das mencionadas\\ndetalharei as que usaremos continuamente em nossa aplica√ß√£o:\\nUsaremos a Dockeriza√ß√£o porque nos ajudar√° a garantir a portabilidade e escalabili-\\ndade da aplica√ß√£o. Este processo, de Dockeriza√ß√£o, envolve a cria√ß√£o de um cont√™iner\\nque inclui todos os componentes necess√°rios para executar a nossa aplica√ß√£o. Isso\\npermitir√° que a nossa aplica√ß√£o seja executada em qualquer ambiente que suporte\\ncont√™ineres Docker.\\nUtilizaremos o aplicativo ngrok para acessar a nossa aplica√ß√£o em execu√ß√£o em nossa\\nm√°quina local (localhost) por meio de uma URL p√∫blica. Isso permitir√° que outros\\nacessem em nossa aplica√ß√£o por meio da internet.\\nEnt√£o, com esses passos b√°sicos conseguiremos rodar nossa aplica√ß√£o e conseguire-\\nmos realizar as infer√™ncias que a aplica√ß√£o envolve. Basicamente, a nossa aplica√ß√£o\\nrealiza a segmenta√ß√£o de um √°udio (Diarization), realiza a transcri√ß√£o de um √°udio\\n(speech-to-text) e identifica quem fala num determinado trecho de √°udio (Speaker\\nIdentification).\\nO desenvolvimento de nossa aplica√ß√£o foi baseada em duas bibliotecas de c√≥digo\\naberto escritas na linguagem de Programa√ß√£o Python1para processamento de sinais\\nde √°udio e fala. A primeira biblioteca √© o Pyannote e a outra √© o SpeechBrain. Ambas\\nbibliotecas foram projetadas para tarefas de Diariza√ß√£o, separa√ß√£o de fontes e s√≠ntese\\nde fala, reconhecimento de fala, etc.\\n1https: //www.python.org /\\n4', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 3}),\n",
       "  0.48988890647888184),\n",
       " (Document(page_content='3 Principais passos para instanciar a API daVale\\nA seguir farei uma explana√ß√£o dos principais passos para executar a nossa aplica√ß√£o.\\n3.1 N grok\\nBasicamente, Ngrok √© uma ferramenta CLI (Comand Line Interface) que te permite\\ncriar um t√∫nel seguro, atr√°s de Network Address Translation (NAT) e Firewalls, que\\nexp√µem servi√ßos locais (do nosso computador) para a Internet, tudo isso de forma\\nf√°cil e segura. A seguinte imagem, Figura 3.1, descreve o uso do ngrok [1]:\\nFigura 3.1: Ngrok expondo um servi√ßo local para a Internet.\\n3.1.1 I nstalando o Ngrok\\nO Ngrok √© multiplataforma, ou seja, pode ser instalado no Linux ,Mac OS ou no\\nWindows . Ent√£o, para usar o Ngrok o primeiro a fazer √© criar uma conta no ngrok.com\\ne seguidamente baixar o arquivo de instala√ß√£o conforme seu sistema operacional (em\\nnossa aplica√ß√£o estamos usando o Linux). Assim:\\n6', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 5}),\n",
       "  0.492635041475296),\n",
       " (Document(page_content='Resumo\\nNeste conciso tutorial aprenderemos a instanciar a nossa API de An√°lise\\nde Comunica√ß√£o de Voz (ACV) da Vale. Mostrarei os passos necess√°rios\\ne suficientes para poder executar a nossa ferramenta, comandos √∫teis de\\nDockeriza√ß√£o, a como expor um servi√ßo local na Internet com ngrok e o\\nuso de outras principais funcionalidades.\\n2', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 1}),\n",
       "  0.4965440034866333)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividir os documentos em chunks:\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Selecione qual EMBEDDINGS quer usar:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Crie o vectorestore para usar como √≠ndece (index):\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "docs_score = db.similarity_search_with_score(\"O que √© system prune?\")\n",
    "\n",
    "docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='Resumo\\nNeste conciso tutorial aprenderemos a instanciar a nossa API de An√°lise\\nde Comunica√ß√£o de Voz (ACV) da Vale. Mostrarei os passos necess√°rios\\ne suficientes para poder executar a nossa ferramenta, comandos √∫teis de\\nDockeriza√ß√£o, a como expor um servi√ßo local na Internet com ngrok e o\\nuso de outras principais funcionalidades.\\n2', metadata={'source': './Eddy_Running_the_Vale_API.pdf', 'page': 1}),\n",
       " 0.4965440034866333)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value = max(docs_score, key=lambda x: x[1])\n",
    "max_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Op√ß√µes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem v√°rias op√ß√µes para voc√™ escolher neste processo:\n",
    "\n",
    "\n",
    "ü§ó [embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html): No exemplo, usamos `OpenAI Embeddings`. Mas existem muitas outras op√ß√µes de Embeddings, como `Cohere Embeddings` e `HuggingFaceEmbeddings` de modelos espec√≠ficos.\n",
    "\n",
    "ü§ó [TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html): Usamos o `Character Text Splitter` no exemplo em que o texto √© dividido por um √∫nico caractere. Voc√™ tamb√©m pode usar diferentes divisores de texto e diferentes `tokens` mencionados neste documento.\n",
    "\n",
    "ü§ó [VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html): Usamos o `Chroma` como nosso `banco de dados de vetores`, onde armazenamos nossos vetores de texto incorporados. Outras op√ß√µes populares s√£o `FAISS`, `Mulvus` e `Pinecone`.\n",
    "\n",
    "ü§ó [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html): usamos um `VectoreStoreRetriver`, que √© apoiado por um `VectorStore`. Para recuperar o texto, existem dois tipos de pesquisa que voc√™ pode escolher: [search_type](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html#mmr):\"similarity\" ou ‚Äúmmr‚Äù. `search_type=\"similarity\"` usa pesquisa de similaridade no objeto retriever, onde seleciona vetores de blocos (chunk) de texto que s√£o mais similares ao vetor de pergunta. `search_type=\"mmr\"` usa a pesquisa de `relev√¢ncia marginal m√°xima` (mmr - maximum marginal relevance) onde otimiza a similaridade para query `AND` a diversidade entre os documentos selecionados.\n",
    "\n",
    "ü§ó [Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html): igual ao `m√©todo 1`. Voc√™ tamb√©m pode definir o tipo de cadeia como uma das quatro op√ß√µes: `‚Äústuff‚Äù`, `‚Äúmap reduce‚Äù`, `‚Äúrefine‚Äù`, `‚Äúmap_rerank‚Äù`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©todo 3: <font color=\"red\">VectorstoreIndexCreator</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorstoreIndexCreator` √© um `wrapper` em torno da funcionalidade acima. √â exatamente o mesmo sob o cap√¥, mas apenas exp√µe uma interface de n√≠vel superior para permitir que voc√™ comece em tr√™s linhas de c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' O objetivo do trabalho de pesquisa foi conhecer e aplicar os comandos necess√°rios para instanciar a aplica√ß√£o da Vale, que realiza a segmenta√ß√£o de um √°udio (Diarization), a transcri√ß√£o de um √°udio (speech-to-text) e a identifica√ß√£o de quem fala num determinado trecho de √°udio (Speaker Identification).'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "query = \"Qual foi o objetivo do trabalho de pesquisa?\"\n",
    "\n",
    "index.query(llm=OpenAI(),\n",
    "            question=query,\n",
    "            chain_type=\"map_reduce\"\n",
    "           )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Claro, voc√™ tamb√©m pode especificar diferentes op√ß√µes neste `wrapper`:\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Dr.Eddy Giusepe Chirinos Isidro'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    # Dividir os documentos em chunks\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),\n",
    "    # Selecione os Embeddings que quer usar:\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    # Use Chroma como o vectorestore para indexar e pesquisar Embeddings:\n",
    "    vectorstore_cls=Chroma\n",
    ").from_loaders([loader])\n",
    "\n",
    "\n",
    "query = \"Quem foi o Cientista de Dados?\"\n",
    "\n",
    "index.query(llm=OpenAI(),\n",
    "            question=query,\n",
    "            chain_type= \"map_rerank\" #\"stuff\" # Usando `map_reduce` n√£o trouxe resposta a minha pergunta.` \n",
    "           )\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©todo 4: <font color=\"red\">ConversationalRetrievalChain</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConversationalRetrievalChain` √© muito semelhante ao `m√©todo 2 RetrievalQA`. Ele adicionou um par√¢metro adicional `chat_history` para passar no hist√≥rico do bate-papo, que pode ser usado para perguntas de acompanhamento.\n",
    "\n",
    "```\n",
    "ConversationalRetrievalChain = mem√≥ria de conversa√ß√£o + RetrievalQAChain\n",
    "```\n",
    "\n",
    "<font color=\"yellow\">Se voc√™ quiser que seu modelo de linguagem tenha uma mem√≥ria da conversa anterior, use este m√©todo.</font> \n",
    "\n",
    "\n",
    "No meu exemplo a seguir, perguntei para que foi importante aprender certos comandos no trabalho de pesquisa?. Como ele tem todo o hist√≥rico do chat, me trouxe uma boa resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# Carregamos nosso documento:\n",
    "loader = PyPDFLoader(\"./Eddy_Running_the_Vale_API.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Dividir os Documentosem Chunks:\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000,\n",
    "                                      chunk_overlap=0\n",
    "                                     )\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Seleciono os Embeddings que queremos usar:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Criamos o vectorestore para usar como √≠ndice:\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Exponha este √≠ndice em uma interface de recupera√ß√£o:\n",
    "retriever = db.as_retriever(search_type=\"similarity\",\n",
    "                            search_kwargs={\"k\":2}\n",
    "                           )\n",
    "\n",
    "# Criamos uma cadeia (chain) para responder a perguntas:  \n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(),\n",
    "                                           retriever\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"Qual foi o objetivo do trabalho de pesquisa?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Qual foi o objetivo do trabalho de pesquisa?',\n",
       " 'chat_history': [],\n",
       " 'answer': ' Conhecer e aplicar os comandos necess√°rios para instanciar a aplica√ß√£o da Vale.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Conhecer e aplicar os comandos necess√°rios para instanciar a aplica√ß√£o da Vale.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"Com os comandos aprendidos se instanciou o que?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Qual foi o objetivo do trabalho de pesquisa?',\n",
       "  ' Conhecer e aplicar os comandos necess√°rios para instanciar a aplica√ß√£o da Vale.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A API de An√°lise de Comunica√ß√£o de Voz (ACV) da Vale.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclus√£o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Agora voc√™ conhece quatro maneiras de responder a perguntas com `LLMs` no `LangChain`. Em resumo, `load_qa_chain` usa todos os textos e aceita v√°rios documentos; O `RetrievalQA` usa **load_qa_chain** sob o cap√¥, mas recupera primeiro os blocos de texto relevantes; `VectorstoreIndexCreator` √© o mesmo que **RetrievalQA** com uma interface de n√≠vel superior; `ConversationalRetrievalChain` √© √∫til quando voc√™ deseja passar seu `hist√≥rico de bate-papo` para o modelo.</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks God!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
