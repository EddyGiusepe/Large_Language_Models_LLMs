{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Launch an LLM App in One Hour</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto é quando usas o arquivo .env: \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY = os.environ['OPENAI_API_KEY']  \n",
    "openai.api_key = Eddy_API_KEY \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrar:\n",
    "\n",
    "```\n",
    "!pip install -qqq langchain openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\",\n",
    "             temperature=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nOlá! Prazer em conhecê-lo! Estou feliz em ver que você está fazendo um teste. Espero que você esteja aproveitando!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Olá, isto é um teste :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Olá! Prazer em conhecê-lo! Estou feliz em ver que você está fazendo um teste. Espero que você esteja aproveitando!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(llm(\"Olá, isto é um teste :)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Zero-shot chain-of-thought prompting é uma técnica de aprendizado de máquina que permite que um modelo aprenda a partir de um conjunto de dados sem a necessidade de etiquetagem. Esta técnica usa um algoritmo de aprendizado profundo para gerar um conjunto de palavras-chave a partir de um conjunto de dados não etiquetado. Estas palavras-chave são então usadas para gerar um conjunto de perguntas que podem ser usadas para estimular o pensamento do usuário. Esta técnica é útil para aplicações como a criação de diálogos, a criação de conteúdo personalizado e a criação de sistemas de recomendação."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm(\"O que é zero-shot chain-of-thought prompting?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve contexto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalamos:\n",
    "\n",
    "```\n",
    "!pip install -qqq arxiv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large Language Models are Zero-Shot Reasoners"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Link do paper --> https://arxiv.org/abs/2205.11916v4\n",
    "paper = next(arxiv.Search(id_list=[\"2205.11916\"]).results())\n",
    "\n",
    "Markdown(paper.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pretrained large language models (LLMs) are widely used in many sub-fields of\n",
       "natural language processing (NLP) and generally known as excellent few-shot\n",
       "learners with task-specific exemplars. Notably, chain of thought (CoT)\n",
       "prompting, a recent technique for eliciting complex multi-step reasoning\n",
       "through step-by-step answer examples, achieved the state-of-the-art\n",
       "performances in arithmetics and symbolic reasoning, difficult system-2 tasks\n",
       "that do not follow the standard scaling laws for LLMs. While these successes\n",
       "are often attributed to LLMs' ability for few-shot learning, we show that LLMs\n",
       "are decent zero-shot reasoners by simply adding \"Let's think step by step\"\n",
       "before each answer. Experimental results demonstrate that our Zero-shot-CoT,\n",
       "using the same single prompt template, significantly outperforms zero-shot LLM\n",
       "performances on diverse benchmark reasoning tasks including arithmetics\n",
       "(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\n",
       "Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\n",
       "Objects), without any hand-crafted few-shot examples, e.g. increasing the\n",
       "accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\n",
       "large InstructGPT model (text-davinci-002), as well as similar magnitudes of\n",
       "improvements with another off-the-shelf large model, 540B parameter PaLM. The\n",
       "versatility of this single prompt across very diverse reasoning tasks hints at\n",
       "untapped and understudied fundamental zero-shot capabilities of LLMs,\n",
       "suggesting high-level, multi-task broad cognitive capabilities may be extracted\n",
       "by simple prompting. We hope our work not only serves as the minimal strongest\n",
       "zero-shot baseline for the challenging reasoning benchmarks, but also\n",
       "highlights the importance of carefully exploring and analyzing the enormous\n",
       "zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\n",
       "few-shot exemplars."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(paper.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "Um prompting chain-of-thought zero-shot é uma técnica de elicitação de raciocínio complexo multi-etapa através de exemplos de resposta passo a passo. Esta técnica foi usada para alcançar desempenhos de ponta nos desafios de aritmética e raciocínio simbólico, tarefas difíceis do sistema-2 que não seguem as leis de escalonamento padrão para modelos de linguagem de grande porte (LLMs). Esta técnica foi mostrada para melhorar significativamente o desempenho zero-shot dos LLMs em diversos benchmarks de raciocínio, sem exemplos de poucos tiros, como aumentar a precisão em MultiArith de 17,7% para 78,7% e GSM8K de 10,4% para 40,7%."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(f\"\"\"Aqui está um resumo de um artigo:\n",
    "{paper.summary}\n",
    "\n",
    "Com base nesse resumo, o que é um prompting chain-of-thought zero-shot\"\"\")\n",
    "\n",
    "Markdown(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torne o paper pesquisável"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar:\n",
    "\n",
    "```\n",
    "!pip install -qqq pypdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_path = paper.download_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(paper_path)\n",
    "pages = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Large Language Models are Zero-Shot Reasoners\\nTakeshi Kojima\\nThe University of Tokyo\\nt.kojima@weblab.t.u-tokyo.ac.jpShixiang Shane Gu\\nGoogle Research, Brain Team\\nMachel Reid\\nGoogle Research\\x03Yutaka Matsuo\\nThe University of TokyoYusuke Iwasawa\\nThe University of Tokyo\\nAbstract\\nPretrained large language models (LLMs) are widely used in many sub-ﬁelds of\\nnatural language processing (NLP) and generally known as excellent few-shot\\nlearners with task-speciﬁc exemplars. Notably, chain of thought (CoT) prompting,\\na recent technique for eliciting complex multi-step reasoning through step-by-\\nstep answer examples, achieved the state-of-the-art performances in arithmetics\\nand symbolic reasoning, difﬁcult system-2 tasks that do not follow the standard\\nscaling laws for LLMs. While these successes are often attributed to LLMs’\\nability for few-shot learning, we show that LLMs are decent zero-shot reasoners\\nby simply adding “Let’s think step by step” before each answer. Experimental\\nresults demonstrate that our Zero-shot-CoT, using the same single prompt template,\\nsigniﬁcantly outperforms zero-shot LLM performances on diverse benchmark\\nreasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SV AMP),\\nsymbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date\\nUnderstanding, Tracking Shufﬂed Objects), without any hand-crafted few-shot\\nexamples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and\\nGSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-\\n002), as well as similar magnitudes of improvements with another off-the-shelf\\nlarge model, 540B parameter PaLM. The versatility of this single prompt across\\nvery diverse reasoning tasks hints at untapped and understudied fundamental\\nzero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive\\ncapabilities may be extracted by simple prompting. We hope our work not only\\nserves as the minimal strongest zero-shot baseline for the challenging reasoning\\nbenchmarks, but also highlights the importance of carefully exploring and analyzing\\nthe enormous zero-shot knowledge hidden inside LLMs before crafting ﬁnetuning\\ndatasets or few-shot exemplars.\\n1 Introduction\\nScaling up the size of language models has been key ingredients of recent revolutions in natural\\nlanguage processing (NLP) [Vaswani et al., 2017, Devlin et al., 2019, Raffel et al., 2020, Brown et al.,\\n2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]. The success of large language\\nmodels (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various\\ntasks by simply conditioning the models on a few examples (few-shot) or instructions describing the\\ntask (zero-shot). The method of conditioning the language model is called “prompting” [Liu et al.,\\n2021b], and designing prompts either manually [Schick and Schütze, 2021, Reynolds and McDonell,\\n2021] or automatically [Gao et al., 2021, Shin et al., 2020] has become a hot topic in NLP.\\n\\x03Work done while at The University of Tokyo.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.11916v4  [cs.CL]  29 Jan 2023', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 0}),\n",
       " Document(page_content='(c) Zero-shot \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA: The answer (arabic numerals) is  \\n(Output) 8 X(d) Zero-shot-CoT (Ours) \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA: Let’s think step by step.  \\n(Output) There are 16 balls in total. Half of the balls are golf  \\nballs. That means that there are 8 golf balls. Half of the golf balls  \\nare blue. That means that there are 4 blue golf balls. ✓Q: Roger has 5 tennis balls. He buys 2 more cans of tennis \\nballs. Each can has 3 tennis balls. How many tennis balls does \\nhe have now? \\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 \\ntennis balls. 5 + 6 = 11. The answer is 11. \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA:\\n(Output) The juggler can juggle 16 balls. Half of the balls are golf  \\nballs. So there are 16 / 2 = 8 golf balls. Half of the golf balls are  \\nblue. So there are 8 / 2 = 4 blue golf balls. The answer is 4. ✓(b) Few-shot-CoT (a) Few-shot \\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis \\nballs. Each can has 3 tennis balls. How many tennis balls does \\nhe have now? \\nA: The answer is 11. \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA:\\n(Output) The answer is 8. XFigure 1: Example inputs and outputs of GPT-3 with (a) standard Few-shot ([Brown et al., 2020]), (b)\\nFew-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to\\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\\ntask, ours does not need any examples and just uses the same prompt “Let’s think step by step” across\\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\\nand West, 2000] tasks with task-speciﬁc few-shot or zero-shot prompting [Liu et al., 2021b], even\\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\\nsatisﬁes the scaling laws better and jumps up with the size of the language models. For example,\\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\\nprompting signiﬁcantly increases the performance over standard few-shot prompting across several\\nbenchmark reasoning tasks, e.g., GSM8K (17.9% !58.1%).\\nWhile the successes of CoT prompting [Wei et al., 2022], along those of many other task-speciﬁc\\nprompting work [Gao et al., 2021, Schick and Schütze, 2021, Liu et al., 2021b], are often attributed\\nto LLMs’ ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot\\nreasoners by adding a simple prompt, Let’s think step by step , to facilitate step-by-step thinking before\\nanswering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully\\ngenerates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a\\nproblem where the standard zero-shot approach fails. Importantly, our Zero-shot-CoT is versatile and', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 1}),\n",
       " Document(page_content='generates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a\\nproblem where the standard zero-shot approach fails. Importantly, our Zero-shot-CoT is versatile and\\ntask-agnostic , unlike most prior task-speciﬁc prompt engineering in the forms of examples (few-shot)\\nor templates (zero-shot) [Liu et al., 2021b]: it can facilitate step-by-step answers across various\\nreasoning tasks, including arithmetic (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021],\\nAQUA-RAT [Ling et al., 2017], and SV AMP [Patel et al., 2021]), symbolic reasoning (Last letter and\\nCoin ﬂip), commonsense reasoning (CommonSenseQA [Talmor et al., 2019] and Strategy QA [Geva\\net al., 2021]), and other logical reasoning tasks (Date understanding and Tracking Shufﬂed Objects\\nfrom BIG-bench [Srivastava et al., 2022]) without modifying the prompt per task.\\nWe empirically evaluate Zero-shot-CoT against other prompting baselines in Table 2. While our\\nZero-shot-CoT underperforms Few-shot-CoT with carefully-crafted and task-speciﬁc step-by-step ex-\\namples, Zero-shot-CoT achieves enormous score gains compared to the zero-shot baseline, e.g. from\\n17.7% to 78.7% on MultiArith and from 10.4% to 40.7% on GSM8K with large-scale InstructGPT\\n2', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 1}),\n",
       " Document(page_content='model (text-davinci-002). We also evaluate Zero-shot-CoT with another off-the-shelf large model,\\n540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K.\\nImportantly, with our single ﬁxed prompt, zero-shot LLMs have a signiﬁcantly better scaling curve\\ncomparable to that of the few-shot CoT baseline. We also show that besides Few-shot-CoT requiring\\nhuman engineering of multi-step reasoning prompts, their performance deteriorates if prompt example\\nquestion types and task question type are unmatched, suggesting high sensitivity to per-task prompt\\ndesigns. In contrast, the versatility of this single prompt across diverse reasoning tasks hints at\\nuntapped and understudied zero-shot fundamental capabilities of LLMs, such as higher-level broad\\ncognitive capabilities like generic logical reasoning [Chollet, 2019]. While the vibrant ﬁeld of LLMs\\nstarted out from the premise of excellent few-shot learners [Brown et al., 2020], we hope our work\\nencourages more research into uncovering high-level andmulti-task zero-shot capabilities hidden\\ninside those models.\\n2 Background\\nWe brieﬂy review the two core preliminary concepts that form the basis of this work: the advent of\\nlarge language models (LLMs) and prompting, and chain of thought (CoT) prompting for multi-step\\nreasoning.\\nLarge language models and prompting A language model (LM), is a model that looks to estimate\\nthe probability distribution over text. Recently, scaling improvements through larger model sizes\\n(from a few million [Merity et al., 2016] to hundreds of millions [Devlin et al., 2019] to hundreds of\\nbillions [Brown et al., 2020] parameters) and larger data (e.g. webtext corpora [Gao et al., 2020])\\nhave enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream\\nNLP tasks. Besides the classic “pre-train and ﬁne-tune” paradigm [Liu et al., 2021b], models scaled\\nto 100B+ parameters exhibit properties conducive to few-shot learning [Brown et al., 2020], by way\\nof in context learning, where one can use a text or template known as a prompt to strongly guide the\\ngeneration to output answers for desired tasks, thus beginning an era of “pre-train and prompt” [Liu\\net al., 2021a]. In work, we call such prompts with explicit conditioning on few task examples as\\nfew-shot prompts, and other template-only prompts as zero-shot prompts.\\nChain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have par-\\nticularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\\nby modifying the answers in few-shot examples to step-by-step answers, and achieved signiﬁcant\\nboosts in performance across these difﬁcult benchmarks, especially when combined with very large\\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\\ngiven for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\\nasFew-shot-CoT in this work.\\n3 Zero-shot Chain of Thought\\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let’s\\nthink step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\\n3.1 Two-stage prompting', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       " Document(page_content='think step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\\n3.1 Two-stage prompting\\nWhile Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and\\nanswer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1)\\nalready uses prompting in the form of “The answer is”, to extract the answers in correct formats.\\nFew-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly\\ndesigning the few-shot example answers to end in such formats (see the top-right and top-left\\n3', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       " Document(page_content=\"Q: On average Joe throws 25 punches per \\nminute.  A fight lasts 5 rounds of 3 minutes.  How \\nmany punches did he throw? \\nA: Let's think step by step. \\nIn one minute, Joe throws 25 punches. \\nIn three minutes, Joe throws 3 * 25 = 75 punches. \\nIn five rounds, Joe throws 5 * 75 = 375 punches. Q: On average Joe throws 25 punches per \\nminute.  A fight lasts 5 rounds of 3 ・・・\\nA: Let's think step by step. \\nIn one minute, Joe throws 25 punches. ・・・In five \\nrounds, Joe throws 5 * 75 = 375 punches. . \\nTherefore, the answer (arabic numerals) is \\n375.LLM\\nLLM【1st prompt 】\\nReasoning Extraction 【2nd prompt 】\\nAnswer Extraction Figure 2: Full pipeline of Zero-shot-CoT as described in § 3: we ﬁrst use the ﬁrst “reasoning” prompt\\nto extract a full reasoning path from a language model, and then use the second “answer” prompt to\\nextract the answer in the correct format from the reasoning text.\\nin Figure 1). In summary, Few-shot-CoT [Wei et al., 2022] requires careful human engineering of\\na few prompt examples with speciﬁc answer formats per task, while Zero-shot-CoT requires less\\nengineering but requires prompting LLMs twice.\\n1st prompt: reasoning extraction In this step we ﬁrst modify the input question xinto a prompt\\nx0using a simple template “Q: [X]. A:[T]”, where [X]is an input slot for xand[T]is an slot\\nfor hand-crafted trigger sentence tthat would extract chain of though to answer the question x. For\\nexample, if we use “Let’s think step by step” as a trigger sentence, the prompt x0would be “Q: [X].\\nA: Let’s think step by step.”. See Table 4 for more trigger examples. Prompted text x0is then fed into\\na language model and generate subsequent sentence z. We can use any decoding strategy, but we\\nused greedy decoding throughout the paper for the simplicity.\\n2nd prompt: answer extraction In the second step, we use generated sentence zalong with\\nprompted sentence x0to extract the ﬁnal answer from the language model. To be concrete, we simply\\nconcatenate three elements as with “ [X0] [Z] [A] ”:[X0]for 1st prompt x0,[Z] for sentence z\\ngenerated at the ﬁrst step, and [A]for a trigger sentence to extract answer. The prompt for this step\\nisself-augmented , since the prompt contains the sentence zgenerated by the same language model.\\nIn experiment, we use slightly different answer trigger depending on the answer format. For example,\\nwe use “Therefore, among A through E, the answer is” for multi-choice QA, and “Therefore, the\\nanswer (arabic numerals) is” for math problem requiring numerical answer. See Appendix A.5 for\\nthe lists of answer trigger sentences. Finally, the language model is fed the prompted text as input to\\ngenerate sentences ^ yand parse the ﬁnal answer. See “Answer Cleansing” at §4 for the parser details.\\n4 Experiment\\nTasks and datasets We evaluate our proposal on 12 datasets from four categories of reasoning\\ntasks: arithmetic, commonsense, symbolic, and other logical reasoning tasks. See Appendix A.2 for\\nthe detailed description of each datasets.\\nFor arithmetic reasoning, we consider the following six datasets: (1) SingleEq [Koncel-Kedziorski\\net al., 2015], (2) AddSub [Hosseini et al., 2014], (3) MultiArith [Roy and Roth, 2015], (4) AQUA-\\nRAT [Ling et al., 2017], (5) GSM8K [Cobbe et al., 2021], and (6) SV AMP [Patel et al., 2021]. The\\nﬁrst three are from the classic Math World Problem Repository [Koncel-Kedziorski et al., 2016],\\nand the last three are from more recent benchmarks. SingleEq and AddSub contain easier problems,\\nwhich do not require multi-step calculation to solve the tasks. MultiArith, AQUA-RAT, GSM8k, and\\nSV AMP are more challenging datasets that require multi-step reasoning to solve.\\nFor commonsense reasoning, we use CommonsenseQA [Talmor et al., 2019] and StrategyQA [Geva\\net al., 2021]. CommonsenseQA asks questions with complex semantics that often require reasoning\\n4\", metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 3}),\n",
       " Document(page_content='based on prior knowledge [Talmor et al., 2019]. StrategyQA requires models to infer an implicit\\nmulti-hop reasoning to answer questions [Geva et al., 2021].\\nFor symbolic reasoning, we use Last Letter Concatenation and Coin Flip [Wei et al., 2022]. Last\\nletter Concatenation asks the model to concatenate the last letters of each word. We used randomly\\nselected four names for each sample. Coin Flip asks the model to answer whether a coin is still heads\\nup after people either ﬂip or do not ﬂip the coin. We created samples of four times ﬂip or not ﬂip\\ntrials. Although these tasks are easy for humans, LMs typically exhibit a ﬂat scaling curve.\\nFor other logical reasoning tasks, we choose two evaluation sets from the BIG-bench effort [Srivastava\\net al., 2022]: Date Understanding2and Tracking Shufﬂed Objects. Date Understanding asks models\\nto infer the date from a context. Tracking Shufﬂed Objects tests a model’s ability to infer the ﬁnal\\nstate of objects given its initial state and a sequence of object shufﬂing. We used a dataset of tracking\\nthree shufﬂed objects for our experiment.\\nModels We experiment with 17 models in total. Main experiments are conducted with Instruct-\\nGPT3 [Ouyang et al., 2022] (text-ada/babbage/curie/davinci-001 and text-davinci-002)3, original\\nGPT3 [Brown et al., 2020] (ada, babbage, curie, and davinci)4, and PaLM [Chowdhery et al., 2022]\\n(8B, 62B, and 540B). In addition, we used GPT-2[Radford et al., 2019], GPT-Neo[Black et al., 2021],\\nGPT-J[Wang and Komatsuzaki, 2021], T0 [Sanh et al., 2022], and OPT [Zhang et al., 2022] for model\\nscaling study. The size of LMs ranges from 0.3B to 540B. We include both standard (e.g. GPT-3 and\\nOPT), and instruction following variants (e.g. Instruct-GPT3 and T0). See Appendix A.3 for model\\ndescription details. Unless otherwise stated, we use text-davinci-002 throughout the experiments.\\nBaselines We compare our Zero-shot-CoT mainly to standard Zero-shot prompting to verify the\\neffectiveness of its chain of thought reasoning. For Zero-shot experiments, similar answer prompts\\nas Zero-shot-CoT are used as default. See Appendix A.5 for detail. To better evaluate the zero-shot\\nability of LLMs on reasoning tasks, we also compare our method to Few-shot and Few-shot-CoT\\nbaselines from [Wei et al., 2022], using the same in-context examples. Throughout the experiments,\\nwe use greedy decoding across all the methods. For the zero-shot approaches, the results are therefore\\ndeterministic. For the few-shot approaches, since the order of in-context examples could affect the\\nresults [Lu et al., 2022], we run each experiment only once with a ﬁxed seed across all methods and\\ndatasets, for fair comparisons with the zero-shot methods. Wei et al. [2022] showed that the order of\\nexamples did not cause large variance in CoT experiments.\\nAnswer cleansing After the model outputs a text by answer extraction (see § 3 and Figure 2), our\\nmethod picks up only the part of the answer text that ﬁrst satisﬁes the answer format. For example,\\nif the answer prompting outputs “probably 375 and 376” on arithmetic tasks, we extract the ﬁrst\\nnumber “375” and set it as the model prediction. In the case of multiple-choice, the ﬁrst large letter\\nwe encounter is set as the prediction. See Appendix A.6 for more detail. Standard Zero-shot method\\nfollows the same idea. For Few-shot and Few-shot-CoT methods, we follow [Wang et al., 2022] and\\nﬁrst extract the answer text after \"The answer is \" from the model output, and apply the same answer\\ncleansing to parse the answer text. If “The answer is” is not found in the model output, we search\\nfrom the back of the text and set the ﬁrst text that satisﬁes the answer format as the prediction.\\n4.1 Results\\nZero-shot-CoT vs. Zero-shot Table 1 summarize accuracy of our method (Zero-shot-CoT) and\\nstandard zero-shot prompting (Zero-shot) for each dataset. Zero-shot-CoT substantially outperforms\\nfour out of six arithmetic reasoning tasks (MultiArith, GSM8K, AQUA, SV AMP), all symbolic', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 4}),\n",
       " Document(page_content='standard zero-shot prompting (Zero-shot) for each dataset. Zero-shot-CoT substantially outperforms\\nfour out of six arithmetic reasoning tasks (MultiArith, GSM8K, AQUA, SV AMP), all symbolic\\nreasoning, and all other logical reasoning tasks (from BIG-bench [Srivastava et al., 2022]). For\\n2While prior work [Wei et al., 2022] categorized Date Understanding task into Common Sense reasoning,\\nour study categorized this task into logical reasoning because this task requires less prior knowledge and more\\nlogical reasoning between dates.\\n3Our experiment for Instruct GPT-3 models includes both text-****-001 and text-davinci-002. Text-davinci-\\n002 differs from text-****-001 in that they use different ﬁne-tuning data depending on the date range collected\\nfrom the APIs. Speciﬁcally, text-davinci-002 uses data up to Jun 2021, while text-****-001 uses data up to Oct\\n2019. (See https://beta.openai.com/docs/engines/gpt-3 )\\n4Our experiments with GPT3 series are conducted by using OpenAI API between April-2022 and May-2022,\\nexcept for No.10-16 in Table 4 in Aug-2022.\\n5', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 4}),\n",
       " Document(page_content='Table 1: Accuracy comparison of Zero-shot-CoT with Zero-shot on each tasks. The values on the left\\nside of each task are the results of using answer extraction prompts depending on answer format as\\ndescribed at § 3. The values on the right side are the result of additional experiment where standard\\nanswer prompt \"The answer is\" is used for answer extraction. See Appendix A.5 for detail setups.\\nArithmetic\\nSingleEq AddSub MultiArith GSM8K AQUA SV AMP\\nzero-shot 74.6/ 78.7 72.2 /77.0 17.7/22.7 10.4/12.5 22.4/22.4 58.8/58.7\\nzero-shot-cot 78.0/78.7 69.6/74.7 78.7/79.3 40.7 /40.5 33.5 /31.9 62.1 /63.7\\nCommon Sense Other Reasoning Tasks Symbolic Reasoning\\nCommon Strategy Date Shufﬂed Last Letter Coin Flip\\nSenseQA QA Understand Objects (4 words) (4 times)\\nzero-shot 68.8/72.6 12.7/ 54.3 49.3/33.6 31.3/29.7 0.2/- 12.8/53.8\\nzero-shot-cot 64.6/64.0 54.8/52.3 67.5/61.8 52.4 /52.9 57.6 /- 91.4/87.8\\nTable 2: Comparison with baseline methods using accuracies on MultiArith and GSM8K. text-davinci-\\n002 is used as the model if not speciﬁed. We used the same 8 examples as described in [Wei et al.,\\n2022] for Few-shot and Few-shot-CoT settings. (*1) To verify the variance of changing examples,\\nwe report two results for 4-shot-cot by splitting the eight examples into two groups. (*2) We insert\\n“Let’s think step by step.” at the beginning of answer part of each exemplars for Few-shot-CoT to test\\nperformance gains. Further experiment results with PaLM are found at Appendix D\\nMultiArith GSM8K\\nZero-Shot 17.7 10.4\\nFew-Shot (2 samples) 33.7 15.6\\nFew-Shot (8 samples) 33.8 15.6\\nZero-Shot-CoT 78.7 40.7\\nFew-Shot-CoT (2 samples) 84.8 41.3\\nFew-Shot-CoT (4 samples : First) (*1) 89.2 -\\nFew-Shot-CoT (4 samples : Second) (*1) 90.5 -\\nFew-Shot-CoT (8 samples) 93.0 48.7\\nZero-Plus-Few-Shot-CoT (8 samples) (*2) 92.8 51.5\\nFinetuned GPT-3 175B [Wei et al., 2022] - 33\\nFinetuned GPT-3 175B + veriﬁer [Wei et al., 2022] - 55\\nPaLM 540B: Zero-Shot 25.5 12.5\\nPaLM 540B: Zero-Shot-CoT 66.1 43.0\\nPaLM 540B: Zero-Shot-CoT + self consistency 89.0 70.1\\nPaLM 540B: Few-Shot [Wei et al., 2022] - 17.9\\nPaLM 540B: Few-Shot-CoT [Wei et al., 2022] - 56.9\\nPaLM 540B: Few-Shot-CoT + self consistency [Wang et al., 2022] - 74.4\\nexample, Zero-shot-CoT achieves score gains from 17.7% to 78.7% on MultiArith and from 10.4% to\\n40.7% on GSM8K. Our method gives on-par performances for the remaining two arithmetic reasoning\\ntasks (SingleEq and AddSub), which is expected since they do not require multi-step reasoning.\\nIn commonsense reasoning tasks, Zero-shot-CoT does not provide performance gains. It is expected\\nas Wei et al. [2022] also reports that even Few-shot-CoT does not provide performance gains on\\nLambda (135B), but does improve StrategyQA when combined with substantially larger PaLM\\n(540B) model, which may also apply for ours. More importantly, we observe that many generated\\nchain of thought themselves are surprisingly logically correct or only contains human-understandable\\nmistakes (See Table 3), suggesting that Zero-shot-CoT does elicit for better commonsense reasoning\\neven when the task metrics do not directly reﬂect it. We provide samples generated by Zero-shot-CoT\\nfor each dataset in Appendix B.\\n6', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 5}),\n",
       " Document(page_content='020406080\\n0.3B 1.3B 6.7B 175BZero-shot Zero-shot-CoT (a) MultiArith on Original GPT-3\\n020406080\\nS M L XLZero-shot Zero-shot-CoT (b) MultiArith on Instruct GPT-3\\n01020304050\\n8B 62B 540BZero-shot Zero-shot-CoT (c) GMS8K on PaLM\\nFigure 3: Model scale study with various types of models. S: text-ada-001, M: text-babbage-001, L:\\ntext-curie-001, XL: text-davinci-002. See Appendix A.3 and E for the detail.\\nTable 3: Examples generated by Zero-Shot-CoT on CommonsenseQA for Error Analysis.\\nExample 1 (Reasoning is correct.) Example 2 (Model cannot choose only one.)\\nQ:Where is a well used toy car likely to be found?\\nAnswer Choices: (A) child’s room (B) boy’s bed-\\nroom (C) own home (D) toy store (E) houseQ:What would be necessary for getting in shape?\\nAnswer Choices: (A) good health (B) exercise (C)\\nmuscle tone (D) sweat (E) feel better\\nA: Let’s think step by step. A toy car is likely\\nto be found in a child’s room. A child’s room is\\nlikely to be found in a house. Therefore, a toy\\ncar is likely to be found in a house.\\nTherefore, among A through E, the answer is E.\\n7(GT : A, Zero-shot: A)A: Let’s think step by step. In order to get in shape,\\nyou need to be able to do three things:\\n1. Exercise regularly\\n2. Eat a healthy diet\\n3. Get enough sleep\\nIf you can do all three of those things, then you will\\nbe well on your way to getting in shape! Therefore,\\namong A through E, the answer is B, C, and D.\\n(GT : B, Zero-shot: B)\\nComparison with other baselines Table 2 compares the performances on two arithmetic reasoning\\nbenchmarks (MultiArith and GSM8K) across Zero-shot-CoT and baselines. The large gap between\\nstandard prompting (1st block) and chain of thought prompting (2nd block) suggests that these\\ntasks are difﬁcult without eliciting multi-step reasoning. Major improvements are conﬁrmed on\\nboth Instruct GPT-3 (text-davinci-002) and PaLM (540B) models (4th block). While Zero-shot-CoT\\nnaturally underperforms Few-shot-CoT, it substantially outperforms standard Few-shot prompting\\nwith even 8 examples per task. For GSM8K, Zero-shot-CoT with Instruct GPT-3 (text-davinci-002)\\nalso outperforms ﬁnetuned GPT-3 and standard few-shot prompting with large models (PaLM, 540B),\\nreported in Wei et al. [2022] (3rd and 4th block). See App. D for more experiment results with PaLM.\\nDoes model size matter for zero-shot reasoning? Figure 3 compares performance of various\\nlanguage models on MultiArith / GSM8K. Without chain of thought reasoning, the performance\\ndoes not increase or increases slowly as the model scale is increased, i.e., the curve is mostly ﬂat. In\\ncontrast, the performance drastically increases with chain of thought reasoning, as the model size\\ngets bigger, for Original/Instruct GPT-3 and PaLM. When the model size is smaller, chain of thought\\nreasoning is not effective. This result aligns with the few-shot experiment results in Wei et al. [2022].\\nAppendix E shows extensive experiment results using wider variety of language models, including\\nGPT-2, GPT-Neo, GPT-J, T0, and OPT. We also manually investigated the quality of generated chain\\nof thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the\\nsampled outputs for each model).\\nError Analysis To better understand the behavior of Zero-shot-CoT, we manually investigated\\nrandomly selected examples generated by Instruct-GPT3 with Zero-shot-CoT prompting. See Ap-\\npendix C for examples, where some of the observations include: (1) In commonsense reasoning\\n(CommonsenseQA), Zero-shot-CoT often produces ﬂexible and reasonable chain of thought even\\nwhen the ﬁnal prediction is not correct. Zero-shot-CoT often output multiple answer choices when\\nthe model ﬁnd it is difﬁcult to narrow it down to one (see Table 3 for examples). (2) In arithmetic\\n7', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 6}),\n",
       " Document(page_content='Table 4: Robustness study against template measured on the MultiArith dataset with text-davinci-002.\\n(*1) This template is used in Ahn et al. [2022] where a language model is prompted to generate\\nstep-by-step actions given a high-level instruction for controlling robotic actions. (*2) This template\\nis used in Reynolds and McDonell [2021] but is not quantitatively evaluated.\\nNo. Category Template Accuracy\\n1 instructive Let’s think step by step. 78.7\\n2 First, (*1) 77.3\\n3 Let’s think about this logically. 74.5\\n4 Let’s solve this problem by splitting it into steps. (*2) 72.2\\n5 Let’s be realistic and think step by step. 70.8\\n6 Let’s think like a detective step by step. 70.3\\n7 Let’s think 57.5\\n8 Before we dive into the answer, 55.7\\n9 The answer is after the proof. 45.7\\n10 misleading Don’t think. Just feel. 18.8\\n11 Let’s think step by step but reach an incorrect answer. 18.7\\n12 Let’s count the number of \"a\" in the question. 16.7\\n13 By using the fact that the earth is round, 9.3\\n14 irrelevant By the way, I found a good restaurant nearby. 17.5\\n15 Abrakadabra! 15.5\\n16 It’s a beautiful day. 13.1\\n- (Zero-shot) 17.7\\nTable 5: Robustness study of Few-shot-CoT against examples. When the examples are from en-\\ntirely different tasks, the performance generally becomes worse, but when the answer formats are\\nmatched (i.e. CommonsenseQA to AQUA-RAT, multiple-choice), the performance loss is less severe.\\nyCommonsenseQA samples are used in this variation\\nZero-shot Few-shot-CoTyZero-shot-CoT Few-shot-CoT\\nAQUA-RAT 22.4 31.9 33.5 39.0\\nMultiArith 17.7 27.0 78.7 88.2\\nreasoning (MultiArith), Zero-shot-CoT and Few-shot-CoT show substantial differences regarding\\nthe error patterns. First, Zero-shot-CoT tends to output unnecessary steps of reasoning after getting\\nthe correct prediction, which results in changing the prediction to incorrect one. Zero-shot-CoT also\\nsometimes does not start reasoning, just rephrasing the input question. In contrast, Few-shot-CoT\\ntend to fail when generated chain of thought include ternary operation, e.g. (3 + 2)\\x034.\\nHow does prompt selection affect Zero-shot-CoT? We validate the robustness of Zero-shot-CoT\\nagainst input prompts. Table 4 summarizes performance using 16 different templates with three\\ncategories. Speciﬁcally, following Webson and Pavlick [2022], the categories include instructive\\n(encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong\\nway), and irrelevant (nothing to do with reasoning). The results indicate that the performance is\\nimproved if the text is written in a way that encourages chain of thought reasoning, i.e., the templates\\nare within \"instructive\" category. However, the difference in accuracy is signiﬁcant depending on\\nthe sentence. In this experiment, \"Let’s think step by step.\" achieves the best results. Interestingly,\\nit is found that different templates encourage the model to express reasoning quite differently (see\\nAppendix B for sample outputs by each template). In contrast, when we use misleading or irrelevant\\ntemplates, the performance does not improve. It remains an open question how to automatically\\ncreate better templates for Zero-shot-CoT.\\nHow does prompt selection affect Few-shot-CoT? Table 5 shows the performance of Few-\\nshot-CoT when using examples from different datasets: CommonsenseQA to AQUA-RAT and\\nCommonsenseQA to MultiArith. The domains are different in both cases, but the answer format\\n8', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 7}),\n",
       " Document(page_content='is the same in the former. Surprisingly, the chain of thought examples from different domains\\n(common sense to arithmetic) but with the same answer (multiple-choice) format provide substantial\\nperformance gain over Zero-shot (to AQUA-RAT), measured relative to the possible improvements\\nfrom Zero-shot-CoT or Few-shot-CoT. In contrast, the performance gain becomes much less when\\nusing examples with different answer types (to MultiArith), conﬁrming prior work [Min et al., 2022]\\nthat suggests LLMs mostly leverage the few-shot examples to infer the repeated format rather than\\nthe task itself in-context. Nevertheless, for both cases the results are worse than Zero-shot-CoT,\\nafﬁrming the importance of task-speciﬁc sample engineering in Few-shot-CoT.\\n5 Discussion and Related Work\\nTable 6: Summary of related work on arithmetic/commonsense reasoning tasks. Category denotes the\\ntraining strategy. CoT denotes whether to output chain of thought. Task column lists the tasks that\\nare performed in corresponding papers. AR: Arithmetic Reasoning, CR: Commonsense Reasoning.\\nMethod Category CoT Task Model\\nRajani et al. [2019] Fine-Tuning X CR GPT\\nCobbe et al. [2021] Fine-Tuning X AR GPT-3\\nZelikman et al. [2022] Fine-Tuning X AR,CR GPT-3, etc\\nNye et al. [2022] Fine-Tuning5X AR Transformer(Decoder)\\nBrown et al. [2020] Few/Zero-Shot CR GPT-3\\nSmith et al. [2022] Few/Zero-Shot AR,CR MT-NLG\\nRae et al. [2021] Few-Shot AR,CR Gopher\\nWei et al. [2022] Few-Shot X AR,CR PaLM, LaMBDA, GPT-3\\nWang et al. [2022] Few-Shot X AR,CR PaLM, etc\\nChowdhery et al. [2022] Few-Shot X AR,CR PaLM\\nShwartz et al. [2020] Zero-Shot X CR GPT-2, etc\\nReynolds and McDonell [2021] Zero-Shot X AR GPT-3\\nZero-shot-CoT (Ours) Zero-Shot X AR,CR PaLM, Instruct-GPT3, GPT-3, etc\\nReasoning Ability of LLMs Several studies have shown that pre-trained models usually are not\\ngood at reasoning [Brown et al., 2020, Smith et al., 2022, Rae et al., 2021], but its ability can be\\nsubstantially increased by making them produce step-by-step reasoning, either by ﬁne-tuning [Rajani\\net al., 2019, Cobbe et al., 2021, Zelikman et al., 2022, Nye et al., 2022] or few-shot prompting [Wei\\net al., 2022, Wang et al., 2022, Chowdhery et al., 2022] (See Table 6 for summary of related work).\\nUnlike most prior work, we focus on zero-shot prompting and show that a single ﬁxed trigger prompt\\nsubstantially increases the zero-shot reasoning ability of LLMs across a variety of tasks requiring\\ncomplex multi-hop thinking (Table 1), especially when the model is scaled up (Figure 3). It also\\ngenerates reasonable and understandable chain of thought across diverse tasks (Appendix B), even\\nwhen the ﬁnal prediction is wrong (Appendix C). Similar to our work, Reynolds and McDonell\\n[2021] demonstrate a prompt, “Let’s solve this problem by splitting it into steps”, would facilitate\\nthe multi-step reasoning in a simple arithmetic problem. However, they treated it as a task-speciﬁc\\nexample and did not evaluate quantitatively on diverse reasoning tasks against baselines. Shwartz et al.\\n[2020] propose to decompose a commonsense question into a series of information seeking question,\\nsuch as “what is the deﬁnition of [X]”. It does not require demonstrations but requires substantial\\nmanual prompt engineering per each reasoning task. Our results strongly suggest that LLMs are\\ndecent zero-shot reasoners, while prior work [Wei et al., 2022] often emphasize only few-shot learning\\nand task-speciﬁc in-context learning, e.g. no zero-shot baselines were reported. Our method does\\nnot require time-consuming ﬁne-tuning or expensive sample engineering, and can be combined with\\nany pre-trained LLM, serving as the strongest zero-shot baseline for all reasoning tasks.\\nZero-shot Abilities of LLMs Radford et al. [2019] show that LLMs have excellent zero-shot\\nabilities in many system-1 tasks, including reading comprehension, translation, and summarization.', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 8}),\n",
       " Document(page_content='Zero-shot Abilities of LLMs Radford et al. [2019] show that LLMs have excellent zero-shot\\nabilities in many system-1 tasks, including reading comprehension, translation, and summarization.\\n5Nye et al. [2022] also evaluates few-shot settings, but the few-shot performances on their domains are worse\\nthan the ﬁne-tuning results.\\n9', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 8}),\n",
       " Document(page_content='Sanh et al. [2022], Ouyang et al. [2022] show that such zero-shot abilities of LLMs can be increased\\nby explicitly ﬁne-tuning models to follow instructions. Although these work focus on the zero-shot\\nperformances of LLMs, we focus on many system-2 tasks beyond system-1 tasks, considered a grand\\nchallenge for LLMs given ﬂat scaling curves. In addition, Zero-shot-CoT is orthogonal to instruction\\ntuning; it increases zero-shot performance for Instruct GPT3, vanilla GPT3, and PaLM (See Figure 3).\\nFrom Narrow (task-speciﬁc) to Broad (multi-task) Prompting Most prompts are task-speciﬁc.\\nWhile few-shot prompts are naturally so due to task-speciﬁc in-context samples [Brown et al., 2020,\\nWei et al., 2022], majority of zero-shot prompts have also focused on per-task engineering (of\\ntemplates) [Liu et al., 2021b, Reynolds and McDonell, 2021]. Borrowing terminologies from Chollet\\n[2019] which builds on hierarchical models of intelligence [McGrew, 2005, Johnson and Bouchard Jr,\\n2005], these prompts are arguably eliciting “narrow generalization” or task-speciﬁc skills from LLMs.\\nOn the other hand, our method is a multi-task prompt and elicits “broad generalization” or broad\\ncognitive abilities in LLMs, such as logical reasoning or system-2 itself. We hope our work can serve\\nas a reference for accelerating not just logical reasoning research with LLMs, but also discovery of\\nother broad cognitive capabilities within LLMs.\\nTraining Dataset Details A limitation of the work is the lack of public information on the details\\nof training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs Instruct-\\nGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big\\nperformance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT\\n001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and non-\\narithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a\\ntask-agnostic multi-step reasoning capability for generic problem solving. While most results are\\nbased on InstructGPT since it is the best performing open-access LLM, key results are reproduced\\non PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also\\nconﬁrm that it is not specially engineered for multi-step reasoning.\\nLimitation and Social Impact Our work is based on prompting methods for large language models.\\nLLMs have been trained on large corpora from various sources on the web (also see “Training Dataset\\nDetails”), and have shown to capture and amplify biases found in the training data. Prompting is a\\nmethod that looks to take advantage of the patterns captured by language models conducive to various\\ntasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way\\nto probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context\\nlearning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.\\n6 Conclusion\\nWe have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large\\nlanguage models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach\\nin previous work that requires hand-crafting few-shot examples per task. Our simple method not only\\nis the minimalist and strongest zero-shot baseline for difﬁcult multi-step system-2 reasoning tasks\\nthat long evaded the scaling laws of LLMs, but also encourages the community to further discover\\nsimilar multi-task prompts that elicit broad cognitive abilities instead of narrow task-speciﬁc skills.\\nAcknowledgements\\nThis work has been supported by the Mohammed bin Salman Center for Future Science and Technol-\\nogy for Saudi-Japan Vision 2030 at The University of Tokyo (MbSC2030). Computational resource\\nof AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 9}),\n",
       " Document(page_content='ogy for Saudi-Japan Vision 2030 at The University of Tokyo (MbSC2030). Computational resource\\nof AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial\\nScience and Technology (AIST) was used for experiments other than PaLM. We also thank Jason\\nWei and Denny Zhou for discussions and support on running PaLM experiments, and Sharan Narang\\nand Aakanksha Chowdhery for generic support on PaLM infrastructures.\\nReferences\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian\\n10', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 9}),\n",
       " Document(page_content='Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth,\\nNikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine,\\nYao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\\nRettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev,\\nVincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can,\\nnot as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/\\n2204.01691 .\\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale\\nAutoregressive Language Modeling with Mesh-Tensorﬂow, March 2021. URL https://doi.\\norg/10.5281/zenodo.5297715 .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\\nand Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\\nR. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in NeurIPS , volume 33, pages 1877–1901.\\nCurran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\\nFrançois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547 , 2019. URL\\nhttps://arxiv.org/abs/1911.01547 .\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.\\nDai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\\nSlav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL\\nhttps://arxiv.org/abs/2204.02311 .\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\\nSchulman. Training veriﬁers to solve math word problems, 2021. URL https://arxiv.org/\\nabs/2110.14168 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL , pages\\n4171–4186, 2019. URL https://aclanthology.org/N19-1423 .\\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb\\ndataset of diverse text for language modeling. arXiv preprint arXiv: Arxiv-2101.00027 , 2020.\\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learn-\\ners. In Proceedings of ACL-IJCNLP , pages 3816–3830, 2021. URL https://aclanthology.\\norg/2021.acl-long.295 .\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\\nuse a laptop? a question answering benchmark with implicit reasoning strategies. TACL , 9:346–361,', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 10}),\n",
       " Document(page_content='Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\\nuse a laptop? a question answering benchmark with implicit reasoning strategies. TACL , 9:346–361,\\n2021. URL https://aclanthology.org/2021.tacl-1.21/ .\\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to\\nsolve arithmetic word problems with verb categorization. In EMNLP , volume 523533. Citeseer,\\n2014. URL https://aclanthology.org/D14-1058/ .\\n11', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 10}),\n",
       " Document(page_content='Wendy Johnson and Thomas J Bouchard Jr. The structure of human intelligence: It is verbal,\\nperceptual, and image rotation (vpr), not ﬂuid and crystallized. Intelligence , 33(4):393–416, 2005.\\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas\\nAng. Parsing algebraic word problems into equations. TACL , 3:585–597, 2015. URL https:\\n//aclanthology.org/Q15-1042 .\\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS:\\nA math word problem repository. In Proceedings of NAACL , pages 1152–1157, 2016. URL\\nhttps://aclanthology.org/N16-1136 .\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen-\\neration: Learning to solve and explain algebraic word problems. In Proceedings of ACL , pages\\n158–167, 2017. URL https://aclanthology.org/P17-1015 .\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What\\nmakes good in-context examples for gpt- 3?arXiv preprint arXiv:2101.06804 , 2021a. URL\\nhttps://arxiv.org/abs/2101.06804 .\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\\nprocessing. arXiv preprint arXiv:2107.13586 , 2021b. URL https://arxiv.org/abs/2107.\\n13586 .\\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered\\nprompts and where to ﬁnd them: Overcoming few-shot prompt order sensitivity. In Proceedings of\\nACL, pages 8086–8098, 2022. URL https://aclanthology.org/2022.acl-long.556 .\\nKevin S McGrew. The cattell-horn-carroll theory of cognitive abilities: Past, present, and future.\\n2005.\\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\\nmodels. arXiv preprint arXiv: Arxiv-1609.07843 , 2016. URL https://arxiv.org/abs/1609.\\n07843 .\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\\nZettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv\\npreprint arXiv:2202.12837 , 2022. URL https://arxiv.org/pdf/2202.12837.pdf .\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\\nmodels. In Deep Learning for Code Workshop , 2022. URL https://openreview.net/forum?\\nid=HBlx2idbkbq .\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\\nRyan Lowe. Training language models to follow instructions with human feedback, 2022. URL\\nhttps://arxiv.org/abs/2203.02155 .\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py-\\ntorch: An imperative style, high-performance deep learning library. Advances in\\nNeurIPS , 32:8026–8037, 2019. URL https://papers.nips.cc/paper/2019/hash/\\nbdbca288fee7f92f2bfa9f7012727740-Abstract.html .\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple\\nmath word problems? In Proceedings of NAACL , pages 2080–2094, 2021. URL https://\\naclanthology.org/2021.naacl-main.168 .\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog , page 9, 2019. URL http://www.\\npersagen.com/files/misc/radford2019language.pdf .\\n12', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 11}),\n",
       " Document(page_content='Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,\\nAida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,\\nJean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume,\\nYujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,\\nAurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher,\\n2021. URL https://arxiv.org/abs/2112.11446 .\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. JMLR , 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html .\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!\\nleveraging language models for commonsense reasoning. In Proceedings of ACL , pages 4932–4942,\\n2019. URL https://aclanthology.org/P19-1487 .\\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the\\nfew-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in\\nComputing Systems , pages 1–7, 2021. URL https://arxiv.org/pdf/2102.07350.pdf .\\nSubhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of EMNLP ,\\npages 1743–1752, 2015. URL https://aclanthology.org/D15-1202 .\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, An-\\ntoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal\\nNayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Man-\\nica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala\\nNeeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush.\\nMultitask prompted training enables zero-shot task generalization. In ICLR , 2022. URL\\nhttps://openreview.net/forum?id=9Vrb9D0WI4 .\\nTimo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are also few-\\nshot learners. In Proceedings of NAACL , pages 2339–2352, 2021. URL https://aclanthology.\\norg/2021.naacl-main.185 .\\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. Auto-\\nPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts.\\nInProceedings of EMNLP , pages 4222–4235, 2020. URL https://aclanthology.org/2020.\\nemnlp-main.346 .\\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised\\ncommonsense question answering with self-talk. In Proceedings of EMNLP , pages 4615–4629,\\n2020. URL https://aclanthology.org/2020.emnlp-main.373 .\\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 12}),\n",
       " Document(page_content='Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon\\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He,\\nMichael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to\\ntrain megatron-turing nlg 530b, a large-scale generative language model, 2022. URL https:\\n//arxiv.org/abs/2201.11990 .\\n13', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 12}),\n",
       " Document(page_content='Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the\\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint\\narXiv:2206.04615 , 2022. URL https://arxiv.org/abs/2206.04615 .\\nKeith E Stanovich and Richard F West. Individual differences in reasoning: Implications for the\\nrationality debate? Behavioral and brain sciences , 23(5):645–665, 2000.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge. In Proceedings of NAACL-HLT , pages\\n4149–4158, 2019. URL https://aclanthology.org/N19-1421/ .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven\\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent\\nZhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh\\nSrinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,\\nRenelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\\nMark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee,\\nLora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton,\\nAaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak,\\nEd Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022. URL https:\\n//arxiv.org/abs/2201.08239 .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on\\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Ad-\\nvances in NeurIPS , 2017. URL https://proceedings.neurips.cc/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\\nhttps://github.com/kingoflolz/mesh-transformer-jax , May 2021.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency\\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022.\\nURL https://arxiv.org/abs/2203.11171 .\\nAlbert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their\\nprompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies , pages 2300–2344. Association for\\nComputational Linguistics, July 2022. URL https://aclanthology.org/2022.naacl-main.\\n167.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.\\nChain of thought prompting elicits reasoning in large language models, 2022. URL https:\\n//arxiv.org/abs/2201.11903 .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural\\nlanguage processing. In Proceedings of EMNLP , 2020. URL https://aclanthology.org/\\n2020.emnlp-demos.6 .\\nEric Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning,\\n2022. URL https://arxiv.org/abs/2203.14465 .\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\\nmodels. arXiv preprint arXiv:2205.01068 , 2022. URL https://arxiv.org/abs/2205.01068 .\\n14', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 13}),\n",
       " Document(page_content='Checklist\\n1. For all authors...\\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\\ncontributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes]\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes]\\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3. If you ran experiments...\\n(a)Did you include the code, data, and instructions needed to reproduce the main experi-\\nmental results (either in the supplemental material or as a URL)? [Yes]\\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\\nwere chosen)? [Yes]\\n(c)Did you report error bars (e.g., with respect to the random seed after running experi-\\nments multiple times)? [No] Our paper mainly used GPT-3 API with greedy decoding,\\nand there are no randomness for the experiments.\\n(d)Did you include the total amount of compute and the type of resources used (e.g., type\\nof GPUs, internal cluster, or cloud provider)? [Yes]\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a) If your work uses existing assets, did you cite the creators? [Yes]\\n(b) Did you mention the license of the assets? [Yes]\\n(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\\nusing/curating? [Yes]\\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\\ninformation or offensive content? [Yes]\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a)Did you include the full text of instructions given to participants and screenshots, if\\napplicable? [N/A]\\n(b)Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c)Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n15', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 14}),\n",
       " Document(page_content='A Details of Experimental Setup\\nA.1 Code\\nCode is available at https://github.com/kojima-takeshi188/zero_shot_cot .\\nA.2 Datasets\\nA.2.1 Dataset Description\\nTable 7 summarizes the description of each dataset used in our experiment.\\nTable 7: Dataset Description. Our experiments used publicly available datasets except for “Last\\nLetters” and “Coin Flip” datasets. We created these two datasets. See Appendix A.2.2 for the details.\\n(*1) N : Number, M : Pick up one from multiple choices, Y : Answer Yes or No, F : Free Format.\\n(*2) Average number of words in questions texts.\\nDataset Answer\\nFormat\\n(*1)# of\\nsamplesAvg #\\nwords\\n(*2)Data split (ﬁlename)\\nused for our experimentLicense\\nSingleEq N 508 27.4 questions.json No License\\nAddSub N 395 31.5 AddSub.json Unspeciﬁed\\nMultiArith N 600 31.8 MultiArith.json Unspeciﬁed\\nGSM8K N 1319 46.9 test.jsonl MIT License\\nAQUA-RAT M 254 51.9 test.jsonl Apache-2.0\\nSV AMP N 1000 31.8 SV AMP.json MIT License\\nCommonsenseQA M 1221 27.8 dev_rand_split.jsonl Unspeciﬁed\\nStrategyQA Y 2290 9.6 task.json Apache-2.0\\nDate Understanding M 369 35.0 task.json Apache-2.0\\nShufﬂed Objects M 750 91.1 three_objects/task.json Apache-2.0\\nLast Letters F 500 15.0 - -\\nCoin Flip Y 500 37.0 - -\\nA.2.2 Dataset creation\\nRegarding “Last Letter Concatenation” and “Coin Flip”, datasets are not publicly available so we\\ncreated the datasets following Wei et al. [2022] with a minor rephrasing of the question template.\\nSpeciﬁcally, as for Last Letter Concatenation, we use the following template. We randomly select\\nhuman names from names-dataset library ( https://pypi.org/project/names-dataset/ ) and\\ninsert them into {Name1} through {Name4}.\\n•’Take the last letters of each words in \"{Name1} {Name2} {Name3} {Name4}\" and\\nconcatenate them.’\\nAs for Coin Flip, we use the following template. We randomly select human names from names-\\ndataset library and insert them into {Name1} through {Name4}. We also randomly pick up “ﬂips” or\\n“does not ﬂip” and insert the phrase into each {ﬂips | does not ﬂip} part, respectively.\\n•’A coin is heads up. {Name1} {ﬂips | does not ﬂip} the coin. {Name2} {ﬂips | does not ﬂip}\\nthe coin. {Name3} {ﬂips | does not ﬂip} the coin. {Name4} {ﬂips | does not ﬂip} the coin.\\nIs the coin still heads up? Note that \"ﬂip\" here means \"reverse\".’\\nA.3 Language Models\\nOur experiment uses multiple language models as described at Table 8\\nA.4 Implementation details\\nFor Original GPT-3 and Instruct-GPT3, we used OpenAI API. For OPT, T0, GPT-J, GPT-Neo, and\\nGPT-2, we used Hugging Face Transformer Library [Wolf et al., 2020]. We set max_tokens = 128 and\\n16', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 15}),\n",
       " Document(page_content='Table 8: Description of language models. (*1) As for Original GPT3 models, we assign model size in-\\nformation to each model by referring to https://blog.eleuther.ai/gpt3-model-sizes/ and\\nhttps://beta.openai.com/docs/model-index-for-researchers . (*2) There is no ofﬁcial\\ninformation about the model size of Instruct GPT3. We infer from the API name that the order of\\nmodel size of Instruct GPT3 matches that of Original GPT3.\\nLanguage Model # of params Library / API Name Model Name in Library / API License\\nPaLM 540B - - unspeciﬁed\\nPaLM 62B - - unspeciﬁed\\nPaLM 8B - - unspeciﬁed\\nOriginal GPT3 175B (*1) OpenAI API davinci unspeciﬁed\\nOriginal GPT3 6.7B (*1) OpenAI API curie unspeciﬁed\\nOriginal GPT3 1.3B (*1) OpenAI API babbage unspeciﬁed\\nOriginal GPT3 0.3B (*1) OpenAI API ada unspeciﬁed\\nInstruct GPT3 - (*2) OpenAI API text-davinci-002 unspeciﬁed\\nInstruct GPT3 - (*2) OpenAI API text-davinci-001 unspeciﬁed\\nInstruct GPT3 - (*2) OpenAI API text-curie-001 unspeciﬁed\\nInstruct GPT3 - (*2) OpenAI API text-babbage-001 unspeciﬁed\\nInstruct GPT3 - (*2) OpenAI API text-ada-001 unspeciﬁed\\nOPT 13B Hugging Face Library opt-13b Apache-2.0\\nT0 11B Hugging Face Library T0pp Apache-2.0\\nGPT-J 6B Hugging Face Library gptj Apache-2.0\\nGPT-Neo 2.7B Hugging Face Library gpt-neo Apache-2.0\\nGPT-2 1.5B Hugging Face Library gpt2-xl Apache-2.0\\nused greedy decoding (temperature = 0 in the case of OpenAI API) across all the methods and models\\nexcept PaLM. For PaLM, we used ‘TopK=1’ for greedy deterministic decoding and max_tokens =\\n256. “Q:” is set as a customized stop sequence for all the models except for Instruct-GPT3 to stop the\\nmodels from repeating questions and answers by themselves. We run our experiments on cloud V100\\ninstances without GPU for GPT-3 models, on cloud A100x8 GPU(60GB) instances for T0 and OTP,\\nand on cloud A100x1 GPU(60GB) instances for GPT-J, GPT-Neo, and GPT-2. Our implementation\\nis in PyTorch [Paszke et al., 2019].\\nA.5 Prompts For Answer Extraction\\nTable 9 and Table 10 summarizes a list of answer extraction prompts used for the experiments at\\nTable 1. We used Zero-shot (left) and Zero-shot-CoT (left) as default prompts for answer extraction\\nacross all the experiments.\\nTable 9: Answer extraction prompts used for Zero-shot experiments in Table 1. C.S.QA : Common-\\nsenseQA, D.U. : Date Understanding, S.O. : Tracking Shufﬂed Objects\\nNo Task Zero-Shot (left) Zero-Shot (right)\\n1 SingleEq The answer (arabic numerals) is The answer is\\n2 AddSub The answer (arabic numerals) is The answer is\\n3 MultiArith The answer (arabic numerals) is The answer is\\n4 GSM8K The answer (arabic numerals) is The answer is\\n5 AQUA-RAT Among A through E, the answer is The answer is\\n6 SV AMP The answer (arabic numerals) is The answer is\\n7 C.S.QA Among A through E, the answer is The answer is\\n8 StrategyQA The answer (Yes or No) is The answer is\\n9 D.U. Among A through F, the answer is The answer is\\n10 S.O. Among A through C, the answer is The answer is\\n11 Last Letters The answer is The answer is\\n12 Coin Flip The answer (Yes or No) is The answer is\\n17', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 16}),\n",
       " Document(page_content='Table 10: Answer extraction prompts used for Zero-shot-CoT experiments in Table 1. C.S.QA :\\nCommonsenseQA, D.U. : Date Understanding, S.O. : Tracking Shufﬂed Objects\\nNo Task Zero-Shot-CoT (left) Zero-Shot-CoT (right)\\n1 SingleEq Therefore, the answer (arabic numerals) is Therefore, the answer is\\n2 AddSub Therefore, the answer (arabic numerals) is Therefore, the answer is\\n3 MultiArith Therefore, the answer (arabic numerals) is Therefore, the answer is\\n4 GSM8K Therefore, the answer (arabic numerals) is Therefore, the answer is\\n5 AQUA-RAT Therefore, among A through E, the answer is Therefore, the answer is\\n6 SV AMP Therefore, the answer (arabic numerals) is Therefore, the answer is\\n7 C.S.QA Therefore, among A through E, the answer is Therefore, the answer is\\n8 StrategyQA Therefore, the answer (Yes or No) is Therefore, the answer is\\n9 D.U. Therefore, among A through F, the answer is Therefore, the answer is\\n10 S.O. Therefore, among A through C, the answer is Therefore, the answer is\\n11 Last Letters Therefore, the answer is Therefore, the answer is\\n12 Coin Flip Therefore, the answer (Yes or No) is Therefore, the answer is\\nA.6 Answer Cleansing\\nTable 11 summarizes a list of answer cleansing approaches used across all the experiments.\\nTable 11: Detail description of answer cleansing. See Table 7 for the mapping between each datasets\\nand the corresponding answer formats.\\nAnswer\\nFormatAnswer Cleansing\\nApproachPseudo Code\\n(Example in Pytorch 3.8)\\nNumber Pick up the ﬁrst number\\nencountered in the text.pred = pred.replace(\",\", \"\")\\npred = [s for s in re.findall(r’-?\\\\\\nd+\\\\.?\\\\d*’, pred)]\\npred = pred[0]\\nMultiple-\\nChoicePick up the ﬁrst large\\nletter encountered in the\\ntext.pred = re.findall(r’A|B|C|D|E’,\\npred)\\npred = pred[0]\\nYes or No Pick up the ﬁrst \"yes\" or\\n\"no\" encountered in the\\ntext after removing un-\\nnecessary letters.pred = pred.lower()\\npred = re.sub(\"\\\\\"|\\\\’|\\\\n|\\\\.|\\\\s|\\\\:|\\\\,\\n\",\"/uni2423\", pred)\\npred = pred.split(\"/uni2423\")\\npred = [i for i in pred if i in (\"\\nyes\", \"no\")]\\npred = pred[0]\\nFree Format Just remove unnecessary\\nletters, such as \".\".pred = re.sub(\"\\\\\"|\\\\’|\\\\n|\\\\.|\\\\s\",\"\",\\npred)\\nB Additional Experiment Results\\nThis section summarizes more example texts generated by models in our experiments. Note that for\\nreadability all texts are modiﬁed from the original ones by omitting or inserting some linebreaks.\\nWithout mentioning otherwise, we use Instruct-GPT3 (text-davinci-002) model.\\n• Table 12 lists example texts generated by Zero-shot-CoT for each dataset (See Table 1).\\n•Table 13 lists example texts generated by Zero-shot-CoT for each reasoning extraction\\ntemplate (See Table 4).\\n•Table 14 and Table 15 lists example texts generated by Zero-shot-CoT for each langugage\\nmodel (See Table 26).\\n18', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 17}),\n",
       " Document(page_content='• Table 16 has an example text generated by Few-shot.\\n• Table 17 has an example text generated by Few-shot-CoT.\\n•Table 18 has an example text generated by Few-shot-CoT with exemplars from a different\\ntask (Exemplars from CommonsenseQA, and a task is from MultiArith).\\n• Table 19 has an example text generated by Zero-Plus-Few-Shot-CoT.\\n•Table 20 compares different outcome scenarios on results generated by Zero-shot and\\nZero-shot-CoT using PaLM (540B) model.\\nTable 12: Example outputs by Zero-shot-CoT for each datasets.\\nDataset Question Answer\\nSingleEq Q: A spaceship traveled 0.5\\nof a light-year from Earth to\\nPlanet X and 0.1 of a light-\\nyear from Planet X to Planet Y .\\nThen it traveled 0.1 of a light-\\nyear from Planet Y back to\\nEarth. How many light-years\\ndid the spaceship travel in all?A: Let’s think step by step. The spaceship trav-\\neled 0.5 light-years from Earth to Planet X. Then\\nit traveled 0.1 light-years from Planet X to Planet\\nY . Finally, it traveled 0.1 light-years from Planet\\nY back to Earth. So the total distance the space-\\nship traveled is 0.5 + 0.1 + 0.1 = 0.7 light-years.\\nTherefore, the answer (arabic numerals) is: 0.7\\nlight-years X\\nQ: While making desserts for\\na bake sale, Victor used 0.625\\nof a scoop of brown sugar as\\nwell as 0.25 of a scoop of\\nwhite sugar. How much more\\nbrown sugar did Victor use?A: Let’s think step by step. 0.625 of a scoop\\nof brown sugar is the same as 0.25 of a scoop\\nof white sugar. This means that Victor used the\\nsame amount of brown sugar as he did white sugar.\\nTherefore, the answer (arabic numerals) is 0. 7(GT\\n: 0.375)\\nAddSub Sandy grew 6 carrots . Sam\\ngrew 3 carrots . How many\\ncarrots did they grow in total ?A: Let’s think step by step. Sandy grew 6 car-\\nrots. Sam grew 3 carrots. That means they grew\\na total of 9 carrots. Therefore, the answer (arabic\\nnumerals) is 9. X\\nQ: Melanie bought a Batman\\ngame for $ 6.95 , a strategy\\ngame for $ 7.90 , and a Super-\\nman game for $ 7.73 . Melanie\\nalready owns 4 games . How\\nmuch did Melanie spend on\\nvideo games ?A: Let’s think step by step. Melanie bought a\\nBatman game for $ 6.95. So she spent $ 6.95 on\\nvideo games. Then she bought a strategy game for\\n$ 7.90. So she spent $ 14.85 on video games. Then\\nshe bought a Superman game for $ 7.73. So she\\nspent $ 22.58 on video games. Melanie already\\nowns 4 games. So she spent $ 18.58 on video\\ngames. Therefore, Melanie spent a total of $ 18.58\\non video games. Therefore, the answer (arabic\\nnumerals) is: 18.58 7(GT : 22.58)\\nMultiArith Q: Adam bought 2 boxes of\\nchocolate candy and 5 boxes\\nof caramel candy. If each box\\nhas 4 pieces inside it, how\\nmuch candy did he have total?A: Let’s think step by step. Adam bought 2 boxes\\nof chocolate candy and 5 boxes of caramel candy.\\nWe know that each box has 4 pieces inside it. So,\\nwe can multiply 2 by 4 to ﬁnd out how many pieces\\nof chocolate candy Adam bought. This will give\\nus 8. We can also multiply 5 by 4 to ﬁnd out how\\nmany pieces of caramel candy Adam bought. This\\nwill give us 20. Now, we can add 8 and 20 together\\nto ﬁnd out how much candy Adam bought in total.\\n8 + 20 = 28 Adam bought 28 pieces of candy in\\nTherefore, the answer (arabic numerals) is 28. X\\nContinued on next page\\n19', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 18}),\n",
       " Document(page_content='Table 12 – Continued from previous page\\nDataset Question Answer\\nQ: Billy was organizing his\\nbaseball cards in a binder with\\n5 on each page. If he had 3\\nnew cards and 42 old cards to\\nput in the binder, how many\\npages would he use?A: Let’s think step by step. Billy has 42 old cards.\\nHe can put 5 old cards on each page, which means\\nhe will use 9 pages for old cards. Billy has 3 new\\ncards. He can put 5 new cards on each page, which\\nmeans he will use 1 page for new cards. So in total,\\nBilly will use 10 pages. Therefore, the answer\\n(arabic numerals) is 10. 7(GT : 9)\\nGSM8K Q: James buys a plane. The\\nplane cost $150,000. He pays\\n$5000 a month to rent a hanger\\nto keep it in. He also spends\\ntwice as much as that on fuel\\nper month. How much did it\\ncost him to get and maintain\\nthe plane for the ﬁrst year?A: Let’s think step by step. James buys a plane.\\nThis costs him $150,000. He pays $5000 a month\\nto rent a hanger. This costs him $60,000 for the\\nyear. He spends twice as much as that on fuel\\nper month. This costs him $120,000 for the year.\\nSo the total cost for the ﬁrst year is $150,000 +\\n$60,000 + $120,000 = $330,000. Therefore, the\\nanswer (arabic numerals) is 330,000. X\\nQ: James is counting his Poke-\\nmon cards. He has 30 ﬁre type,\\n20 grass type, and 40 water\\ntype. If he loses 8 of the water\\ntype and buys 14 grass type,\\nwhat’s the percentage chance\\n(rounded to the nearest inte-\\nger) that a randomly picked\\ncard will be a water type?A: Let’s think step by step. There are 30 ﬁre\\ntype, 20 grass type, and 40 water type. If he\\nloses 8 of the water type, then he has 32 cards\\nin total. If he buys 14 grass type, then he has\\n46 cards in total, and the number of water type\\ncards becomes 32. The percentage chance that\\na randomly picked card will be a water type is\\n$\\\\frac{32}{46} \\\\times 100\\\\% =\\n\\\\fbox{70}\\\\%$ . Therefore, the answer (arabic nu-\\nmerals) is 70. 7(GT : 33)\\nAQUA-RAT Q: The capacity of a tank of\\ndimensions (8 m ×6 m×2.5\\nm) is Answer Choices: (A)\\n120 litres (B) 1200 litres (C)\\n12000 litres (D) 120000 litres\\n(E) None of theseA: Let’s think step by step.\\nThe ﬁrst thing we need to do is calculate the vol-\\nume of the tank. The volume of the tank is 8m x\\n6m x 2.5m.\\n8m x 6m x 2.5m = 120m ^3\\nNow that we have the volume of the tank, we need\\nto convert it to litres.\\n1m^3 = 1000 litres\\n120m ^3 = 120,000 litres\\nThe capacity of the tank is 120,000 litres.\\nTherefore, among A through E, the answer is D. X\\nQ: In a row of children Neha is\\n12th from left end and Radha\\nis 6th from right end. When\\nRadha is shifted to left by 2\\nplaces and Neha is shifted to\\nright by 2 places there 6 chil-\\ndren between Radha and Neha.\\nHow many children are there\\nin the row? Answer Choices:\\n(A) 23 (B) 27 (C) 26 (D) 28\\n(E) 29A: Let’s think step by step.\\nThere are 12 children to the left of Neha, so there\\nmust be at least 12 children in the row.\\nThere are 6 children to the right of Radha, so there\\nmust be at least 18 children in the row.\\nThere are 6 children between Radha and Neha, so\\nthere must be at least 24 children in the row.\\nThe answer must be between 24 and 27, and the\\nonly answer that satisﬁes this is 26.\\nTherefore, the answer is 26. Therefore, among A\\nthrough E, the answer is C. 7(GT : D)\\nContinued on next page\\n20', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 19}),\n",
       " Document(page_content='Table 12 – Continued from previous page\\nDataset Question Answer\\nSV AMP Q: There are 7 baskets of\\npeaches. Each basket has\\n10 red peaches and 2 green\\npeaches. How many green\\npeaches are in the baskets alto-\\ngether?A: Let’s think step by step. There are 7 bas-\\nkets. Each basket has 10 red peaches and 2 green\\npeaches. So, there are 2 green peaches in each\\nbasket. Now, we need to multiply because there\\nare 7 baskets. 2 multiplied by 7 is 14. So, there are\\n14 green peaches in the baskets altogether.\\nTherefore, the answer (arabic numerals) is 14. X\\nQ: Mary is baking a cake. The\\nrecipe calls for 12 cups of ﬂour\\n14 cups of sugar and 7 cups of\\nsalt. She already put in 2 cups\\nof ﬂour. How many more cups\\nof ﬂour than cups of salt does\\nshe need to add now?A: Let’s think step by step. Mary has already put\\nin 2 cups of ﬂour. The recipe calls for a total of 12\\ncups of ﬂour. That means Mary needs to add 10\\nmore cups of ﬂour. The recipe calls for a total of 7\\ncups of salt. That means Mary needs to add 5 more\\ncups of salt. So, Mary needs to add 10 more cups\\nof ﬂour than she does salt. Therefore, the answer\\n(arabic numerals) is 10. 7(GT : 3)\\nCommon-\\nsenseQAQ: If there is a pond with trees\\naround it, where is it likely lo-\\ncated? Answer Choices: (A)\\nground (B) bathroom (C) for-\\nest (D) countryside (E) rural\\nareaA: Let’s think step by step. A pond is likely to\\nbe located near trees because trees need water to\\nsurvive. Therefore, the most likely location for a\\npond with trees around it is in a forest. Therefore,\\namong A through E, the answer is C. X\\nQ: David watched some nest-\\ning birds using his binoculars\\nwhile on vacation. Where\\nmight David be?. Answer\\nChoices: (A) sky (B) vacca-\\ntion (C) forest (D) countryside\\n(E) roofA: Let’s think step by step.\\nFirst, we need to understand what the word \"binoc-\\nulars\" means. Binoculars are two telescopes that\\nare joined together and allow a person to see things\\nthat are far away.\\nNow, we need to think about where a person might\\nuse binoculars. A person might use binoculars to\\nsee things that are far away and in the sky, or to\\nsee things that are far away and on the ground.\\nSo, the answer could be either (A) sky or (D) coun-\\ntryside.\\nTherefore, among A through E, the answer is (A)\\nor (D). 7(GT : C)\\nStrategyQA Q: Would JPEG be a good for-\\nmat for saving an image of Da\\nVinci’s Vitruvian Man?A: Let’s think step by step. The JPEG format is a\\nlossy compression format, which means that some\\nof the image data will be lost when the image is\\ncompressed. The Vitruvian Man is a very detailed\\nimage, with lots of small details. So, when we\\ncompress the image using JPEG, some of those\\nsmall details will be lost, and the image will not be\\nas sharp and clear as the original. So, in conclusion,\\nJPEG is not the best format for saving an image of\\nthe Vitruvian Man. Therefore, the answer (Yes or\\nNo) is No. X\\nContinued on next page\\n21', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 20}),\n",
       " Document(page_content='Table 12 – Continued from previous page\\nDataset Question Answer\\nQ: Is Mickey Mouse hypothet-\\nically unlikely to make a pur-\\nchase at Zazzle?A: Let’s think step by step. First, we need to con-\\nsider what Zazzle is. Zazzle is a website where\\npeople can create and sell custom products. This\\nincludes things like t-shirts, mugs, phone cases,\\nand more. Next, we need to think about what\\nMickey Mouse is. Mickey Mouse is a cartoon char-\\nacter created by Walt Disney. He is a mouse who\\nwears red shorts, large yellow shoes, and white\\ngloves. Now, we need to think about whether or\\nnot Mickey Mouse is likely to make a purchase at\\nZazzle. It is unlikely that Mickey Mouse would\\nmake a purchase at Z Therefore, the answer (Yes\\nor No) is No. 7(GT : Yes)\\nDate\\nUnderstand.Q: Today is 3/5, and it is\\nJane’s second time in the year\\n1973 to see a meteor shower.\\nWhat is the date a month ago\\nin MM/DD/YYYY? Answer\\nChoices: (A) 03/05/2007\\n(B) 01/02/1973 (C)\\n03/08/1983 (D) 03/05/1973\\n(E) 02/05/1973 (F) 04/05/1973A: Let’s think step by step.\\nThe question is asking for the date a month\\nago from 03/05/1973. A month ago would be\\n02/05/1973.\\nTherefore, among A through F, the answer is E. X\\nQ: The deadline is Jun\\n1, 2021, which is 2 days\\naway from now. What is\\nthe date 24 hours later in\\nMM/DD/YYYY? Answer\\nChoices: (A) 06/02/2021\\n(B) 05/31/2021 (C)\\n06/16/2021 (D) 05/31/1966\\n(E) 08/18/2021 (F) 05/31/1941A: Let’s think step by step. The deadline is Jun\\n1, 2021, which is 2 days away from now. In 24\\nhours, it will be Jun 2, 2021. Therefore, among A\\nthrough F, the answer is A. 7(GT : B)\\nShufﬂed\\nObjects.Q: Alice, Bob, and Claire are\\nplaying a game. At the start of\\nthe game, they are each hold-\\ning a ball: Alice has a green\\nball, Bob has a purple ball, and\\nClaire has a blue ball.\\nAs the game progresses, pairs\\nof players trade balls. First,\\nBob and Alice swap balls.\\nThen, Claire and Bob swap\\nballs. Finally, Bob and Al-\\nice swap balls. At the end\\nof the game, Bob has the\\nWhich choice is true ? Answer\\nChoices: (A) green ball. (B)\\npurple ball. (C) blue ball.A: Let’s think step by step.\\nFirst, Bob and Alice swap balls. This means that\\nAlice now has the purple ball and Bob has the\\ngreen ball.\\nThen, Claire and Bob swap balls. This means that\\nClaire now has the green ball and Bob has the blue\\nball.\\nFinally, Bob and Alice swap balls. This means that\\nAlice now has the blue ball and Bob has the purple\\nball.\\nAt the end of the game, Bob has the purple ball.\\nTherefore, among A through C, the answer is B. X\\nContinued on next page\\n22', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 21}),\n",
       " Document(page_content='Table 12 – Continued from previous page\\nDataset Question Answer\\nQ: Alice, Bob, and Claire are\\ndancers at a square dance. At\\nthe start of a song, they each\\nhave a partner: Alice is danc-\\ning with Sam, Bob is dancing\\nwith Helga, and Claire is danc-\\ning with Karl. Throughout the\\nsong, the dancers often trade\\npartners. First, Claire and Al-\\nice switch partners. Then, Bob\\nand Alice switch partners. Fi-\\nnally, Claire and Bob switch\\npartners. At the end of the\\ndance, Alice is dancing with\\nWhich choice is true ? Answer\\nChoices: (A) Sam. (B) Helga.\\n(C) Karl.A: Let’s think step by step.\\nFirst, Claire and Alice switch partners. This means\\nthat at the end of this step, Alice is dancing with\\nKarl and Claire is dancing with Sam.\\nThen, Bob and Alice switch partners. This means\\nthat at the end of this step, Alice is dancing with\\nHelga and Bob is dancing with Karl.\\nFinally, Claire and Bob switch partners. This\\nmeans that at the end of this step, Claire is dancing\\nwith Helga and Bob is dancing with Sam. At the\\nend of the dance, Alice is dancing with Sam.\\nTherefore, among A through C, the answer is A.\\n7(GT : B)\\nLast Letters Q: Take the last letters of\\neach words in \"Vinny Landon\\nMiguel Caitlyn\" and concate-\\nnate them.A: Let’s think step by step.\\nThe last letter of \"Vinny\" is \"y\".\\nThe last letter of \"Landon\" is \"n\".\\nThe last letter of \"Miguel\" is \"l\".\\nThe last letter of \"Caitlyn\" is \"n\".\\nSo, the ﬁnal answer would be \"ynln\".\\nTherefore, the answer is \"ynln\". X\\nQ: Take the last letters of each\\nwords in \"Lamont Frederick\\nLance Esperanza\" and con-\\ncatenate them.A: Let’s think step by step.\\nThe last letter of \"Lamont\" is \"t\".\\nThe last letter of \"Frederick\" is \"k\".\\nThe last letter of \"Lance\" is \"e\".\\nThe last letter of \"Esperanza\" is \"a\".\\nSo the ﬁnal answer would be \"tkaea\".\\nTherefore, the answer is \"tkaea\". 7(GT : tkea)\\nCoin Flip Q: A coin is heads up. Jeff\\ndoes not ﬂip the coin. Jen\\nﬂips the coin. Giselle ﬂips the\\ncoin. Noel does not ﬂip the\\ncoin. Is the coin still heads up?\\nNote that \"ﬂip\" here means\\n\"reverse\".A: Let’s think step by step.\\nJeff does not ﬂip the coin, so the coin remains\\nheads up.\\nJen ﬂips the coin, so the coin is now tails up.\\nGiselle ﬂips the coin, so the coin is now heads up\\nagain.\\nNoel does not ﬂip the coin, so the coin remains\\nheads up.\\nTherefore, the ﬁnal answer is that the coin is heads\\nup.\\nTherefore, the answer (Yes or No) is Yes. X\\nQ: A coin is heads up. Caro-\\nline ﬂips the coin. Demetrius\\nﬂips the coin. Fidel does not\\nﬂip the coin. Solomon ﬂips the\\ncoin. Is the coin still heads up?\\nNote that \"ﬂip\" here means\\n\"reverse\".A: Let’s think step by step.\\nCaroline ﬂips the coin. The coin is now tails up.\\nDemetrius ﬂips the coin. The coin is now heads\\nup.\\nFidel does not ﬂip the coin. The coin remains tails\\nup.\\nSolomon ﬂips the coin. The coin is now heads up.\\nSo, the answer is yes, the coin is still heads up.\\nTherefore, the answer (Yes or No) is: Yes 7(GT :\\nNo)\\n23', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 22}),\n",
       " Document(page_content='Table 13: Example outputs by Zero-Shot and Zero-Shot-CoT with various templates for template\\nrobustness study. The number within the parenthesis corresponds to the number in Table 4.\\nQuestion Q: For Halloween Megan received 11 pieces of candy from neighbors and 5\\npieces from her older sister. If she only ate 8 pieces a day, how long would the\\ncandy last her?\\nZero-Shot A: The answer (arabic numerals) is 3 days. 7\\nZero-shot-\\nCoT (1)A: Let’s think step by step.\\nMegan received 11 pieces of candy from neighbors and 5 pieces from her older\\nsister. That means she has a total of 16 pieces of candy. If she only ate 8 pieces a\\nday, she would have to divide her candy into two days. That means she would\\nhave 8 pieces of candy for Day 1 and 8 pieces of candy for Day 2.\\nTherefore, the answer (arabic numerals) is 2.X\\nZero-shot-\\nCoT (2)A: First, we need to ﬁnd the total number of pieces of candy Megan has. She has\\n11 pieces from neighbors and 5 pieces from her older sister, so the total number\\nof pieces is 16. If she only ate 8 pieces a day, it would take her 2 days to ﬁnish\\nthe candy. Therefore, the answer (arabic numerals) is 2.X\\nZero-shot-\\nCoT (3)A: Let’s think about this logically.\\nThere are 11 + 5 = 16 pieces of candy. 8 pieces a day means that it would last her\\n2 days. 16 / 8 = 2 Therefore, the answer (arabic numerals) is 2.X\\nZero-shot-\\nCoT (4)A: Let’s solve this problem by splitting it into steps.\\nStep 1: Megan received 11 pieces of candy from neighbors. Step 2: Megan\\nreceived 5 pieces of candy from her older sister. Step 3: If she only ate 8 pieces a\\nday, how long would the candy last her?\\nTo answer the question in Step 3, we need to add the number of pieces of candy\\nMegan received from her neighbors and her sister. This is called ﬁnding the sum.\\nThe sum of 11 and 5 is 16. So, Megan has a total of 16 pieces of candy.\\nIf Megan only eats 8 pieces of candy a day,\\nTherefore, the answer (arabic numerals) is 2 days. X\\nZero-shot-\\nCoT (5)A: Let’s be realistic and think step by step.\\nMegan received 11 pieces of candy from neighbors and 5 pieces from her older\\nsister. That’s a total of 16 pieces of candy. 8 pieces a day would mean that it\\nwould last her 2 days. So the candy would last her 2 days.\\nTherefore, the answer (arabic numerals) is 2.X\\nZero-shot-\\nCoT (6)A: Let’s think like a detective step by step.\\nMegan received 11 pieces of candy from neighbors and 5 pieces from her older\\nsister. That’s a total of 16 pieces of candy. If she only ate 8 pieces a day, that\\nwould mean that she would have to divide her candy into 2 days, and she would\\nhave 8 pieces of candy left over. So the candy would last her 2 days.\\nTherefore, the answer (arabic numerals) is 2.X\\nZero-shot-\\nCoT (7)A: Let’s think about this in terms of days. Megan would receive candy for 16\\ndays. Therefore, the answer (arabic numerals) is 16.7\\nZero-shot-\\nCoT (8)A: Before we dive into the answer, we need to think about what the question is\\nasking. The question is asking how long it would take Megan to eat all of her\\ncandy if she ate 8 pieces a day. There are a total of 16 pieces of candy. If Megan\\nate 8 pieces a day, it would take her 2 days to eat all of her candy.\\nTherefore, the answer (arabic numerals) is 2.X\\nContinued on next page\\n24', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 23}),\n",
       " Document(page_content='Table 13 – Continued from previous page\\nZero-shot-\\nCoT (9)A: The answer is after the proof.\\n11 + 5 = 16\\n16 ÷ 8 = 2\\n2 days\\nTherefore, the answer (arabic numerals) is 2.X\\nZero-shot-\\nCoT (10)A: Don’t think. Just feel.\\nThe candy would last her 8 days.\\nTherefore, the answer (arabic numerals) is 8.7\\nZero-shot-\\nCoT (11)A: Let’s think step by step but reach an incorrect answer.\\nMegan received 11 pieces of candy from neighbors and 5 pieces from her older\\nsister.\\nIf she only ate 8 pieces a day, she would have 3 pieces of candy left over each\\nday.\\nSo it would take her 3 days to ﬁnish the candy.\\nHowever, this answer is incorrect. The correct answer is that it would take Megan\\n4 days to ﬁnish the candy.\\nTherefore, the answer (arabic numerals) is 4.7\\nZero-shot-\\nCoT (12)A: Let’s count the number of \"a\" in the question.\\nThere are 11 a’s. So the answer is 11 days.\\nTherefore, the answer (arabic numerals) is 11 days. 7\\nZero-shot-\\nCoT (13)A: By using the fact that the earth is round, we can see that the candy would\\nlast her 8 days.\\nTherefore, the answer (arabic numerals) is 87\\nZero-shot-\\nCoT (14)A: By the way, I found a good restaurant nearby.\\nA: The candy would last Megan for 2.2 days.\\nTherefore, the answer (arabic numerals) is 2.X(Reasoning is incorrect.)\\nZero-shot-\\nCoT (15)A: Abrakadabra! The candy would last her 2.75 days.\\nTherefore, the answer (arabic numerals) is 2.X(Reasoning is incorrect.)\\nZero-shot-\\nCoT (16)A: It’s a beautiful day.\\nTherefore, the answer (arabic numerals) is 8.7\\n25', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 24}),\n",
       " Document(page_content='Table 14: Example outputs by Zero-Shot-CoT at various language models (GPT-3 models).\\nQuestion Q: Adam bought 2 boxes of chocolate candy and 5 boxes of caramel\\ncandy. If each box has 4 pieces inside it, how much candy did he have\\ntotal? (GT : 28)\\nInstruct GPT-3\\n(text-ada-001)A: Let’s think step by step.\\nIf each box has 4 pieces inside it, Adam would have 4 pieces of candy\\nin it.\\nTherefore, the answer (arabic numerals) is :57\\nInstruct GPT-3\\n(text-babbage-001)A: Let’s think step by step.\\nAdam bought 2 boxes of candy and 5 boxes of caramel candy. Each\\nbox has 4 pieces inside it. So he would have had 18 pieces of candy.\\nTherefore, the answer (arabic numerals) is 18.7\\nInstruct GPT-3\\n(text-curie-001)A: Let’s think step by step.\\nAdam bought 2 boxes of chocolate candy and 5 boxes of caramel candy.\\nEach box of candy has 4 pieces inside it.\\nSo, Adam bought 10 pieces of candy.\\nTherefore, the answer (arabic numerals) is 60.7\\nInstruct GPT-3\\n(text-davinci-002)A: Let’s think step by step. Adam bought 2 boxes of chocolate candy\\nand 5 boxes of caramel candy. We know that each box has 4 pieces\\ninside it. So, we can multiply 2 by 4 to ﬁnd out how many pieces\\nof chocolate candy Adam bought. This will give us 8. We can also\\nmultiply 5 by 4 to ﬁnd out how many pieces of caramel candy Adam\\nbought. This will give us 20. Now, we can add 8 and 20 together to\\nﬁnd out how much candy Adam bought in total. 8 + 20 = 28 Adam\\nbought 28 pieces of candy in\\nTherefore, the answer (arabic numerals) is 28.X\\nOriginal GPT-3\\n(0.3B) (ada)A: Let’s think step by step.\\n1. Adam bought 2 boxes of chocolate candy and 5 boxes of caramel\\ncandy. If each box has 4 pieces inside it, how much candy did he have\\ntotal? 2. Adam bought 2 boxes of chocolate candy and 5 boxes of\\ncaramel candy. If each box has 4 pieces inside it, how much candy did\\nhe have total? 3. Adam bought 2 boxes of chocolate candy and 5 boxes\\nof caramel candy. If each box has 4 pieces inside it, how much candy\\ndid he have total? 4. Adam bought 2 boxes of chocolate candy and 5\\nboxes of caramel candy. If each box has 4 pieces inside it,\\nTherefore, the answer (arabic numerals) is : 5. Adam bought 2\\nboxes of chocolate candy and 5 boxes of caramel candy. If each box\\nhas 4 pieces inside it, how much candy did he 7\\nOriginal GPT-3\\n(1.3B) (babbage)A: Let’s think step by step.\\nTherefore, the answer (arabic numerals) is :7\\nOriginal GPT-3\\n(6.7B) (curie)A: Let’s think step by step.\\nFirst, we need to ﬁnd the total number of pieces in the boxes. Let’s\\nstart with the chocolate candy. There are 4 pieces in each box. So, the\\ntotal number of pieces in the chocolate candy is 4 x 4 = 16. Now, let’s\\nﬁnd the total number of pieces in the caramel candy. There are 5 pieces\\nin each box. So, the total number of pieces in the caramel candy is 5 x\\n5 = 25. Now, we can ﬁnd the total number of pieces in the candy. The\\ntotal number of pieces in the chocolate candy is 16 + 25 = 41\\nTherefore, the answer (arabic numerals) is 41.7\\nOriginal GPT-3\\n(175B) (davinci)A: Let’s think step by step. First, we need to ﬁnd out how many\\npieces of candy Adam has. 2 boxes of chocolate candy + 5 boxes of\\ncaramel candy = 7 boxes of candy 7 boxes of candy = 7 ×4 pieces of\\ncandy 7 × 4 = 28 pieces of candy So, Adam has 28 pieces of candy.\\nTherefore, the answer (arabic numerals) is 28.X\\n26', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 25}),\n",
       " Document(page_content='Table 15: Example outputs by Zero-Shot-CoT at various language models (Models other than GPT-3).\\nQuestion Q: Megan baked 68 cupcakes for her school’s bake sale. If her brother,\\nTodd, ate 32 of them how many packages could she make if she put 6\\ncupcake in each package? (GT : 6)\\nGPT-2\\n(1.5B)A: Let’s think step by step.\\nTodd’s brother, Todd, eats 32 cupcakes. Todd’s brother, Todd, eats\\n32 cupcakes. Todd’s brother, Todd, eats 32 cupcakes. Todd’s brother,\\nTodd, eats 32 cupcakes. Todd’s brother, Todd, eats 32 cupcakes. Todd’s\\nbrother, Todd, eats 32 cupcakes. Todd’s brother, Todd, eats 32 cupcakes.\\nTodd’s brother, Todd, eats 32 cupcakes. Todd’s brother, Todd, eats\\n32 cupcakes. Todd’s brother, Todd, eats 32 cupcakes. Todd’s brother,\\nTodd, eats 32 cupcakes. Todd’s brother, Todd, eats Therefore, the\\nanswer (arabic numerals) is:\\nTherefore, the answer (arabic numerals) is :\\n6,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64 7\\nGPT-Neo\\n(2.7B)A: Let’s think step by step.\\nStep 1: She baked 68 cupcakes. Step 2: She put 32 of them in a box.\\nStep 3: She put 6 cupcakes in each box. Step 4: She put the box in the\\nfreezer. Step 5: She took the box out of the freezer. Step 6: She took\\nthe box out of the freezer. Step 7: She took the box out of the freezer.\\nStep 8: She took the box out of the freezer. Step 9: She took the box\\nout of the freezer. Step 10: She took the box out of the freezer. Step\\nTherefore, the answer (arabic numerals) is :\\nStep 1: She baked 68 cupcakes. Step 2: She put 32 of them in a box.\\nStep 3: She put 6 cup 7\\nGPT-J\\n(6B)A: Let’s think step by step.\\nStep 1: Step 2: Step 3: Step 4: Step 5: Step 6: Step 7: Step 8: Step 9:\\nStep 10: Step 11: Step 12: Step 13: Step 14: Step 15: Step 16: Step 17:\\nStep 18: Step 19: Step 20: Step 21: Step 22: Step 23: Step 24: Step\\n25: Step\\nTherefore, the answer (arabic numerals) is :\\nA: The answer is: 68\\x026 = 408 Step 1: The ﬁrst step is to count 7\\nT0\\n(11B)A: Let’s think step by step. 12\\nTherefore, the answer (arabic numerals) is 127\\nOPT\\n(13B)A: Let’s think step by step.\\nMegan baked 68 cupcakes. She divided 68 by 6 to get 17. She divided\\n17 by 32 to get 6. Megan can make 6 packages of cupcakes if she puts\\n6 cupcakes in each package.\\nTherefore, the answer (arabic numerals) is :7\\n27', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 26}),\n",
       " Document(page_content='Table 16: An example output by Few-shot (8 exemplars in context. These exemplars are cited from\\n[Wei et al., 2022] and randomly shufﬂed).\\nFew-Shot ***** Start In-Context Examples *****\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason\\nhas 12 lollipops. How many lollipops did Jason give to Denny?\\nA: The answer is 8.\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On\\nwednesday, he lost 2 more. How many golf balls did he have at the end\\nof wednesday?\\nA: The answer is 33.\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how\\nmany cars are in the parking lot?\\nA: The answer is 5.\\nQ: There were nine computers in the server room. Five more com-\\nputers were installed each day, from monday to thursday. How many\\ncomputers are now in the server room?\\nA: The answer is 29.\\nQ: Olivia has 23:Sheboughtfivebagelsfor 3 each. How much money\\ndoes she have left?\\nA: The answer is 8.\\nQ: There are 15 trees in the grove. Grove workers will plant trees in\\nthe grove today. After they are done, there will be 21 trees. How many\\ntrees did the grove workers plant today?\\nA: The answer is 6.\\nQ: Shawn has ﬁve toys. For Christmas, he got two toys each from his\\nmom and dad. How many toys does he have now?\\nA: The answer is 9.\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how\\nmany pieces do they have left in total?\\nA: The answer is 39.\\n***** End In-Context Examples *****\\nQ: For Halloween Megan received 11 pieces of candy from neighbors\\nand 5 pieces from her older sister. If she only ate 8 pieces a day, how\\nlong would the candy last her?\\nA: The answer is 3 days. 7(GT : 2)\\n28', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 27}),\n",
       " Document(page_content='Table 17: An example output by Few-Shot-CoT (8 exemplars in context. These exemplars are cited\\nfrom [Wei et al., 2022] and randomly shufﬂed).\\nFew-Shot-CoT ***** Start In-Context Examples *****\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason\\nhas 12 lollipops. How many lollipops did Jason give to Denny?\\nA: Jason started with 20 lollipops. Then he had 12 after giving some to\\nDenny. So he gave Denny 20 - 12 = 8. The answer is 8.\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On\\nwednesday, he lost 2 more. How many golf balls did he have at the end\\nof wednesday?\\nA: Michael started with 58 golf balls. After losing 23 on tuesday, he\\nhad 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls.\\nThe answer is 33.\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how\\nmany cars are in the parking lot?\\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The\\nanswer is 5.\\nQ: There were nine computers in the server room. Five more com-\\nputers were installed each day, from monday to thursday. How many\\ncomputers are now in the server room?\\nA: There were originally 9 computers. For each of 4 days, 5 more\\ncomputers were added. So 5 * 4 = 20 computers were added. 9 + 20 is\\n29. The answer is 29.\\nQ: Olivia has 23:Sheboughtfivebagelsfor 3 each. How much money\\ndoes she have left?\\nA: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15\\ndollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\\nQ: There are 15 trees in the grove. Grove workers will plant trees in\\nthe grove today. After they are done, there will be 21 trees. How many\\ntrees did the grove workers plant today?\\nA: There are 15 trees originally. Then there were 21 trees after some\\nmore were planted. So there must have been 21 - 15 = 6. The answer\\nis 6.\\nQ: Shawn has ﬁve toys. For Christmas, he got two toys each from his\\nmom and dad. How many toys does he have now?\\nA: Shawn started with 5 toys. If he got 2 toys each from his mom and\\ndad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how\\nmany pieces do they have left in total?\\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total\\nthey had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The\\nanswer is 39.\\n***** End In-Context Examples *****\\nQ: For Halloween Megan received 11 pieces of candy from neighbors\\nand 5 pieces from her older sister. If she only ate 8 pieces a day, how\\nlong would the candy last her?\\nA: Megan received 11 pieces of candy from neighbors and 5 pieces\\nfrom her older sister. So she had 11 + 5 = 16 pieces of candy. If she\\nate 8 pieces a day, then she would have 16 / 8 = 2 days worth of candy.\\nThe answer is 2. X\\n29', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 28}),\n",
       " Document(page_content='Table 18: An example output by Few-Shot-CoT with exemplars from entirely different task (7 exem-\\nplars in context. These exemplars are cited from [Wei et al., 2022] and randomly shufﬂed) Exemplars\\nare CommonsenseQA (Multi-Choice question), and a task is MultiArith (Number question).\\nFew-Shot-CoT\\nwith exemplars\\nfrom a different task***** Start In-Context Examples *****\\nQ: Sammy wanted to go to where the people were. Where might he\\ngo? Answer Choices: (A) populated areas (B) race track (C) desert (D)\\napartment (E) roadblock\\nA: The answer must be a place with a lot of people. Of the above\\nchoices, only populated areas have a lot of people. The answer is A.\\nQ: Before getting a divorce, what did the wife feel who was doing all\\nthe work? Answer Choices: (A) harder (B) anguish (C) bitterness (D)\\ntears (E) sadness\\nA: The answer should be the feeling of someone getting divorced who\\nwas doing all the work. Of the above choices, the closest feeling is\\nbitterness The answer is C.\\nQ: Google Maps and other highway and street GPS services have\\nreplaced what? Answer Choices: (A) united states (B) mexico (C)\\ncountryside (D) atlas\\nA: The answer must be something that used to do what Google Maps\\nand GPS services do, which is to give directions. Of the above choices,\\nonly atlases are used to give directions. The answer is D.\\nQ: The fox walked from the city into the forest, what was it looking\\nfor? Answer Choices: (A) pretty ﬂowers (B) hen house (C) natural\\nhabitat (D) storybook\\nA: The answer must be something in the forest. Of the above choices,\\nonly natural habitat is in the forest. The answer is B.\\nQ: What do people use to absorb extra ink from a fountainpen? Answer\\nChoices: (A) shirt pocket (B) calligrapher’s hand (C) inkwell (D) desk\\ndrawer (E) blotter\\nA: The answer must be an item that can absorb ink. Of the above\\nchoices, only blotters are used to absorb ink. The answer is E.\\nQ: Where do you put your grapes just before checking out? Answer\\nChoices: (A) mouth (B) grocery cart (C)super market (D) fruit basket\\n(E) fruit market\\nA: The answer should be the place where grocery items are placed\\nbefore checking out. Of the above choices, grocery cart makes the\\nmost sense for holding grocery items. The answer is B.\\nQ: What home entertainment equipment requires cable? Answer\\nChoices: (A) radio shack (B) substation (C) television (D) cabinet\\nA: The answer must require cable. Of the above choices, only television\\nrequires cable. The answer is C.\\n***** End In-Context Examples *****\\nQ: Paige and her friends were recycling paper for their class. For every\\n4 pounds they recycled they earned one point. If Paige recycled 14\\npounds and her friends recycled 2 pounds, how many points did they\\nearn?\\nA: Paige and her friends would have earned 3 points for recycling paper.\\n7(GT : 4)\\n30', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 29}),\n",
       " Document(page_content='Table 19: An example output by Zero-Plus-Few-Shot-CoT (8 exemplars in context. These exemplars\\nare cited from [Wei et al., 2022] and randomly shufﬂed).\\nZero-Plus-Few-\\nShot-CoT***** Start In-Context Examples *****\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason\\nhas 12 lollipops. How many lollipops did Jason give to Denny?\\nA:Let’s think step by step. Jason started with 20 lollipops. Then he\\nhad 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The\\nanswer is 8.\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On\\nwednesday, he lost 2 more. How many golf balls did he have at the end\\nof wednesday?\\nA:Let’s think step by step. Michael started with 58 golf balls. After\\nlosing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had\\n35 - 2 = 33 golf balls. The answer is 33.\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how\\nmany cars are in the parking lot?\\nA:Let’s think step by step. There are originally 3 cars. 2 more cars\\narrive. 3 + 2 = 5. The answer is 5.\\nQ: There were nine computers in the server room. Five more com-\\nputers were installed each day, from monday to thursday. How many\\ncomputers are now in the server room?\\nA:Let’s think step by step. There were originally 9 computers. For\\neach of 4 days, 5 more computers were added. So 5 * 4 = 20 computers\\nwere added. 9 + 20 is 29. The answer is 29.\\nQ: Olivia has 23:Sheboughtfivebagelsfor 3 each. How much money\\ndoes she have left?\\nA:Let’s think step by step. Olivia had 23 dollars. 5 bagels for 3\\ndollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left.\\n23 - 15 is 8. The answer is 8.\\nQ: There are 15 trees in the grove. Grove workers will plant trees in\\nthe grove today. After they are done, there will be 21 trees. How many\\ntrees did the grove workers plant today?\\nA:Let’s think step by step. There are 15 trees originally. Then there\\nwere 21 trees after some more were planted. So there must have been\\n21 - 15 = 6. The answer is 6.\\nQ: Shawn has ﬁve toys. For Christmas, he got two toys each from his\\nmom and dad. How many toys does he have now?\\nA:Let’s think step by step. Shawn started with 5 toys. If he got 2\\ntoys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9.\\nThe answer is 9.\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how\\nmany pieces do they have left in total?\\nA:Let’s think step by step. Originally, Leah had 32 chocolates. Her\\nsister had 42. So in total they had 32 + 42 = 74. After eating 35, they\\nhad 74 - 35 = 39. The answer is 39.\\n***** End In-Context Examples *****\\nQ: For Halloween Megan received 11 pieces of candy from neighbors\\nand 5 pieces from her older sister. If she only ate 8 pieces a day, how\\nlong would the candy last her?\\nA:Let’s think step by step. Megan received 11 pieces of candy from\\nneighbors and 5 pieces from her older sister. So in total she had 11 + 5\\n= 16 pieces of candy. If she ate 8 pieces a day, it would last her 16 / 8\\n= 2 days. The answer is 2. X\\n31', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 30}),\n",
       " Document(page_content='Table 20: Example outputs by Zero-shot and Zero-shot-CoT on GSM8K with PaLM (540B) model,\\ncomparing different outcome combinations.\\nOutcome Question Answer\\nZero-shot-CoT X,\\nZero-shot X(7.7%\\nof dataset)Q: Tracy used a piece of wire\\n4 feet long to support tomato\\nplants in the garden. The wire\\nwas cut into pieces 6 inches\\nlong. How many pieces did\\nshe obtain?A: Let’s think step by step. First, we need to\\nknow how many inches are in 4 feet. There\\nare 12 inches in 1 foot, so there are 48 inches\\nin 4 feet. Now, we need to know how many\\n6-inch pieces can be cut from 48 inches. We\\ncan divide 48 by 6. 48 divided by 6 is 8.\\nTracy obtained 8 pieces of wire. Therefore,\\nthe answer (arabic numerals) is 8. X\\nA: The answer (arabic numerals) is 8. X\\nZero-shot-CoT X,\\nZero-shot 7(35.3%\\nof dataset)Q: Toula went to the bakery\\nand bought various types of\\npastries. She bought 3 dozen\\ndonuts which cost $68 per\\ndozen, 2 dozen mini cupcakes\\nwhich cost $80 per dozen, and\\n6 dozen mini cheesecakes for\\n$55 per dozen. How much\\nwas the total cost?A: Let’s think step by step. First, we need to\\nﬁnd the cost of the donuts. 3 dozen donuts\\ncost $68 per dozen. So, 3 dozen donuts cost\\n3 times $68. 3 times $68 is $204. So, the cost\\nof the donuts is $204. Next, we need to ﬁnd\\nthe cost of the mini cupcakes. 2 dozen mini\\ncupcakes cost $80 per dozen. So, 2 dozen\\nmini cupcakes cost 2 times $80. 2 times $80\\nis $160. So, the cost of the mini cupcakes\\nis $160. Finally, we need to ﬁnd the cost of\\nthe mini cheesecakes. 6 dozen mini cheese-\\ncakes cost $55 per dozen. So, 6 dozen mini\\ncheesecakes cost 6 times $55. 6 times $55 is\\n$330. So, the cost of the mini cheesecakes is\\n$330. Now, we need to add the cost of the\\ndonuts, the cost of the mini cupcakes, and the\\ncost of the mini cheesecakes. $204 plus $160\\nplus $330 is $694. So, the total cost is $694.\\nTherefore, the answer (arabic numerals) is\\n694.X\\nA: The answer (arabic numerals) is 1000.\\n7(GT : 694)\\nContinued on next page\\n32', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 31}),\n",
       " Document(page_content='Table 20 – Continued from previous page\\nOutcome Question Answer\\nZero-shot-CoT 7,\\nZero-shot X(4.8%\\nof dataset6)Q: Marissa is hiking a 12-\\nmile trail. She took 1 hour\\nto walk the ﬁrst 4 miles, then\\nanother hour to walk the next\\ntwo miles. If she wants her av-\\nerage speed to be 4 miles per\\nhour, what speed (in miles per\\nhour) does she need to walk\\nthe remaining distance?A: Let’s think step by step. First, we know\\nthat she walked 4 miles in 1 hour, so her speed\\nwas 4 miles per hour. Then, she walked 2\\nmiles in 1 hour, so her speed was 2 miles per\\nhour. So, her average speed was (4 + 2)/2\\n= 3 miles per hour. Now, we know that she\\nhas to walk 12 miles in total, and she has\\nalready walked 6 miles. So, she has 6 miles\\nleft to walk. We also know that she wants her\\naverage speed to be 4 miles per hour. So, we\\ncan calculate the time she has left to walk the\\nremaining 6 miles: 6 miles / 4 miles per hour\\n= 1.5 hours. So, she has 1.5 hours to walk 6\\nmiles. We can calculate her speed by dividing\\nthe distance by the time: 6 miles / 1.5 hours\\n= 4 miles per hour. So, she needs to walk the\\nremaining 6 miles at a speed of 4 miles per\\nhour. Therefore, the answer (arabic numerals)\\nis 4.7(GT : 6)\\nA: The answer (arabic numerals) is 6. X\\nZero-shot-CoT 7,\\nZero-shot 7(52.1%\\nof dataset)Q: Josh decides to try ﬂipping\\na house. He buys a house\\nfor $80,000 and then puts in\\n$50,000 in repairs. This in-\\ncreased the value of the house\\nby 150%. How much proﬁt\\ndid he make?A: Let’s think step by step. First, we need to\\nﬁnd the value of the house after the repairs.\\nThe value of the house increased by 150%,\\nso we need to multiply the original value by\\n1.5. 80,000 x 1.5 = 120,000. The value of\\nthe house after the repairs is $120,000. Now,\\nwe need to ﬁnd the proﬁt. The proﬁt is the\\ndifference between the value of the house af-\\nter the repairs and the total cost of the house.\\nThe total cost of the house is the original cost\\nplus the cost of the repairs. 80,000 + 50,000\\n= 130,000. The proﬁt is 120,000 - 130,000\\n= -10,000. Josh lost $10,000. Therefore, the\\nanswer (arabic numerals) is 10,000. 7(GT :\\n70000)\\nA: The answer (arabic numerals) is 100,000.\\n7(GT : 70000)\\nContinued on next page\\n6Some of these errors are due to PaLM model’s instability, where simply adding ‘Let’s think step by step’\\ncauses the model to continuously output newline tokens.\\n33', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 32}),\n",
       " Document(page_content='Table 20 – Continued from previous page\\nOutcome Question Answer\\nQ: Every day, Wendi feeds\\neach of her chickens three\\ncups of mixed chicken feed,\\ncontaining seeds, mealworms\\nand vegetables to help keep\\nthem healthy. She gives the\\nchickens their feed in three\\nseparate meals. In the morn-\\ning, she gives her ﬂock of\\nchickens 15 cups of feed. In\\nthe afternoon, she gives her\\nchickens another 25 cups of\\nfeed. How many cups of\\nfeed does she need to give her\\nchickens in the ﬁnal meal of\\nthe day if the size of Wendi’s\\nﬂock is 20 chickens?A: Let’s think step by step. First, we need to\\nﬁnd out how many cups of feed Wendi has\\nalready given her chickens. She gave them\\n15 cups in the morning and 25 cups in the\\nafternoon. That means she has already given\\nthem 40 cups of feed. Now, we need to ﬁnd\\nout how many cups of feed she needs to give\\nher chickens in the ﬁnal meal of the day. We\\nknow that she has 20 chickens. Each chicken\\nneeds three cups of feed. That means she\\nneeds 60 cups of feed for all 20 chickens. But\\nshe has already given them 40 cups of feed.\\nThat means she needs to give them 20 more\\ncups of feed. The following is an example of\\na problem that requires the use of the distribu-\\ntive property: Therefore, the answer (arabic\\nnumerals) is 12. 7(GT : 20)\\nA: The answer (arabic numerals) is 15 + 25 +\\n20 = 60. 7(GT : 20)\\n34', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 33}),\n",
       " Document(page_content='C Sample Study\\nTo validate the correctness of chain of thought, we analyze texts generated by Zero-shot-CoT for\\nCommonsenseQA and MultiArith datasets. Instruct-GPT3 (text-davinci-002) model is used for the\\nanalysis.\\nC.1 CommonsenseQA\\nTable 21: Categorization results of generated chain of thought by\\nZero-shot-CoT for CommonsenseQA datasets.\\nPrediction CoT Category Zero-Shot-CoT (%)\\nCorrectCoT is correct 78.0\\nCoT is incorrect 22.0\\nIncorrectCommonSense Mistake 62.0\\nFactual Mistake 0.0\\nLogical Mistake 28.0\\nOthers 10.0\\nTable 21 summarizes the categorization results of texts generated by Zero-shot-CoT for Common-\\nsenseQA. We randomly picked up 50 samples whose prediction results were correct and 50 samples\\nwhose prediction results were incorrect. We categorized those samples by CoT types. Some picked-up\\nsamples from each category are found Table 22.\\nFirst, it is found that the correct samples contain a certain amount of incorrect chain of thought. The\\nmain tendency is that Zero-shot-CoT cannot narrow down the prediction to one from multiple answer\\nchoices, and produce multiple predictions as answers but fortunately the ﬁrst output answer was\\ncorrect. See “Correct - CoT is INCORRECT ” rows in Table 22\\nSecond, as for incorrect samples, commonsense mistake is the most frequent error type. By observing\\nthe produced chain of thought texts, it is found that Zero-shot-CoT often produces a ﬂexible and\\nreasonable chain of thought (logically correct but lacks common sense) even when the ﬁnal prediction\\nis not correct. See “CommonSense Mistake” rows in Table 22\\nTable 22: Prediction examples produced by Zero-shot-CoT for CommonsenseQA.\\nCategory Question Answer\\nCorrect - CoT\\nis correctQ: If there is a pond with\\ntrees around it, where it\\nit likely located? Answer\\nChoices: (A) ground (B)\\nbathroom (C) forest (D)\\ncountryside (E) rural areaA: Let’s think step by step. A pond is likely to\\nbe located near trees because trees need water to\\nsurvive. Therefore, the most likely location for a\\npond with trees around it is in a forest. Therefore,\\namong A through E, the answer is C. X\\nCorrect - CoT\\nis correctQ: Human beings learn\\nabout current events from\\nwhat print item? Answer\\nChoices: (A) question au-\\nthority (B) melt (C) read\\nnewspapers (D) act (E)\\ndictionaryA: Let’s think step by step. How do human beings\\nlearn? They learn by observing and listening to\\nothers, and they also learn by reading. So it’s most\\nlikely that human beings learn about current events\\nfrom reading newspapers. Therefore, among A\\nthrough E, the answer is C. X\\nContinued on next page\\n35', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 34}),\n",
       " Document(page_content='Table 22 – Continued from previous page\\nCategory Question Answer\\nCorrect - CoT\\nis correctQ: The artist was sit-\\nting quietly pondering,\\nthen suddenly he began\\nto paint when what struck\\nhim? Answer Choices:\\n(A) sadness (B) anxiety\\n(C) inspiration (D) dis-\\ncomfort (E) insightsA: Let’s think step by step. The ﬁrst thing that\\nhappens is that the artist is sitting quietly and pon-\\ndering. This means he’s thinking deeply about\\nsomething, probably trying to come up with an\\nidea. Then, suddenly, he begins to paint. This\\nmeans he was inspired by something he thought\\nof. The most likely explanation is that he had an\\ninsight, or a sudden realization, that led him to\\nstart painting. Therefore, among A through E, the\\nanswer is most likely (C), inspiration. X\\nCorrect - CoT\\nis\\nINCORRECTQ: What is likely to sat-\\nisfy someone’s curiosity?\\nAnswer Choices: (A)\\nhear news (B) read book\\n(C) see favorite show\\n(D) comedy show (E) go\\nsomewhereA: Let’s think step by step. In order to satisfy\\nsomeone’s curiosity, they would need to want to\\nknow more about something. So, (A) hearing news\\nand (B) reading a book are both good answers\\nbecause they provide information that the person\\nmay be curious about. (C) Seeing a favorite show\\ncould also work because it might be something\\nthe person is curious about and wants to know\\nmore about. (D) A comedy show could potentially\\nsatisfy someone’s curiosity if the person is curious\\nabout the comedian’s life or what they are like in\\nperson. (E) Going somewhere could also work\\nif the person is curious about a certain place and\\nwants to know more Therefore, among A through\\nE, the answer is A, B, C, D, or E. X(Model cannot\\nchoose only one.) (GT : (A), Zero-shot: (E))\\nCorrect - CoT\\nis\\nINCORRECTQ: What would be neces-\\nsary for getting in shape?\\nAnswer Choices: (A)\\ngood health (B) exer-\\ncise (C) muscle tone (D)\\nsweat (E) feel betterA: Let’s think step by step. In order to get in shape,\\nyou need to be able to do three things: 1. Exercise\\nregularly 2. Eat a healthy diet 3. Get enough sleep\\nIf you can do all three of those things, then you will\\nbe well on your way to getting in shape! Therefore,\\namong A through E, the answer is B, C, and D.\\nX(Model cannot choose only one.) (GT : (B),\\nZero-shot: (B))\\nCorrect - CoT\\nis\\nINCORRECTQ: The man had a fear of\\nillness, so he never vis-\\nited friends who were a\\nwhat? Answer Choices:\\n(A) sick person (B) hospi-\\ntal (C) elderly person (D)\\ngraveyard (E) doctor’s of-\\nﬁceA: Let’s think step by step. The man has a fear\\nof illness, so he would avoid anything that would\\nput him in close proximity to an ill person. This\\nwould include a hospital, where sick people are\\ntreated, and a doctor’s ofﬁce, where sick people go\\nto receive care. An elderly person is more likely\\nto be ill than a young person, so the man would\\navoid them as well. A graveyard is where people\\nwho have died from illness are buried, so the man\\nwould avoid that as well. The answer is A, sick\\nperson. Therefore, among A through E, the answer\\nis A.X(Model cannot narrow down choice to\\none in CoT.) (GT : (A), Zero-shot: (A))\\nContinued on next page\\n36', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 35}),\n",
       " Document(page_content='Table 22 – Continued from previous page\\nCategory Question Answer\\nCommonSense\\nMistakeQ: Where is a well\\nused toy car likely to be\\nfound? Answer Choices:\\n(A) child’s room (B)\\nboy’s bedroom (C) own\\nhome (D) toy store (E)\\nhouseA: Let’s think step by step. A toy car is likely to be\\nfound in a child’s room. A child’s room is likely to\\nbe found in a house. Therefore, a toy car is likely to\\nbe found in a house. Therefore, among A through\\nE, the answer is E. 7(GT : (A), Zero-shot: (A))\\nCommonSense\\nMistakeQ: Where can you go to\\nuse a piano in your neigh-\\nborhood if you don’t\\nhave one? Answer\\nChoices: (A) music\\nschool (B) music store\\n(C) neighbor’s house (D)\\nlunch (E) drawing roomA: Let’s think step by step. If you don’t have a\\npiano, you can’t use your own. You could go to a\\nmusic school, but that seems like it would be for a\\nlesson rather than just to use a piano. A music store\\nmight have a piano that you could use, but you\\nwould probably have to buy something. You could\\ngo to a neighbor’s house, but that seems like a lot\\nto ask. You could go to a lunch, but that doesn’t\\nmake sense. You could go to a drawing room, but\\nthat also doesn’t make sense. The answer must be\\n(B) music store. Therefore, among A through E,\\nthe answer is B. 7(GT : (C), Zero-shot: (C))\\nCommonSense\\nMistakeQ: David watched some\\nnesting birds using his\\nbinoculars while on vaca-\\ntion. Where might David\\nbe?. Answer Choices:\\n(A) sky (B) vaccation (C)\\nforest (D) countryside (E)\\nroofA: Let’s think step by step. First, we need to under-\\nstand what the word \"binoculars\" means. Binocu-\\nlars are two telescopes that are joined together and\\nallow a person to see things that are far away. Now,\\nwe need to think about where a person might use\\nbinoculars. A person might use binoculars to see\\nthings that are far away and in the sky, or to see\\nthings that are far away and on the ground. So, the\\nanswer could be either (A) sky or (D) countryside.\\nTherefore, among A through E, the answer is (A)\\nor (D). 7(GT : (C), Zero-shot: (D))\\nCommonSense\\nMistakeQ: When you get up\\nin the morning before\\nyou begin work you\\nshould do what? Answer\\nChoices: (A) apply for\\njob (B) sleep (C) concen-\\ntrate (D) shower (E) just\\ndoA: Let’s think step by step. The ﬁrst thing you\\nshould do when you wake up is probably to stretch\\nand yawn to get your body moving. Then, you\\nshould probably brush your teeth and wash your\\nface to get ready for the day. After that, you might\\nwant to eat breakfast to give yourself some energy\\nfor the day. Once you’re all set, you can start your\\nwork for the day. Therefore, among A through E,\\nthe answer is C. 7(GT : (D), Zero-shot: (C))\\nCommonSense\\nMistakeQ: What is someone do-\\ning if he or she is sit-\\nting quietly and his or her\\neyes are moving? An-\\nswer Choices: (A) read-\\ning (B) meditate (C) fall\\nasleep (D) bunk (E) thinkA: Let’s think step by step. If someone is sitting\\nquietly, that means they are not moving around.\\nAnd if their eyes are moving, they are probably not\\nasleep. So it’s probably not (C) or (D). If they are\\nreading, they would be looking at the book, not\\nmoving their eyes around, so it’s probably not (A),\\nwhich leaves us with (B) and (E). It’s more likely\\nthat they are thinking, so (E) is the best answer.\\nTherefore, among A through E, the answer is E.\\n7(GT : (A), Zero-shot: (E))\\nContinued on next page\\n37', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 36}),\n",
       " Document(page_content='Table 22 – Continued from previous page\\nCategory Question Answer\\nLogical\\nMistakeQ: If you really wanted a\\ngrape, where would you\\ngo to get it? Answer\\nChoices: (A) winery (B)\\nfruit stand (C) ﬁeld (D)\\nkitchen (E) foodA: Let’s think step by step. If you want a grape, the\\nﬁrst place you might think to look is a fruit stand. If\\nthere are no grapes at the fruit stand, your next best\\nbet would be a grocery store. If there are no grapes\\nat the grocery store, you might try a vineyard or\\nwinery. If there are no grapes at the vineyard or\\nwinery, your last resort would be to grow your own\\ngrapes. Therefore, among A through E, the answer\\nis E.7(GT : (B), Zero-shot: (B))\\nC.2 MultiArith\\nTable 23: Categorization results of produced chain of thought for MultiArith datasets. (*1)\\nThese categories are cited from Wei et al. [2022].\\nPrediction CoT Category Zero-Shot-CoT (%) Few-Shot-CoT (%)\\nCorrectCoT is correct 94.0 98.0\\nCoT is incorrect 6.0 2.0\\nIncorrectCommonSense Mistake 10.0 23.8\\nFactual Mistake 2.0 0.0\\nLogical Mistake 68.0 73.8\\n- Calculator error (*1) (8.) ( 26.2)\\n- Symbol mapping error (*1) (4.) (2.4)\\n- One step missing error (*1) (6.) (7.1)\\n- One unnecessary step error ( 10.) (2.4)\\n- More complicated (40.) (35.7)\\nOthers 20.0 2.4\\nTable 23 summarizes the categorization results of texts generated by Zero-shot-CoT and Few-shot-\\nCoT for MultiArith. We compared Zero-shot-CoT and Few-shot-CoT to contrast the difference of\\nchain of thought produced by these two methods. Speciﬁcally, we randomly picked up correct 50\\nsamples and incorrect 50 samples produced by each method and categorized them by types. As an\\nexception, the maximum number of incorrect samples from Few-shot-CoT for MultiArith was 42.\\nAs for correct samples, we examined if the produced chain of thought is logical and consistent with\\nthe correct prediction. The result shows that almost all the chain of thought is correct, with slightly\\nmore reasoning mistakes found in Zero-shot-CoT than Few-shot-CoT.\\nAs for incorrect samples, it is found that Zero-shot-CoT tends to output unnecessary steps of reasoning\\nafter getting the correct prediction, which results in changing the prediction to incorrect one. Zero-\\nshot-CoT also sometimes does not start reasoning, just rephrasing the input question. In contrast,\\nFew-shot-CoT tends to fail when generated chain of thought include ternary operation, e.g. (3 +2)\\x034.\\nAnother ﬁnding is that Zero-shot-CoT and Few-shot-CoT have a certain amount of common sense\\nmistakes to interpret a question. Some examples are found at Table 24.\\n38', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 37}),\n",
       " Document(page_content='Table 24: Example-based comparison between Zero-shot-CoT and Few-shot-CoT from MultiArith.\\nQuestion Zero-shot-CoT Few-shot-CoT\\nQ: At the fair Adam\\nbought 13 tickets. Af-\\nter riding the ferris wheel\\nhe had 4 tickets left. If\\neach ticket cost 9 dol-\\nlars, how much money\\ndid Adam spend riding\\nthe ferris wheel?A(ONE UNNECESSARY STEP ER-\\nROR) : Let’s think step by step. Adam\\nbought 13 tickets. That means he spent\\n13 * 9 = 117 dollars on tickets. After\\nriding the ferris wheel, he had 4 tickets\\nleft. That means he used 9 tickets to ride\\nthe ferris wheel. That means he spent 9 *\\n9 = 81 dollars on riding the ferris wheel.\\nSo the answer is 117 - 81 = 36 dollars.\\nTherefore, the answer (arabic numerals)\\nis 36. 7A: Adam started with 13\\ntickets. He had 4 left af-\\nter riding the ferris wheel.\\nSo he must have spent 13\\n- 4 = 9 tickets. 9 tickets\\ntimes 9 dollars per ticket\\nis 9 * 9 = 81. The answer\\nis 81.X\\nQ: At a restaurant each\\nadult meal costs $5 and\\nkids eat free. If a group\\nof 15 people came in\\nand 8 were kids, how\\nmuch would it cost for\\nthe group to eat?A(ONE UNNECESSARY STEP ER-\\nROR) : Let’s think step by step. There\\nare 8 kids, so that means there are 7\\nadults. Each adult meal costs $5, so\\nthat means that the 7 adults will cost\\n$5*7=$35 Now we just need to add the\\ntwo numbers together. $35+8=43$ So\\nit would cost $43 for the group to eat.\\nTherefore, the answer (arabic numerals)\\nis 43. 7A: If 8 were kids, then\\nthat means there were 15\\n- 8 = 7 adults. Each adult\\nmeal costs $5. So the to-\\ntal cost would be 7 * 5 =\\n35. The answer is 35. X\\nQ: April’s discount ﬂow-\\ners was having a sale\\nwhere each ﬂower was 6\\ndollars. If Katie bought 5\\nroses and 5 daisies, how\\nmuch did she spend?A: Let’s think step by step. First, we\\nneed to calculate how much each type\\nof ﬂower costs. There are 5 roses, and\\neach rose costs 6 dollars. So, the cost of\\nthe roses is 5*6=30 dollars. There are\\n5 daisies, and each daisy costs 6 dollars.\\nSo, the cost of the daisies is 5*6=30 dol-\\nlars. Then, we need to calculate the total\\ncost. The total cost is the cost of the\\nroses plus the cost of the daisies. So, the\\ntotal cost is 30+30=60 dollars. There-\\nfore, the answer (arabic numerals) is 60.\\nXA (CALCULATOR\\nERROR) : Each ﬂower\\nwas 6 dollars. Katie\\nbought 5 roses and 5\\ndaisies. So she spent 5 *\\n6 + 5 * 6 = 60 + 30 = 90.\\nThe answer is 90. 7\\nQ: While shopping\\nfor music online, Faye\\nbought 2 country albums\\nand 3 pop albums. Each\\nalbum came with a lyric\\nsheet and had 6 songs.\\nHow many songs did\\nFaye buy total?A: Let’s think step by step. Faye bought\\n2 country albums. Each album has 6\\nsongs. So she bought 2 * 6 = 12 songs\\nfrom the country albums. Faye bought\\n3 pop albums. Each album has 6 songs.\\nSo she bought 3 * 6 = 18 songs from the\\npop albums. In total, Faye bought 12 +\\n18 = 30 songs. Therefore, the answer\\n(arabic numerals) is 30. XA (CALCULATOR\\nERROR) : Faye bought\\n2 country albums and 3\\npop albums. Each album\\nhad 6 songs. So she\\nbought 2 * 6 + 3 * 6 = 24\\nsongs. The answer is 24.\\n7\\nContinued on next page\\n39', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 38}),\n",
       " Document(page_content='Table 24 – continued from previous page\\nQuestion Zero-shot-CoT Few-shot-CoT\\nQ: Jerry was helping the\\ncafeteria workers pick up\\nlunch trays, but he could\\nonly carry 8 trays at a\\ntime. If he had to pick\\nup 9 trays from one table\\nand 7 trays from another,\\nhow many trips will he\\nmake?A(COMMONSENSE MISTAKE) :\\nLet’s think step by step. Jerry can carry\\n8 trays at a time. There are 9 trays on the\\nﬁrst table. That means Jerry will need to\\nmake 2 trips to the ﬁrst table. There are\\n7 trays on the second table. That means\\nJerry will need to make 1 trip to the sec-\\nond table. In total, Jerry will make 3\\ntrips. Therefore, the answer (arabic nu-\\nmerals) is 3. 7A: Jerry can carry 8 trays\\nat a time. If he has to pick\\nup 9 trays from one table\\nand 7 trays from another,\\nhe will have to make 2\\ntrips. The answer is 2. X\\nQ: Kaleb bought 14\\nboxes of chocolate candy\\nand gave 5 to his little\\nbrother. If each box has\\n6 pieces inside it, how\\nmany pieces did Kaleb\\nstill have?A: Let’s think step by step. Kaleb\\nbought 14 boxes of chocolate candy.\\nThis means that he had 84 pieces of\\nchocolate candy. He gave 5 boxes to\\nhis little brother, which means that he\\ngave his little brother 30 pieces of choco-\\nlate candy. Kaleb still had 54 pieces of\\nchocolate candy. Therefore, the answer\\n(arabic numerals) is 54. XA(COMMONSENSE\\nMISTAKE) : Kaleb\\nbought 14 boxes. Each\\nbox has 6 pieces. So 14 *\\n6 = 84 pieces. He gave 5\\nto his brother. So he has\\n84 - 5 = 79 pieces. The\\nanswer is 79. 7\\nQ: At the fair there were\\n12 people in line for the\\nbumper cars. If 10 of\\nthem got tired of waiting\\nand left and 15 more got\\nin line, how many people\\nwould be in line?A(OTHERS; REASONING IS NOT\\nSTARTED) : Let’s think step by step.\\nThere were 12 people in line for the\\nbumper cars. 10 of them got tired of\\nwaiting and left. 15 more got in line.\\nThat means that there are now 15 people\\nin line for the bumper cars. Therefore,\\nthe answer (arabic numerals) is 15. 7A: There were originally\\n12 people in line. 10 of\\nthem left, so that left 12 -\\n10 = 2. Then 15 more got\\nin line, so that is 2 + 15 =\\n17. The answer is 17. X\\nQ: Luke made 9 dollars\\nmowing lawns and 18\\ndollars weed eating. If\\nhe only spent 3 dollar a\\nweek, how long would\\nthe money last him?A(OTHERS; QUESTION UNDER-\\nSTANDING ERROR) : Let’s think step\\nby step. How much money does Luke\\nmake in a week? How much money\\ndoes Luke spend in a week? How much\\nmoney does Luke have left over at the\\nend of the week? How long will it take\\nLuke to save up $54? Luke makes $27\\na week. He spends $3 a week. He has\\n$24 left over at the end of the week. It\\nwill take Luke 2 weeks to save up $54.\\nTherefore, the answer (arabic numerals)\\nis 2.7A: Luke made 9 dollars\\nmowing lawns and 18\\ndollars weed eating. In\\ntotal, he made 9 + 18 =\\n27 dollars. If he spends\\n3 dollars a week, that is\\n3 dollars * 4 weeks = 12\\ndollars a month. So the\\nmoney would last him 27\\n/ 12 = 2.25 months. The\\nanswer is 2.25 months.\\n7(GT : 9 weeks)\\nContinued on next page\\n40', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 39}),\n",
       " Document(page_content='Table 24 – continued from previous page\\nQuestion Zero-shot-CoT Few-shot-CoT\\nQ: Each chocolate bar\\nin a box cost $3. If a\\nbox had 9 bars total and\\nWendy sold all but 3 bars,\\nhow much money would\\nshe have made?A(OTHERS; MAXIMUM LIMIT\\nOF TEXT LENGTH) : Let’s think step\\nby step. Each chocolate bar in a box\\ncosts $3. If a box had 9 bars total, then\\nWendy sold all but 3 bars. How much\\nmoney would she have made? We can\\nsolve this problem using algebra. Let’s\\nstart by creating a variable to represent\\nthe number of chocolate bars Wendy\\nsold. We’ll call this variable \"x\". If\\nWendy sold \"x\" chocolate bars, then she\\nwould have 9 - x chocolate bars left in the\\nbox. We know that each chocolate bar\\ncosts $3, so the total cost of the choco-\\nlate bars Wendy sold\\nTherefore, the answer (arabic numerals)\\nis: Wendy would have made $12. 7A: Each chocolate bar\\ncost 3 dollars. So if\\nWendy sold all but 3, she\\nwould have sold 9 - 3 = 6.\\n6 * 3 = 18. The answer is\\n18.X\\n41', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 40}),\n",
       " Document(page_content='D Further Zero-shot Experiments with PaLM 540B\\nWe additionally evaluated Zero-shot-CoT on PaLM 540B, without and with self-consistency [Wang\\net al., 2022]. Self-consistency [Wang et al., 2022] generates reasoning paths by random sampling\\nstrategy N times and decides the ﬁnal prediction by majority voting.\\nTable 25: Further experiment results with PaLM (540B). Evaluation metric is Accuracy.\\nAQUA-RAT SV AMP GSM8K MultiArith\\nZero-shot 23.4 63.1 12.5 25.5\\nZero-shot-CoT 36.1 63.1 43.0 66.1\\nZero-shot-CoT + self consistency\\n(40 paths)46.5 80.5 70.1 89.0\\nFew-shot-CoT [Wei et al., 2022] 35.8 79.0 56.9 -\\nFew-shot-CoT + self consistency\\n(40 paths) [Wang et al., 2022]48.3 86.6 74.4 -\\nE Detailed experiment results of model scale study\\nThis section describes the detailed experiment results of model scale study. The curve within Figure 3\\nuses the values of Table 26 and Table 27.\\nTable 26: Model scale study. Evaluation metric is accuracy on MultiArith dataset. S: text-ada-\\n001, M: text-babbage-001, L: text-curie-001, XL-1: text-davinci-001, XL-2: text-davinci-002. It\\nis veriﬁed that CoT is effective when the model is larger, such as Instruct GPT-3 (text-davinci-001\\nand text-davinci-002) and Original GPT-3 (175B parameters; davinci). In this experiment, the order\\nof performance (ascending) is Zero-shot, Few-shot (8samples), Zero-shot-CoT, and Few-shot-CoT\\n(8samples) for davinci and text-davinci-002.\\nOriginal GPT-3 (0.3B / 1.3B / 6.7B / 175B) Instruct GPT-3 (S / M / L / XL-1 / XL-2)\\nZero-shot 2.0 / 1.3 / 1.5 / 3.3 3.7 / 3.8 / 4.3 / 8.0 / 17.7\\nFew-shot 5.2 / 5.2 / 4.0 / 8.1 3.0 / 2.2 / 4.8 / 14.0 / 33.7\\nZero-shot-CoT 1.7 / 2.2 / 2.3 / 19.0 2.0 / 3.7 / 3.3 / 47.8 /78.7\\nFew-shot-CoT 4.3 / 1.8 / 6.3 / 44.3 2.5 / 2.5 / 3.8 / 36.8 /93.0\\nGPT-2 (1.5B) GPT-Neo (2.7B) GPT-J (6B) T0 (11B) OPT (13B)\\nZero-shot 3.2 3.0 2.7 2.8 3.7\\nZero-shot-CoT 2.2 1.3 2.5 3.2 2.2\\nTable 27: Model scale study with PaLM. Evalua-\\ntion metric is accuracy on GSM8K dataset.\\nPaLM (8B / 62B / 540B)\\nZero-shot 2.1 / 7.0 / 12.5\\nZero-shot-CoT 2.4 / 10.5 / 43.0\\n42', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 41})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\\n\\n\".join([page.page_content for page in pages[0:2]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Seguidamente juntamos apenas os 2 primeiros documentos (páginas):</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large Language Models are Zero-Shot Reasoners\n",
       "Takeshi Kojima\n",
       "The University of Tokyo\n",
       "t.kojima@weblab.t.u-tokyo.ac.jpShixiang Shane Gu\n",
       "Google Research, Brain Team\n",
       "Machel Reid\n",
       "Google Research\u0003Yutaka Matsuo\n",
       "The University of TokyoYusuke Iwasawa\n",
       "The University of Tokyo\n",
       "Abstract\n",
       "Pretrained large language models (LLMs) are widely used in many sub-ﬁelds of\n",
       "natural language processing (NLP) and generally known as excellent few-shot\n",
       "learners with task-speciﬁc exemplars. Notably, chain of thought (CoT) prompting,\n",
       "a recent technique for eliciting complex multi-step reasoning through step-by-\n",
       "step answer examples, achieved the state-of-the-art performances in arithmetics\n",
       "and symbolic reasoning, difﬁcult system-2 tasks that do not follow the standard\n",
       "scaling laws for LLMs. While these successes are often attributed to LLMs’\n",
       "ability for few-shot learning, we show that LLMs are decent zero-shot reasoners\n",
       "by simply adding “Let’s think step by step” before each answer. Experimental\n",
       "results demonstrate that our Zero-shot-CoT, using the same single prompt template,\n",
       "signiﬁcantly outperforms zero-shot LLM performances on diverse benchmark\n",
       "reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SV AMP),\n",
       "symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date\n",
       "Understanding, Tracking Shufﬂed Objects), without any hand-crafted few-shot\n",
       "examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and\n",
       "GSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-\n",
       "002), as well as similar magnitudes of improvements with another off-the-shelf\n",
       "large model, 540B parameter PaLM. The versatility of this single prompt across\n",
       "very diverse reasoning tasks hints at untapped and understudied fundamental\n",
       "zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive\n",
       "capabilities may be extracted by simple prompting. We hope our work not only\n",
       "serves as the minimal strongest zero-shot baseline for the challenging reasoning\n",
       "benchmarks, but also highlights the importance of carefully exploring and analyzing\n",
       "the enormous zero-shot knowledge hidden inside LLMs before crafting ﬁnetuning\n",
       "datasets or few-shot exemplars.\n",
       "1 Introduction\n",
       "Scaling up the size of language models has been key ingredients of recent revolutions in natural\n",
       "language processing (NLP) [Vaswani et al., 2017, Devlin et al., 2019, Raffel et al., 2020, Brown et al.,\n",
       "2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]. The success of large language\n",
       "models (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various\n",
       "tasks by simply conditioning the models on a few examples (few-shot) or instructions describing the\n",
       "task (zero-shot). The method of conditioning the language model is called “prompting” [Liu et al.,\n",
       "2021b], and designing prompts either manually [Schick and Schütze, 2021, Reynolds and McDonell,\n",
       "2021] or automatically [Gao et al., 2021, Shin et al., 2020] has become a hot topic in NLP.\n",
       "\u0003Work done while at The University of Tokyo.\n",
       "36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.11916v4  [cs.CL]  29 Jan 2023\n",
       "\n",
       "(c) Zero-shot \n",
       "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, \n",
       "and half of the golf balls are blue. How many blue golf balls are \n",
       "there? \n",
       "A: The answer (arabic numerals) is  \n",
       "(Output) 8 X(d) Zero-shot-CoT (Ours) \n",
       "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, \n",
       "and half of the golf balls are blue. How many blue golf balls are \n",
       "there? \n",
       "A: Let’s think step by step.  \n",
       "(Output) There are 16 balls in total. Half of the balls are golf  \n",
       "balls. That means that there are 8 golf balls. Half of the golf balls  \n",
       "are blue. That means that there are 4 blue golf balls. ✓Q: Roger has 5 tennis balls. He buys 2 more cans of tennis \n",
       "balls. Each can has 3 tennis balls. How many tennis balls does \n",
       "he have now? \n",
       "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 \n",
       "tennis balls. 5 + 6 = 11. The answer is 11. \n",
       "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, \n",
       "and half of the golf balls are blue. How many blue golf balls are \n",
       "there? \n",
       "A:\n",
       "(Output) The juggler can juggle 16 balls. Half of the balls are golf  \n",
       "balls. So there are 16 / 2 = 8 golf balls. Half of the golf balls are  \n",
       "blue. So there are 8 / 2 = 4 blue golf balls. The answer is 4. ✓(b) Few-shot-CoT (a) Few-shot \n",
       "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis \n",
       "balls. Each can has 3 tennis balls. How many tennis balls does \n",
       "he have now? \n",
       "A: The answer is 11. \n",
       "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, \n",
       "and half of the golf balls are blue. How many blue golf balls are \n",
       "there? \n",
       "A:\n",
       "(Output) The answer is 8. XFigure 1: Example inputs and outputs of GPT-3 with (a) standard Few-shot ([Brown et al., 2020]), (b)\n",
       "Few-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to\n",
       "Few-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\n",
       "where standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\n",
       "task, ours does not need any examples and just uses the same prompt “Let’s think step by step” across\n",
       "all tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\n",
       "In contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\n",
       "and West, 2000] tasks with task-speciﬁc few-shot or zero-shot prompting [Liu et al., 2021b], even\n",
       "language models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\n",
       "slow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\n",
       "Wang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\n",
       "step-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\n",
       "Such chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\n",
       "the complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\n",
       "satisﬁes the scaling laws better and jumps up with the size of the language models. For example,\n",
       "when combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\n",
       "prompting signiﬁcantly increases the performance over standard few-shot prompting across several\n",
       "benchmark reasoning tasks, e.g., GSM8K (17.9% !58.1%).\n",
       "While the successes of CoT prompting [Wei et al., 2022], along those of many other task-speciﬁc\n",
       "prompting work [Gao et al., 2021, Schick and Schütze, 2021, Liu et al., 2021b], are often attributed\n",
       "to LLMs’ ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot\n",
       "reasoners by adding a simple prompt, Let’s think step by step , to facilitate step-by-step thinking before\n",
       "answering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully\n",
       "generates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a\n",
       "problem where the standard zero-shot approach fails. Importantly, our Zero-shot-CoT is versatile and"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Prompting de zero-shot chain-of-thought é uma técnica de aprendizagem que visa ajudar os alunos a desenvolver habilidades de pensamento crítico e criativo. Esta técnica envolve o uso de um prompt para estimular o aluno a pensar em uma cadeia de pensamentos que possam levar a uma solução para um problema. O prompt pode ser uma pergunta, uma imagem, um vídeo ou qualquer outro estímulo que possa ajudar o aluno a pensar de forma criativa. O objetivo é que o aluno desenvolva habilidades de pensamento crítico e criativo, ao invés de apenas memorizar informações."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(f\"\"\" Aqui estão as duas primeiras páginas do paper:\n",
    "\n",
    "com base nesse conteúdo, o que é prompting de zero-shot chain-of-thought?\"\"\")\n",
    "\n",
    "\n",
    "Markdown(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontre conteúdo relevante usando a pesquisa de Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar:\n",
    "\n",
    "```\n",
    "!pip install -qqq faiss-cpu tiktoken\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só para teste usaremos algumas páginas de \"pages\", assim:\n",
    "\n",
    "pages = pages[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Large Language Models are Zero-Shot Reasoners\\nTakeshi Kojima\\nThe University of Tokyo\\nt.kojima@weblab.t.u-tokyo.ac.jpShixiang Shane Gu\\nGoogle Research, Brain Team\\nMachel Reid\\nGoogle Research\\x03Yutaka Matsuo\\nThe University of TokyoYusuke Iwasawa\\nThe University of Tokyo\\nAbstract\\nPretrained large language models (LLMs) are widely used in many sub-ﬁelds of\\nnatural language processing (NLP) and generally known as excellent few-shot\\nlearners with task-speciﬁc exemplars. Notably, chain of thought (CoT) prompting,\\na recent technique for eliciting complex multi-step reasoning through step-by-\\nstep answer examples, achieved the state-of-the-art performances in arithmetics\\nand symbolic reasoning, difﬁcult system-2 tasks that do not follow the standard\\nscaling laws for LLMs. While these successes are often attributed to LLMs’\\nability for few-shot learning, we show that LLMs are decent zero-shot reasoners\\nby simply adding “Let’s think step by step” before each answer. Experimental\\nresults demonstrate that our Zero-shot-CoT, using the same single prompt template,\\nsigniﬁcantly outperforms zero-shot LLM performances on diverse benchmark\\nreasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SV AMP),\\nsymbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date\\nUnderstanding, Tracking Shufﬂed Objects), without any hand-crafted few-shot\\nexamples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and\\nGSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-\\n002), as well as similar magnitudes of improvements with another off-the-shelf\\nlarge model, 540B parameter PaLM. The versatility of this single prompt across\\nvery diverse reasoning tasks hints at untapped and understudied fundamental\\nzero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive\\ncapabilities may be extracted by simple prompting. We hope our work not only\\nserves as the minimal strongest zero-shot baseline for the challenging reasoning\\nbenchmarks, but also highlights the importance of carefully exploring and analyzing\\nthe enormous zero-shot knowledge hidden inside LLMs before crafting ﬁnetuning\\ndatasets or few-shot exemplars.\\n1 Introduction\\nScaling up the size of language models has been key ingredients of recent revolutions in natural\\nlanguage processing (NLP) [Vaswani et al., 2017, Devlin et al., 2019, Raffel et al., 2020, Brown et al.,\\n2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]. The success of large language\\nmodels (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various\\ntasks by simply conditioning the models on a few examples (few-shot) or instructions describing the\\ntask (zero-shot). The method of conditioning the language model is called “prompting” [Liu et al.,\\n2021b], and designing prompts either manually [Schick and Schütze, 2021, Reynolds and McDonell,\\n2021] or automatically [Gao et al., 2021, Shin et al., 2020] has become a hot topic in NLP.\\n\\x03Work done while at The University of Tokyo.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2205.11916v4  [cs.CL]  29 Jan 2023', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 0}),\n",
       " Document(page_content='(c) Zero-shot \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA: The answer (arabic numerals) is  \\n(Output) 8 X(d) Zero-shot-CoT (Ours) \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA: Let’s think step by step.  \\n(Output) There are 16 balls in total. Half of the balls are golf  \\nballs. That means that there are 8 golf balls. Half of the golf balls  \\nare blue. That means that there are 4 blue golf balls. ✓Q: Roger has 5 tennis balls. He buys 2 more cans of tennis \\nballs. Each can has 3 tennis balls. How many tennis balls does \\nhe have now? \\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 \\ntennis balls. 5 + 6 = 11. The answer is 11. \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA:\\n(Output) The juggler can juggle 16 balls. Half of the balls are golf  \\nballs. So there are 16 / 2 = 8 golf balls. Half of the golf balls are  \\nblue. So there are 8 / 2 = 4 blue golf balls. The answer is 4. ✓(b) Few-shot-CoT (a) Few-shot \\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis \\nballs. Each can has 3 tennis balls. How many tennis balls does \\nhe have now? \\nA: The answer is 11. \\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, \\nand half of the golf balls are blue. How many blue golf balls are \\nthere? \\nA:\\n(Output) The answer is 8. XFigure 1: Example inputs and outputs of GPT-3 with (a) standard Few-shot ([Brown et al., 2020]), (b)\\nFew-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to\\nFew-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer\\nwhere standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per\\ntask, ours does not need any examples and just uses the same prompt “Let’s think step by step” across\\nall tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).\\nIn contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich\\nand West, 2000] tasks with task-speciﬁc few-shot or zero-shot prompting [Liu et al., 2021b], even\\nlanguage models at the scale of 100B or more parameters had struggled on system-2 tasks requiring\\nslow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022],\\nWang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the\\nstep-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a).\\nSuch chain of thought demonstrations facilitate models to generate a reasoning path that decomposes\\nthe complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then\\nsatisﬁes the scaling laws better and jumps up with the size of the language models. For example,\\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\\nprompting signiﬁcantly increases the performance over standard few-shot prompting across several\\nbenchmark reasoning tasks, e.g., GSM8K (17.9% !58.1%).\\nWhile the successes of CoT prompting [Wei et al., 2022], along those of many other task-speciﬁc\\nprompting work [Gao et al., 2021, Schick and Schütze, 2021, Liu et al., 2021b], are often attributed\\nto LLMs’ ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot\\nreasoners by adding a simple prompt, Let’s think step by step , to facilitate step-by-step thinking before\\nanswering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully\\ngenerates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a\\nproblem where the standard zero-shot approach fails. Importantly, our Zero-shot-CoT is versatile and', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 1}),\n",
       " Document(page_content='generates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a\\nproblem where the standard zero-shot approach fails. Importantly, our Zero-shot-CoT is versatile and\\ntask-agnostic , unlike most prior task-speciﬁc prompt engineering in the forms of examples (few-shot)\\nor templates (zero-shot) [Liu et al., 2021b]: it can facilitate step-by-step answers across various\\nreasoning tasks, including arithmetic (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021],\\nAQUA-RAT [Ling et al., 2017], and SV AMP [Patel et al., 2021]), symbolic reasoning (Last letter and\\nCoin ﬂip), commonsense reasoning (CommonSenseQA [Talmor et al., 2019] and Strategy QA [Geva\\net al., 2021]), and other logical reasoning tasks (Date understanding and Tracking Shufﬂed Objects\\nfrom BIG-bench [Srivastava et al., 2022]) without modifying the prompt per task.\\nWe empirically evaluate Zero-shot-CoT against other prompting baselines in Table 2. While our\\nZero-shot-CoT underperforms Few-shot-CoT with carefully-crafted and task-speciﬁc step-by-step ex-\\namples, Zero-shot-CoT achieves enormous score gains compared to the zero-shot baseline, e.g. from\\n17.7% to 78.7% on MultiArith and from 10.4% to 40.7% on GSM8K with large-scale InstructGPT\\n2', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 1}),\n",
       " Document(page_content='model (text-davinci-002). We also evaluate Zero-shot-CoT with another off-the-shelf large model,\\n540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K.\\nImportantly, with our single ﬁxed prompt, zero-shot LLMs have a signiﬁcantly better scaling curve\\ncomparable to that of the few-shot CoT baseline. We also show that besides Few-shot-CoT requiring\\nhuman engineering of multi-step reasoning prompts, their performance deteriorates if prompt example\\nquestion types and task question type are unmatched, suggesting high sensitivity to per-task prompt\\ndesigns. In contrast, the versatility of this single prompt across diverse reasoning tasks hints at\\nuntapped and understudied zero-shot fundamental capabilities of LLMs, such as higher-level broad\\ncognitive capabilities like generic logical reasoning [Chollet, 2019]. While the vibrant ﬁeld of LLMs\\nstarted out from the premise of excellent few-shot learners [Brown et al., 2020], we hope our work\\nencourages more research into uncovering high-level andmulti-task zero-shot capabilities hidden\\ninside those models.\\n2 Background\\nWe brieﬂy review the two core preliminary concepts that form the basis of this work: the advent of\\nlarge language models (LLMs) and prompting, and chain of thought (CoT) prompting for multi-step\\nreasoning.\\nLarge language models and prompting A language model (LM), is a model that looks to estimate\\nthe probability distribution over text. Recently, scaling improvements through larger model sizes\\n(from a few million [Merity et al., 2016] to hundreds of millions [Devlin et al., 2019] to hundreds of\\nbillions [Brown et al., 2020] parameters) and larger data (e.g. webtext corpora [Gao et al., 2020])\\nhave enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream\\nNLP tasks. Besides the classic “pre-train and ﬁne-tune” paradigm [Liu et al., 2021b], models scaled\\nto 100B+ parameters exhibit properties conducive to few-shot learning [Brown et al., 2020], by way\\nof in context learning, where one can use a text or template known as a prompt to strongly guide the\\ngeneration to output answers for desired tasks, thus beginning an era of “pre-train and prompt” [Liu\\net al., 2021a]. In work, we call such prompts with explicit conditioning on few task examples as\\nfew-shot prompts, and other template-only prompts as zero-shot prompts.\\nChain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have par-\\nticularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\\nby modifying the answers in few-shot examples to step-by-step answers, and achieved signiﬁcant\\nboosts in performance across these difﬁcult benchmarks, especially when combined with very large\\nlanguage models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\\ngiven for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\\nasFew-shot-CoT in this work.\\n3 Zero-shot Chain of Thought\\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let’s\\nthink step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\\n3.1 Two-stage prompting', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       " Document(page_content='think step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\\n3.1 Two-stage prompting\\nWhile Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and\\nanswer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1)\\nalready uses prompting in the form of “The answer is”, to extract the answers in correct formats.\\nFew-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly\\ndesigning the few-shot example answers to end in such formats (see the top-right and top-left\\n3', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=0)\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"O que é prompting zero-shot chain-of-thought?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\n",
       "few-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\n",
       "given for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\n",
       "in the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\n",
       "asFew-shot-CoT in this work.\n",
       "3 Zero-shot Chain of Thought\n",
       "We propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\n",
       "It differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\n",
       "step-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\n",
       "2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\n",
       "with a single template. The core idea of our method is simple, as described in Figure 1: add Let’s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(docs[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando e carregando o índice FAISS\n",
    "\n",
    "\n",
    "<font color=\"orange\">Você também pode salvar e carregar um `índice FAISS`. Isso é útil para que você não precise recriá-lo toda vez que usá-lo.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desta maneira salvamos nosso índice:\n",
    "db.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos carregar o nosso índice e re-utilizá-lo:\n",
    "\n",
    "new_db = FAISS.load_local(\"faiss_index\", OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='model (text-davinci-002). We also evaluate Zero-shot-CoT with another off-the-shelf large model,\\n540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K.\\nImportantly, with our single ﬁxed prompt, zero-shot LLMs have a signiﬁcantly better scaling curve\\ncomparable to that of the few-shot CoT baseline. We also show that besides Few-shot-CoT requiring\\nhuman engineering of multi-step reasoning prompts, their performance deteriorates if prompt example\\nquestion types and task question type are unmatched, suggesting high sensitivity to per-task prompt\\ndesigns. In contrast, the versatility of this single prompt across diverse reasoning tasks hints at\\nuntapped and understudied zero-shot fundamental capabilities of LLMs, such as higher-level broad\\ncognitive capabilities like generic logical reasoning [Chollet, 2019]. While the vibrant ﬁeld of LLMs\\nstarted out from the premise of excellent few-shot learners [Brown et al., 2020], we hope our work', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       " Document(page_content='satisﬁes the scaling laws better and jumps up with the size of the language models. For example,\\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\\nprompting signiﬁcantly increases the performance over standard few-shot prompting across several\\nbenchmark reasoning tasks, e.g., GSM8K (17.9% !58.1%).\\nWhile the successes of CoT prompting [Wei et al., 2022], along those of many other task-speciﬁc\\nprompting work [Gao et al., 2021, Schick and Schütze, 2021, Liu et al., 2021b], are often attributed\\nto LLMs’ ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot\\nreasoners by adding a simple prompt, Let’s think step by step , to facilitate step-by-step thinking before\\nanswering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully\\ngenerates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 1}),\n",
       " Document(page_content='encourages more research into uncovering high-level andmulti-task zero-shot capabilities hidden\\ninside those models.\\n2 Background\\nWe brieﬂy review the two core preliminary concepts that form the basis of this work: the advent of\\nlarge language models (LLMs) and prompting, and chain of thought (CoT) prompting for multi-step\\nreasoning.\\nLarge language models and prompting A language model (LM), is a model that looks to estimate\\nthe probability distribution over text. Recently, scaling improvements through larger model sizes\\n(from a few million [Merity et al., 2016] to hundreds of millions [Devlin et al., 2019] to hundreds of\\nbillions [Brown et al., 2020] parameters) and larger data (e.g. webtext corpora [Gao et al., 2020])\\nhave enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream\\nNLP tasks. Besides the classic “pre-train and ﬁne-tune” paradigm [Liu et al., 2021b], models scaled', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       " Document(page_content='results demonstrate that our Zero-shot-CoT, using the same single prompt template,\\nsigniﬁcantly outperforms zero-shot LLM performances on diverse benchmark\\nreasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SV AMP),\\nsymbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date\\nUnderstanding, Tracking Shufﬂed Objects), without any hand-crafted few-shot\\nexamples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and\\nGSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-\\n002), as well as similar magnitudes of improvements with another off-the-shelf\\nlarge model, 540B parameter PaLM. The versatility of this single prompt across\\nvery diverse reasoning tasks hints at untapped and understudied fundamental\\nzero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive\\ncapabilities may be extracted by simple prompting. We hope our work not only', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_eddy = \"O que é PaLM?\"\n",
    "doc_eddy = new_db.similarity_search(query_eddy)\n",
    "doc_eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='model (text-davinci-002). We also evaluate Zero-shot-CoT with another off-the-shelf large model,\\n540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K.\\nImportantly, with our single ﬁxed prompt, zero-shot LLMs have a signiﬁcantly better scaling curve\\ncomparable to that of the few-shot CoT baseline. We also show that besides Few-shot-CoT requiring\\nhuman engineering of multi-step reasoning prompts, their performance deteriorates if prompt example\\nquestion types and task question type are unmatched, suggesting high sensitivity to per-task prompt\\ndesigns. In contrast, the versatility of this single prompt across diverse reasoning tasks hints at\\nuntapped and understudied zero-shot fundamental capabilities of LLMs, such as higher-level broad\\ncognitive capabilities like generic logical reasoning [Chollet, 2019]. While the vibrant ﬁeld of LLMs\\nstarted out from the premise of excellent few-shot learners [Brown et al., 2020], we hope our work', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_eddy[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Tmabém, acho muito interessante fazer:</font>\n",
    "\n",
    "## Pesquisa de Similaridade com score\n",
    "\n",
    "Existem alguns métodos específicos do `FAISS`. Um deles é a `similarity_search_with_score`, que permite retornar não apenas os documentos, mas também o `score de similaridade` da query a eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"O que é prompting zero-shot chain-of-thought?\"\n",
    "\n",
    "docs_and_scores = db.similarity_search_with_score(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs_and_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\\ngiven for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\\nasFew-shot-CoT in this work.\\n3 Zero-shot Chain of Thought\\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let’s', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       "  0.30369905),\n",
       " (Document(page_content='to 100B+ parameters exhibit properties conducive to few-shot learning [Brown et al., 2020], by way\\nof in context learning, where one can use a text or template known as a prompt to strongly guide the\\ngeneration to output answers for desired tasks, thus beginning an era of “pre-train and prompt” [Liu\\net al., 2021a]. In work, we call such prompts with explicit conditioning on few task examples as\\nfew-shot prompts, and other template-only prompts as zero-shot prompts.\\nChain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have par-\\nticularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought\\n(CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution\\nby modifying the answers in few-shot examples to step-by-step answers, and achieved signiﬁcant\\nboosts in performance across these difﬁcult benchmarks, especially when combined with very large', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       "  0.3099984),\n",
       " (Document(page_content='satisﬁes the scaling laws better and jumps up with the size of the language models. For example,\\nwhen combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought\\nprompting signiﬁcantly increases the performance over standard few-shot prompting across several\\nbenchmark reasoning tasks, e.g., GSM8K (17.9% !58.1%).\\nWhile the successes of CoT prompting [Wei et al., 2022], along those of many other task-speciﬁc\\nprompting work [Gao et al., 2021, Schick and Schütze, 2021, Liu et al., 2021b], are often attributed\\nto LLMs’ ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot\\nreasoners by adding a simple prompt, Let’s think step by step , to facilitate step-by-step thinking before\\nanswering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully\\ngenerates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 1}),\n",
       "  0.34435833),\n",
       " (Document(page_content='think step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\\n3.1 Two-stage prompting\\nWhile Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and\\nanswer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1)\\nalready uses prompting in the form of “The answer is”, to extract the answers in correct formats.\\nFew-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly\\ndesigning the few-shot example answers to end in such formats (see the top-right and top-left\\n3', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       "  0.3675319)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\\ngiven for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\\nasFew-shot-CoT in this work.\\n3 Zero-shot Chain of Thought\\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let’s', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       " 0.30369905)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Na documentação fala com o índice 0:\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='think step by step , or a a similar text (see Table 4), to extract step-by-step reasoning.\\n3.1 Two-stage prompting\\nWhile Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and\\nanswer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1)\\nalready uses prompting in the form of “The answer is”, to extract the answers in correct formats.\\nFew-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly\\ndesigning the few-shot example answers to end in such formats (see the top-right and top-left\\n3', metadata={'source': './2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf', 'page': 2}),\n",
       " 0.3675319)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value = max(docs_and_scores, key=lambda x: x[1])\n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3675319"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score_value = max(docs_and_scores, key=lambda x: x[1])[1]\n",
    "max_score_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use uma cadeia existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "\n",
    "chain = load_qa_with_sources_chain(llm,\n",
    "                                   chain_type=\"stuff\")\n",
    "\n",
    "query = \"O que é prompting zero-shot chain-of-thought?\"\n",
    "\n",
    "sources = db.similarity_search(query)\n",
    "\n",
    "results = chain({\"input_documents\": sources, \"question\":query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' Zero-shot chain-of-thought prompting is a template-based prompting for chain of thought reasoning that does not require step-by-step few-shot examples and is task-agnostic. It uses prompting twice to extract both reasoning and answer, with the first prompting being \"Let\\'s think step by step\".\\nSOURCES: ./2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Zero-shot chain-of-thought prompting is a template-based prompting for chain of thought reasoning that does not require step-by-step few-shot examples and is task-agnostic. It uses prompting twice to extract both reasoning and answer, with the first prompting being \"Let's think step by step\".\n",
       "SOURCES: ./2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(results[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
