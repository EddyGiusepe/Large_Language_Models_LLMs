{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Uma vis√£o geral do LangChain</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain, √© uma ferramenta de c√≥digo aberto e adicionou recentemente `Plugins ChatGPT`. Ele fornece tantos recursos que considero √∫teis:\n",
    "\n",
    "\n",
    "ü§ó integra√ß√£o com v√°rios fornecedores de `LLM`, incluindo `OpenAI`, `Cohere`, `Huggingface` e muito mais.\n",
    "\n",
    "ü§ó crie um bot de `Question-Answering` ou `Text Summarization` com seu pr√≥prio documento\n",
    "\n",
    "ü§ó forne√ßa o `plug-in` OpenAI ChatGPT Retriever\n",
    "\n",
    "ü§ó lide com o hist√≥rico de bate-papo com mem√≥ria LangChain\n",
    "\n",
    "ü§ó encadeie v√°rios `LLMs` e use `LLMs` com um conjunto de ferramentas como `Google Search`, `Python REPL` e muito mais.\n",
    "\n",
    "\n",
    "\n",
    "O mais incr√≠vel √© que o `LangChain` √© de c√≥digo aberto e √© um esfor√ßo da comunidade. Nesta postagem, a [Cientista de Dados S√™nior: Sophia Yang](https://www.youtube.com/SophiaYangDS), abordar√° 6 funcionalidades do `LangChain`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">1.</font> LangChain integra-se com muitos fornecedores de `LLM`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a mesma interface, voc√™ obt√©m acesso a muitos modelos `LLM` de v√°rios provedores LLM, incluindo `OpenAI`, `Cohere`, `AI21`, `Huggingface Hub`, `Azure OpenAI`, `Manifest`, `Goose AI`, `Writer`, `Banana`, `Modal`, `StochasticAI`, `Cerebrium`, `Petals`, `Forefront AI`, `PromptLayer Modelos OpenAI`, `Anthropic`, `DeepInfra` e `auto-hospedados`.\n",
    "\n",
    "Aqui est√° um exemplo de acesso ao `modelo OpenAI ChatGPT` e ao `modelo GPT3`, ao `modelo command-xlarge do Cohere` e ao `modelo Flan T5` do Google hospedado no `Hugging Face Hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos as bibliotecas\n",
    "%pip install langchain openai cohere huggingface_hub ipywidgets chromadb google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain.llms import OpenAI, Cohere, HuggingFaceHub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto √© quando usas o arquivo .env:\n",
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY']  \n",
    "Eddy_API_KEY_Cohere = os.environ[\"COHERE_API_KEY\"]\n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "#Eddy_API_KEY_SerpApi = os.environ[\"SERPAPI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ent√£o podemos dar um `prompt` para cada um desses modelos (a seguir) e ver o que eles retornam. Uma coisa a observar √© que, para modelos de bate-papo, podemos especificar se a mensagem √© uma `mensagem humana`, uma `mensagem de IA` ou uma `mensagem do sistema`. √â por isso que, para o modelo `ChatGPT, especifiquei minha mensagem como uma mensagem humana`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Como √© ser feliz?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo ChatGPT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Como sou uma intelig√™ncia artificial, n√£o sou capaz de experimentar emo√ß√µes como um ser humano. No entanto, posso explicar que a felicidade √© um estado de esp√≠rito em que uma pessoa se sente satisfeita, grata e alegre com sua vida. Ela pode ser alcan√ßada de v√°rias maneiras, como cultivando rela√ß√µes saud√°veis, praticando atividades que trazem prazer, buscando e alcan√ßando seus objetivos e metas pessoais, estando em paz consigo mesma, entre outras. Cada indiv√≠duo experimenta a felicidade de maneira diferente, mas √© um estado que todos buscam e que pode trazer muita satisfa√ß√£o e bem-estar.' additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(chatgpt([HumanMessage(content=text)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo GPT3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = OpenAI(model_name='text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ser feliz √© sentir-se plenamente realizado e satisfeito com a sua vida, ter um senso de prop√≥sito e alegria, e aproveitar o momento presente. √â tamb√©m sentir-se conectado aos outros e √† natureza e apreciar as pequenas coisas da vida. Significa ter esperan√ßa e otimismo para o futuro.\n"
     ]
    }
   ],
   "source": [
    "print(gpt3(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo Cohere</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere = Cohere(model='command-xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Essa pergunta √© muito interessante. Eu acho que ser feliz √© respeitar as suas belezas e suas vertentes, lidar com as suas falhas, pasmando pelas suas situa√ß√µes, e sentindo que, nos seus momentos mais dif√≠ceis, algo \"outside\" o teu controle estar√° a ajud√°-lo a ultrapassar esses momentos. Seja feliz significa sentir o seu lado humano, sentindo aos poucos que temos a satisfa√ß√£o de \"conquist√°-lo\", e ao mesmo tempo ter a certeza de que, no entanto, algum outro homem poder√° \"conquist√°-lo\", apesar de tudo o que n√≥s temos e o que podemos fazer. Seja feliz √© acreditar.\n",
      "Being happy is ...\n",
      "this is a very interesting question. I think being happy is showing respect for your beauty and your facets, dealing with your flaws, going through your situations, and feeling that, in your most difficult moments, something \"outside\" your control will help you overcome these moments. Being happy means feeling human, feeling from time to time that we have the satisfaction of \"conquering\" it, and at the same time feeling that, however, another man could\n"
     ]
    }
   ],
   "source": [
    "print(cohere(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo Flan</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu no tenho ningu√©m que \n"
     ]
    }
   ],
   "source": [
    "print(flan(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">2.</font> Resposta a perguntas com documentos externos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voc√™ est√° interessado em criar um `bot` de resposta a perguntas com documentos externos? Seus documentos s√£o salvos em `PDF`, `arquivo txt` ou `mesmo Notion`? E se eu quiser fazer perguntas e respostas com v√≠deos do YouTube?\n",
    "\n",
    "Sem problemas, a `LangChain` cuida de voc√™. LangChain fornece muitos carregadores de documentos: `File Loader`, `Directory Loader`, `Notion`, `ReadTheDocs`, `HTML`, `PDF`, `PowerPoint`, `Email`, `GoogleDrive`, `Obsidian`, `Roam`, `EverNote`, `YouTube`, `Hacker News`, `GitBook`, `Arquivo S3`, `Diret√≥rio S3`, `Arquivo GCS`, `GCS Directory`, `Web Base`, `IMSDb`, `AZLyrics`, `College Confidential`, `Gutenberg`, `Airbyte Json`, `CoNLL-U`, `iFixit`, `Notebook`, `Copypaste`, `CSV`, `Facebook Chat`, `Image`, `Markdown`, `SRT`, `Telegram`, `URL`, `Word Document`, `Blackboard`. <font color=\"orange\">Voc√™ tem outra fonte que n√£o est√° listada aqui? Voc√™ pode contribuir e adicionar ao LangChain, pois √© de c√≥digo aberto!</font>\n",
    "\n",
    "\n",
    "Aqui est√° um exemplo em que usamos o `TextLoader` para carregar um `arquivo txt local`, criar o `√≠ndice VectoreStore` e fazer uma query do seu √≠ndice. Intuitivamente, seus documentos ser√£o divididos e incorporados em vetores e armazenados em um `banco de dados de vetores`. A `query` localiza qual vetor est√° mais pr√≥ximo do vetor da pergunta no banco de dados de vetores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.document_loaders.text.TextLoader at 0x7f29978f2ad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregamos nosso documento:\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./estado_da_Uniao.txt')\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O presidente disse que ela √© uma das maiores mentes jur√≠dicas de nosso pa√≠s e que ela continuar√° o legado de excel√™ncia do juiz Breyer. Ele tamb√©m disse que ela √© um ex-litigante superior em pr√°tica privada, ex-defensor p√∫blico federal e de uma fam√≠lia de educadores de escola p√∫blica e policiais. Ele disse que ela √© um construtor de consenso e que recebeu amplo apoio desde que foi nomeada.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"O que o presidente disse sobre Ketanji Brown Jackson\"\n",
    "index.query(query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A not√≠cia empolgante √© que o `LangChain` integrou recentemente o `plug-in de recupera√ß√£o ChatGPT` para que as pessoas possam usar esse recuperador em vez de um √≠ndice. Qual √© a diferen√ßa entre um `√≠ndice` e um `recuperador`(retriever)? De acordo com `LangChain`, ‚ÄúUm √≠ndice √© uma estrutura de dados que oferece suporte √† pesquisa eficiente, `e um recuperador √© o componente que usa o √≠ndice para localizar e retornar documentos relevantes em resposta √† consulta de um usu√°rio`. O √≠ndice √© um componente chave do qual o retriever depende para desempenhar sua fun√ß√£o.‚Äù</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">3.</font> Manter/resumir o hist√≥rico de bate-papo (Chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Quando voc√™ conversa com o ChatGPT, ele n√£o mant√©m seu hist√≥rico de bate-papo.</font> Voc√™ precisar√° copiar e colar seu chat no novo prompt para deix√°-lo ciente do hist√≥rico. O `LangChain` resolve esse problema fornecendo v√°rias op√ß√µes diferentes para lidar com o hist√≥rico de bate-papo - `manter todas as conversas`, `manter as k conversas mais recentes`, `resumir a conversa` e uma combina√ß√£o dos itens acima.\n",
    "\n",
    "Aqui est√° um exemplo onde usamos `ConversationBufferWindowMemory` para manter a √∫ltima rodada de conversa, usamos `ConversationSummaryMemory` para resumir conversas anteriores e usamos `CombinedMemory` para combinar dois m√©todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, CombinedMemory, ConversationSummaryMemory\n",
    "\n",
    "\n",
    "conv_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history_lines\",\n",
    "    input_key=\"input\",\n",
    "    k=1\n",
    "    )\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=OpenAI(),\n",
    "    input_key=\"input\"\n",
    "    )\n",
    "\n",
    "# Combinado\n",
    "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"O que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
    "\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\", \"chat_history_lines\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=memory,\n",
    "    prompt=PROMPT\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Aqui est√° o resultado onde voc√™ pode ver o `resumo da conversa` e a `√∫ltima conversa passada` para o prompt.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mO que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
      "\n",
      "Summary of conversation:\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Ol√°!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ol√°! Meu nome √© AI. Como posso ajudar?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Ol√°!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mO que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
      "\n",
      "Summary of conversation:\n",
      "\n",
      "The human greets the AI, to which the AI responds with a greeting and introduces itself as AI, offering to help.\n",
      "Current conversation:\n",
      "Human: Ol√°!\n",
      "AI:  Ol√°! Meu nome √© AI. Como posso ajudar?\n",
      "Human: Voc√™ pode me contar uma piada??\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Claro! Qual √© o animal mais pregui√ßoso? Um panda, porque ele passa 12 horas por dia comendo e 12 horas por dia dormindo!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Voc√™ pode me contar uma piada??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mO que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
      "\n",
      "Summary of conversation:\n",
      "\n",
      "\n",
      "The human greets the AI, to which the AI responds with a greeting and introduces itself as AI, offering to help. The human then asks the AI to tell them a joke, to which the AI responds with a joke about a lazy panda.\n",
      "Current conversation:\n",
      "Human: Voc√™ pode me contar uma piada??\n",
      "AI:  Claro! Qual √© o animal mais pregui√ßoso? Um panda, porque ele passa 12 horas por dia comendo e 12 horas por dia dormindo!\n",
      "Human: Voc√™ pode me contar uma piada parecida?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sim, com certeza! Qual √© o animal mais pregui√ßoso do mundo? Um hipop√≥tamo, porque ele passa 16 horas por dia na √°gua e 8 horas por dia dormindo!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Voc√™ pode me contar uma piada parecida?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">4.</font> Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√Äs vezes, voc√™ pode querer encadear (`chain`) diferentes `LLMs`. <font color=\"pink\">Por exemplo:</font>\n",
    "\n",
    "ü§ì Voc√™ pode querer que a sa√≠da do `primeiro LLM` sirva como entrada do `segundo LLM`.\n",
    "\n",
    "ü§ì Ou voc√™ pode querer `resumir` (summarize) as sa√≠das do `primeiro` e do `segundo` LLM e passar o resumo como entrada para o `terceiro LLM`.\n",
    "\n",
    "ü§ì Ou voc√™ pode querer `encadear seu modelo de linguagem` com outra cadeia de utilit√°rios para fazer `opera√ß√µes matem√°ticas` ou Python.\n",
    "\n",
    "\n",
    "<font color=\"orange\">Aqui est√° um exemplo b√°sico do m√©todo `SimpleSequentialChain`. <font color=\"pink\">A primeira rede (chain) pede ao `ChatGPT` para obter um bom nome de produto para uma empresa que fabrica meias coloridas. ChatGPT retorna `‚ÄúRainbow Sox Co.‚Äù`</font> <font color=\"green\">A segunda cadeia usa esse nome como entrada e pede ao ChatGPT para escrever uma frase de efeito para a empresa `‚ÄúRainbow Sox Co.‚Äù` ChatGPT escreveu: ‚ÄúColoque um pouco de cor no seu passo com Rainbow Sox Co!‚Äù</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "    prompt = PromptTemplate(template=\"Qual √© um bom nome para uma empresa que faz {product}?\",\n",
    "                            input_variables=[\"product\"],\n",
    "                           )\n",
    "                                                )\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "\n",
    "#print(chain.run(\"Meias coloridas\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(input_variables=[\"company_name\"],\n",
    "                               template=\"Escreva uma frase de efeito para a seguinte empresa: {company_name}\",\n",
    "                              )\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "#print(chain_two.run(\"Meias coloridas\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSocktastic.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "\"A Socktastic tem o poder de transformar os seus p√©s em obras de arte!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\"A Socktastic tem o poder de transformar os seus p√©s em obras de arte!\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Execute a cadeia (chain) especificando apenas a vari√°vel de entrada para a primeira cadeia.\n",
    "catchphrase = overall_chain.run(\"Meias coloridas\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
