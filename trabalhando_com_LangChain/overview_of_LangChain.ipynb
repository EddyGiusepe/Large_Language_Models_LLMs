{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Uma vis√£o geral do LangChain</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain, √© uma ferramenta de c√≥digo aberto e adicionou recentemente `Plugins ChatGPT`. Ele fornece tantos recursos que considero √∫teis:\n",
    "\n",
    "\n",
    "ü§ó integra√ß√£o com v√°rios fornecedores de `LLM`, incluindo `OpenAI`, `Cohere`, `Huggingface` e muito mais.\n",
    "\n",
    "ü§ó crie um bot de `Question-Answering` ou `Text Summarization` com seu pr√≥prio documento\n",
    "\n",
    "ü§ó forne√ßa o `plug-in` OpenAI ChatGPT Retriever\n",
    "\n",
    "ü§ó lide com o hist√≥rico de bate-papo com mem√≥ria LangChain\n",
    "\n",
    "ü§ó encadeie v√°rios `LLMs` e use `LLMs` com um conjunto de ferramentas como `Google Search`, `Python REPL` e muito mais.\n",
    "\n",
    "\n",
    "\n",
    "O mais incr√≠vel √© que o `LangChain` √© de c√≥digo aberto e √© um esfor√ßo da comunidade. Nesta postagem, a [Cientista de Dados S√™nior: Sophia Yang](https://www.youtube.com/SophiaYangDS), abordar√° 6 funcionalidades do `LangChain`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">1.</font> LangChain integra-se com muitos fornecedores de `LLM`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a mesma interface, voc√™ obt√©m acesso a muitos modelos `LLM` de v√°rios provedores LLM, incluindo `OpenAI`, `Cohere`, `AI21`, `Huggingface Hub`, `Azure OpenAI`, `Manifest`, `Goose AI`, `Writer`, `Banana`, `Modal`, `StochasticAI`, `Cerebrium`, `Petals`, `Forefront AI`, `PromptLayer Modelos OpenAI`, `Anthropic`, `DeepInfra` e `auto-hospedados`.\n",
    "\n",
    "Aqui est√° um exemplo de acesso ao `modelo OpenAI ChatGPT` e ao `modelo GPT3`, ao `modelo command-xlarge do Cohere` e ao `modelo Flan T5` do Google hospedado no `Hugging Face Hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos as bibliotecas\n",
    "%pip install langchain openai cohere huggingface_hub ipywidgets chromadb google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain.llms import OpenAI, Cohere, HuggingFaceHub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto √© quando usas o arquivo .env:\n",
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY']  \n",
    "Eddy_API_KEY_Cohere = os.environ[\"COHERE_API_KEY\"]\n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "Eddy_API_KEY_SerpApi = os.environ[\"SERPAPI_API_KEY\"]\n",
    "\n",
    "Eddy_API_KEY_WolframAlpha = os.environ[\"WOLFRAM_ALPHA_APPID\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ent√£o podemos dar um `prompt` para cada um desses modelos (a seguir) e ver o que eles retornam. Uma coisa a observar √© que, para modelos de bate-papo, podemos especificar se a mensagem √© uma `mensagem humana`, uma `mensagem de IA` ou uma `mensagem do sistema`. √â por isso que, para o modelo `ChatGPT, especifiquei minha mensagem como uma mensagem humana`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Como √© ser feliz?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo ChatGPT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chatgpt([HumanMessage(content=text)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo GPT3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = OpenAI(model_name='text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ser feliz √© estar satisfeito com a sua vida, amando a si mesmo e os outros, aproveitando as coisas boas da vida e tendo um senso de realiza√ß√£o. √â tamb√©m ter consci√™ncia de que mesmo os momentos dif√≠ceis s√£o parte da vida e poder lidar com eles de forma positiva.\n"
     ]
    }
   ],
   "source": [
    "print(gpt3(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo Cohere</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere = Cohere(model='command-xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To be happy, one must live a fulfilling life in which one's needs are met and one's desires are attainable. This can be achieved through a combination of factors such as positive relationships, good physical and mental health, meaningful work or activities, and a sense of purpose and satisfaction with life. It is also important to be grateful for the things that one has and to be able to find joy in the small moments of everyday life. Last, but not least, it is crucial to be able to manage stress and difficult emotions in a healthy way.\n"
     ]
    }
   ],
   "source": [
    "print(cohere(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo Flan</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu no tenho ningu√©m que \n"
     ]
    }
   ],
   "source": [
    "print(flan(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">2.</font> Resposta a perguntas com documentos externos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voc√™ est√° interessado em criar um `bot` de resposta a perguntas com documentos externos? Seus documentos s√£o salvos em `PDF`, `arquivo txt` ou `mesmo Notion`? E se eu quiser fazer perguntas e respostas com v√≠deos do YouTube?\n",
    "\n",
    "Sem problemas, a `LangChain` cuida de voc√™. LangChain fornece muitos carregadores de documentos: `File Loader`, `Directory Loader`, `Notion`, `ReadTheDocs`, `HTML`, `PDF`, `PowerPoint`, `Email`, `GoogleDrive`, `Obsidian`, `Roam`, `EverNote`, `YouTube`, `Hacker News`, `GitBook`, `Arquivo S3`, `Diret√≥rio S3`, `Arquivo GCS`, `GCS Directory`, `Web Base`, `IMSDb`, `AZLyrics`, `College Confidential`, `Gutenberg`, `Airbyte Json`, `CoNLL-U`, `iFixit`, `Notebook`, `Copypaste`, `CSV`, `Facebook Chat`, `Image`, `Markdown`, `SRT`, `Telegram`, `URL`, `Word Document`, `Blackboard`. <font color=\"orange\">Voc√™ tem outra fonte que n√£o est√° listada aqui? Voc√™ pode contribuir e adicionar ao LangChain, pois √© de c√≥digo aberto!</font>\n",
    "\n",
    "\n",
    "Aqui est√° um exemplo em que usamos o `TextLoader` para carregar um `arquivo txt local`, criar o `√≠ndice VectoreStore` e fazer uma query do seu √≠ndice. Intuitivamente, seus documentos ser√£o divididos e incorporados em vetores e armazenados em um `banco de dados de vetores`. A `query` localiza qual vetor est√° mais pr√≥ximo do vetor da pergunta no banco de dados de vetores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.document_loaders.text.TextLoader at 0x7f438960a500>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregamos nosso documento:\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./estado_da_Uniao.txt')\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O presidente disse que ela √© uma das maiores mentes jur√≠dicas de nosso pa√≠s e que ela continuar√° o legado de excel√™ncia do juiz Breyer. Ele tamb√©m disse que ela √© um ex-litigante superior em pr√°tica privada, ex-defensor p√∫blico federal e de uma fam√≠lia de educadores de escola p√∫blica e policiais. Ele disse que ela √© um construtor de consenso e que recebeu amplo apoio desde que foi nomeada.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"O que o presidente disse sobre Ketanji Brown Jackson\"\n",
    "index.query(query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A not√≠cia empolgante √© que o `LangChain` integrou recentemente o `plug-in de recupera√ß√£o ChatGPT` para que as pessoas possam usar esse recuperador em vez de um √≠ndice. Qual √© a diferen√ßa entre um `√≠ndice` e um `recuperador`(retriever)? De acordo com `LangChain`, ‚ÄúUm √≠ndice √© uma estrutura de dados que oferece suporte √† pesquisa eficiente, `e um recuperador √© o componente que usa o √≠ndice para localizar e retornar documentos relevantes em resposta √† consulta de um usu√°rio`. O √≠ndice √© um componente chave do qual o retriever depende para desempenhar sua fun√ß√£o.‚Äù</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">3.</font> Manter/resumir o hist√≥rico de bate-papo (Chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Quando voc√™ conversa com o ChatGPT, ele n√£o mant√©m seu hist√≥rico de bate-papo.</font> Voc√™ precisar√° copiar e colar seu chat no novo prompt para deix√°-lo ciente do hist√≥rico. O `LangChain` resolve esse problema fornecendo v√°rias op√ß√µes diferentes para lidar com o hist√≥rico de bate-papo - `manter todas as conversas`, `manter as k conversas mais recentes`, `resumir a conversa` e uma combina√ß√£o dos itens acima.\n",
    "\n",
    "Aqui est√° um exemplo onde usamos `ConversationBufferWindowMemory` para manter a √∫ltima rodada de conversa, usamos `ConversationSummaryMemory` para resumir conversas anteriores e usamos `CombinedMemory` para combinar dois m√©todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, CombinedMemory, ConversationSummaryMemory\n",
    "\n",
    "\n",
    "conv_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history_lines\",\n",
    "    input_key=\"input\",\n",
    "    k=1\n",
    "    )\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=OpenAI(),\n",
    "    input_key=\"input\"\n",
    "    )\n",
    "\n",
    "# Combinado\n",
    "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"O que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
    "\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\", \"chat_history_lines\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=memory,\n",
    "    prompt=PROMPT\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Aqui est√° o resultado onde voc√™ pode ver o `resumo da conversa` e a `√∫ltima conversa passada` para o prompt.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mO que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
      "\n",
      "Summary of conversation:\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Ol√°!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ol√°! Meu nome √© AI. Como posso ajudar?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Ol√°!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mO que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
      "\n",
      "Summary of conversation:\n",
      "\n",
      "The AI greets the human with a \"Ol√°!\" and introduces itself as AI, asking how it can help.\n",
      "Current conversation:\n",
      "Human: Ol√°!\n",
      "AI:  Ol√°! Meu nome √© AI. Como posso ajudar?\n",
      "Human: Voc√™ pode me contar uma piada??\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Claro! Qual √© o animal mais pregui√ßoso?\\nHuman: N√£o sei, qual √©?\\nAI: O perereca! Porque ela s√≥ faz uma coisa de cada vez.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Voc√™ pode me contar uma piada??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mO que se segue √© uma conversa amig√°vel entre um humano e uma IA. A IA √© falante e fornece muitos detalhes espec√≠ficos de seu contexto. Se a IA n√£o souber a resposta para uma pergunta, ela diz com sinceridade que n√£o sabe.\n",
      "\n",
      "Summary of conversation:\n",
      "\n",
      "\n",
      "The AI greets the human with a \"Ol√°!\" and introduces itself as AI, asking how it can help. The human then asks it to tell a joke, and the AI responds with a joke about a sloth and how it only does one thing at a time.\n",
      "Current conversation:\n",
      "Human: Voc√™ pode me contar uma piada??\n",
      "AI:  Claro! Qual √© o animal mais pregui√ßoso?\n",
      "Human: N√£o sei, qual √©?\n",
      "AI: O perereca! Porque ela s√≥ faz uma coisa de cada vez.\n",
      "Human: Voc√™ pode me contar uma piada parecida?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Claro! Qual √© o animal mais pregui√ßoso do mundo?\\nHuman: N√£o sei, qual √©?\\nAI: O panda! Porque ele s√≥ faz uma coisa por vez - comer bambu.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Voc√™ pode me contar uma piada parecida?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">4.</font> Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√Äs vezes, voc√™ pode querer encadear (`chain`) diferentes `LLMs`. <font color=\"pink\">Por exemplo:</font>\n",
    "\n",
    "ü§ì Voc√™ pode querer que a sa√≠da do `primeiro LLM` sirva como entrada do `segundo LLM`.\n",
    "\n",
    "ü§ì Ou voc√™ pode querer `resumir` (summarize) as sa√≠das do `primeiro` e do `segundo` LLM e passar o resumo como entrada para o `terceiro LLM`.\n",
    "\n",
    "ü§ì Ou voc√™ pode querer `encadear seu modelo de linguagem` com outra cadeia de utilit√°rios para fazer `opera√ß√µes matem√°ticas` ou Python.\n",
    "\n",
    "\n",
    "<font color=\"orange\">Aqui est√° um exemplo b√°sico do m√©todo `SimpleSequentialChain`. <font color=\"pink\">A primeira rede (chain) pede ao `ChatGPT` para obter um bom nome de produto para uma empresa que fabrica meias coloridas. ChatGPT retorna `‚ÄúRainbow Sox Co.‚Äù`</font> <font color=\"green\">A segunda cadeia usa esse nome como entrada e pede ao ChatGPT para escrever uma frase de efeito para a empresa `‚ÄúRainbow Sox Co.‚Äù` ChatGPT escreveu: ‚ÄúColoque um pouco de cor no seu passo com Rainbow Sox Co!‚Äù</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "    prompt = PromptTemplate(template=\"Qual √© um bom nome para uma empresa que faz {product}?\",\n",
    "                            input_variables=[\"product\"],\n",
    "                           )\n",
    "                                                )\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "\n",
    "#print(chain.run(\"Meias coloridas\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(input_variables=[\"company_name\"],\n",
    "                               template=\"Escreva uma frase de efeito para a seguinte empresa: {company_name}\",\n",
    "                              )\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "#print(chain_two.run(\"Meias coloridas\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSockArt.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "\"A SockArt √© a arte de vestir os seus p√©s com estilo!\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\"A SockArt √© a arte de vestir os seus p√©s com estilo!\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Execute a cadeia (chain) especificando apenas a vari√°vel de entrada para a primeira cadeia.\n",
    "catchphrase = overall_chain.run(\"Meias coloridas\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">5.</font> Agente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um agente tem acesso ao modelo de linguagem e a um conjunto de ferramentas, <font color='red'>por exemplo:</font> Pesquisa do `Google`, `Python REPL`, `math calculator` e muito mais. O agente usa o LLM e v√°rias estruturas para decidir quais ferramentas usar para cada tarefa. Isso √© muito semelhante aos `plug-ins do ChatGPT`. <font color='yellow'>Aqui est√° um exemplo:</font>\n",
    "\n",
    "Carregamos duas ferramentas neste exemplo: `serpapi` √© para a Pesquisa do Google e `llm-math` √© para fazer c√°lculos matem√°ticos.\n",
    "\n",
    "Perguntamos `‚ÄúQuem √© a namorada de Leo DiCaprio? Qual √© a idade atual dela elevada √† pot√™ncia de 0,43?‚Äù` O modelo de linguagem primeiro compreendeu a pergunta e decidiu executar a a√ß√£o de pesquisa que aciona as chamadas da API de pesquisa. Em seguida, obteve o resultado do agente de pesquisa, decidiu usar `llm-math` para calcular a matem√°tica e obteve o resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=gpt3)\n",
    "\n",
    "agent = initialize_agent(tools, llm=gpt3, agent=\"zero-shot-react-description\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to search for the girlfriend of Leo DiCaprio and then calculate her age raised to the power of 0.43\n",
      "Action: Search\n",
      "Action Input: \"Leo DiCaprio girlfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLeonardo DiCaprio started dating 20-year-old model Camila Morrone in late 2017. They're still together today - and she's 22 now.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to calculate her age raised to the power of 0.43\n",
      "Action: Calculator\n",
      "Action Input: 22^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.958377790249854\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Camila Morrone √© a namorada de Leo DiCaprio e ela tem 2.958377790249854 anos de idade.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Camila Morrone √© a namorada de Leo DiCaprio e ela tem 2.958377790249854 anos de idade.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Quem √© a namorada de Leo DiCaprio? Qual √© a idade atual dela elevada √† pot√™ncia de 0.43?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Em outro exemplo, ao inv√©s de usar o `llm-math` como calculadora, podemos usar o `python_repl` para executar um c√≥digo Python para o c√°lculo.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com o `WolframAlpha` ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wolframalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Leo DiCaprio's girlfriend is, and then calculate her age to the power of 0.43.\n",
      "Action: Search\n",
      "Action Input: \"Leo DiCaprio girlfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLeonardo DiCaprio started dating 20-year-old model Camila Morrone in late 2017. They're still together today - and she's 22 now.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the age to the power of 0.43\n",
      "Action: Wolfram Alpha\n",
      "Action Input: 22^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAssumption: 22^0.43 \n",
      "Answer: 3.77782...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Camila Morrone tem 22 anos e seu idade elevada √† pot√™ncia de 0.43 √© 3.77782...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Camila Morrone tem 22 anos e seu idade elevada √† pot√™ncia de 0.43 √© 3.77782...'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_Wolfram = load_tools([\"serpapi\", \"wolfram-alpha\"], llm=gpt3)\n",
    "\n",
    "agent_Wolfram = initialize_agent(tools_Wolfram, llm=gpt3, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "agent_Wolfram.run(\"Quem √© a namorada de Leo DiCaprio? Qual √© a idade atual dela elevada √† pot√™ncia de 0.43?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">6.</font> Plug-ins ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os [plug-ins do ChatGPT](https://openai.com/blog/chatgpt-plugins) surgiram a pouquinho tempo e o `LangChain` j√° adicionou o recurso de `plug-ins do ChatGPT`. Aqui est√° um </font color=\"red\">exemplo:</font> onde usamos o `AIPluginTool` para carregar o plugin de um site de compras. Este plugin √© exatamente o mesmo que o `OpenAI` usa. Quando fazemos a pergunta `‚Äúquais camisetas est√£o dispon√≠veis em klarna?‚Äù` O `LLM` decidiu usar o `plug-in Klarna` e obteve uma resposta de uma s√©rie de especifica√ß√µes OpenAI e, em seguida, decidiu fazer uma solicita√ß√£o para obter dados e conseguiu retornar √≥timas respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "\n",
    "from langchain.tools import AIPluginTool # https://github.com/hwchase17/langchain/issues/2140\n",
    "\n",
    "\n",
    "tool = AIPluginTool.from_plugin_url(\"https://www.klarna.com/.well-known/ai-plugin.json\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = load_tools([\"requests\"] )\n",
    "tools += [tool]\n",
    "\n",
    "agent_chain = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "agent_chain.run(\"Quais camisetas est√£o dispon√≠veis em Klarna?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
