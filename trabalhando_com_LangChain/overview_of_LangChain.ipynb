{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Uma vis√£o geral do LangChain</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain, √© uma ferramenta de c√≥digo aberto e adicionou recentemente `Plugins ChatGPT`. Ele fornece tantos recursos que considero √∫teis:\n",
    "\n",
    "\n",
    "ü§ó integra√ß√£o com v√°rios fornecedores de `LLM`, incluindo `OpenAI`, `Cohere`, `Huggingface` e muito mais.\n",
    "\n",
    "ü§ó crie um bot de `Question-Answering` ou `Text Summarization` com seu pr√≥prio documento\n",
    "\n",
    "ü§ó forne√ßa o `plug-in` OpenAI ChatGPT Retriever\n",
    "\n",
    "ü§ó lide com o hist√≥rico de bate-papo com mem√≥ria LangChain\n",
    "\n",
    "ü§ó encadeie v√°rios `LLMs` e use `LLMs` com um conjunto de ferramentas como `Google Search`, `Python REPL` e muito mais.\n",
    "\n",
    "\n",
    "\n",
    "O mais incr√≠vel √© que o `LangChain` √© de c√≥digo aberto e √© um esfor√ßo da comunidade. Nesta postagem, a [Cientista de Dados S√™nior: Sophia Yang](https://www.youtube.com/SophiaYangDS), abordar√° 6 funcionalidades do `LangChain`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">1.</font> LangChain integra-se com muitos fornecedores de `LLM`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a mesma interface, voc√™ obt√©m acesso a muitos modelos `LLM` de v√°rios provedores LLM, incluindo `OpenAI`, `Cohere`, `AI21`, `Huggingface Hub`, `Azure OpenAI`, `Manifest`, `Goose AI`, `Writer`, `Banana`, `Modal`, `StochasticAI`, `Cerebrium`, `Petals`, `Forefront AI`, `PromptLayer Modelos OpenAI`, `Anthropic`, `DeepInfra` e `auto-hospedados`.\n",
    "\n",
    "Aqui est√° um exemplo de acesso ao `modelo OpenAI ChatGPT` e ao `modelo GPT3`, ao `modelo command-xlarge do Cohere` e ao `modelo Flan T5` do Google hospedado no `Hugging Face Hub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos as bibliotecas\n",
    "%pip install langchain openai cohere huggingface_hub ipywidgets chromadb google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain.llms import OpenAI, Cohere, HuggingFaceHub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando a minha chave Key:  True\n"
     ]
    }
   ],
   "source": [
    "# Isto √© quando usas o arquivo .env:\n",
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print('Carregando a minha chave Key: ', load_dotenv())\n",
    "Eddy_API_KEY_OpenAI = os.environ['OPENAI_API_KEY']  \n",
    "Eddy_API_KEY_Cohere = os.environ[\"COHERE_API_KEY\"]\n",
    "Eddy_API_KEY_HuggingFace = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "#Eddy_API_KEY_SerpApi = os.environ[\"SERPAPI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ent√£o podemos dar um `prompt` para cada um desses modelos (a seguir) e ver o que eles retornam. Uma coisa a observar √© que, para modelos de bate-papo, podemos especificar se a mensagem √© uma `mensagem humana`, uma `mensagem de IA` ou uma `mensagem do sistema`. √â por isso que, para o modelo `ChatGPT, especifiquei minha mensagem como uma mensagem humana`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Como √© ser feliz?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo ChatGPT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Como sou uma intelig√™ncia artificial, n√£o possuo a capacidade de sentir emo√ß√µes. Ser feliz pode ser descrito como ter uma sensa√ß√£o de satisfa√ß√£o, alegria, contentamento e bem-estar em rela√ß√£o √† sua vida e √†s coisas que a cercam. Isso pode ser alcan√ßado por meio de diversas experi√™ncias positivas, como relacionamentos saud√°veis, realiza√ß√£o pessoal, conquistas profissionais, atividades de lazer, entre outros. Cada pessoa pode ter sua pr√≥pria defini√ß√£o e caminho para a felicidade.' additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(chatgpt([HumanMessage(content=text)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo GPT3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = OpenAI(model_name='text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ser feliz √© um estado de esp√≠rito que vem de dentro. Para alcan√ßar esse estado, √© preciso encontrar um prop√≥sito, ter relacionamentos saud√°veis, cuidar da sa√∫de, buscar o equil√≠brio emocional e mental, al√©m de praticar atividades que trazem satisfa√ß√£o e prazer. √â importante cuidar de si mesmo, aceitando seus sentimentos e emo√ß√µes, aprendendo a express√°-los de forma adequada, e cultivando o otimismo.\n"
     ]
    }
   ],
   "source": [
    "print(gpt3(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo Cohere</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere = Cohere(model='command-xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "√â ser feliz.\n"
     ]
    }
   ],
   "source": [
    "print(cohere(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Modelo Flan</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu no tenho ningu√©m que \n"
     ]
    }
   ],
   "source": [
    "print(flan(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"yellow\">2.</font> Resposta a perguntas com documentos externos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voc√™ est√° interessado em criar um `bot` de resposta a perguntas com documentos externos? Seus documentos s√£o salvos em `PDF`, `arquivo txt` ou `mesmo Notion`? E se eu quiser fazer perguntas e respostas com v√≠deos do YouTube?\n",
    "\n",
    "Sem problemas, a `LangChain` cuida de voc√™. LangChain fornece muitos carregadores de documentos: `File Loader`, `Directory Loader`, `Notion`, `ReadTheDocs`, `HTML`, `PDF`, `PowerPoint`, `Email`, `GoogleDrive`, `Obsidian`, `Roam`, `EverNote`, `YouTube`, `Hacker News`, `GitBook`, `Arquivo S3`, `Diret√≥rio S3`, `Arquivo GCS`, `GCS Directory`, `Web Base`, `IMSDb`, `AZLyrics`, `College Confidential`, `Gutenberg`, `Airbyte Json`, `CoNLL-U`, `iFixit`, `Notebook`, `Copypaste`, `CSV`, `Facebook Chat`, `Image`, `Markdown`, `SRT`, `Telegram`, `URL`, `Word Document`, `Blackboard`. <font color=\"orange\">Voc√™ tem outra fonte que n√£o est√° listada aqui? Voc√™ pode contribuir e adicionar ao LangChain, pois √© de c√≥digo aberto!</font>\n",
    "\n",
    "\n",
    "Aqui est√° um exemplo em que usamos o `TextLoader` para carregar um `arquivo txt local`, criar o `√≠ndice VectoreStore` e fazer uma query do seu √≠ndice. Intuitivamente, seus documentos ser√£o divididos e incorporados em vetores e armazenados em um `banco de dados de vetores`. A `query` localiza qual vetor est√° mais pr√≥ximo do vetor da pergunta no banco de dados de vetores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.document_loaders.text.TextLoader at 0x7fd34e852e60>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregamos nosso documento:\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./estado_da_Uniao.txt')\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
