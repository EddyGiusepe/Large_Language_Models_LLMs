{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Sincronizando fontes de dados com armazenamentos de vetores</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links de estudo:\n",
    "\n",
    "* [Syncing data sources to vector stores](https://blog.langchain.dev/syncing-data-sources-to-vector-stores/?ref=langchain-blog-newsletter)\n",
    "\n",
    "* [LangChain Indexing API](https://python.langchain.com/docs/modules/data_connection/indexing?ref=blog.langchain.dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Contextualizando</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Os aplicativos LLM mais complexos e com uso intensivo de conhecimento exigem recuperação de dados em tempo de execução para `Retrieval Augmented Generation` (RAG). Um componente central da pilha RAG típica é um armazenamento de vetores, que é usado para potencializar a recuperação de documentos.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">O uso de um armazenamento de vetores requer a configuração de um `pipeline de indexação` para carregar dados de fontes (`um site, um arquivo, etc.`), transformar os dados em documentos, incorporar esses documentos e inserir os `Embeddings` e documentos no armazenamento de vetores.\n",
    "\n",
    "Se suas fontes de dados ou etapas de processamento mudarem, os dados precisarão ser reindexados. Se isso acontecer regularmente e as alterações forem incrementais, torna-se valioso desduplicar o conteúdo que está sendo indexado com o conteúdo já no armazenamento de vetores. Isso evita gastar tempo e dinheiro com trabalho redundante. Também se torna importante configurar processos de limpeza de armazenamento de vetores para remover dados obsoletos de seu armazenamento de vetores.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">API de indexação LangChain</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nova `API de indexação LangChain` facilita carregar e manter sincronizados (`sync`) documentos de qualquer fonte em um armazenamento de vetores. Especificamente, isso ajuda:\n",
    "\n",
    "* Evitar escrever conteúdo duplicado no armazenamento de vetores\n",
    "\n",
    "* Evitar reescrever conteúdo inalterado\n",
    "\n",
    "* Evitar recalcular `Embeddings` sobre conteúdo inalterado\n",
    "\n",
    "\n",
    "Fundamentalmente, a `API de indexação` funcionará mesmo com documentos que passaram por diversas etapas de transformação (`por exemplo`, através de chunking de texto) em relação aos documentos de origem originais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Como funciona</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `indexação LangChain` utiliza um gerenciador de registros (`RecordManager`) que monitora as gravações de documentos em um armazenamento de vetores (`Vector Store`).\n",
    "\n",
    "Ao indexar o conteúdo, os `hashes` são calculados para cada documento e as seguintes informações são armazenadas no gerenciador de registros:\n",
    "\n",
    "* o hash do documento (hash do conteúdo da página e dos metadados)\n",
    "\n",
    "* hora de escrever\n",
    "\n",
    "* o ID da fonte – cada documento deve incluir informações em seus metadados para nos permitir determinar a fonte final deste documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Modos de limpeza</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao `reindexar documentos em um armazenamento de vetores`, é possível que alguns documentos existentes no armazenamento de vetores sejam excluídos. Se você fez alterações na forma como os documentos são processados ​​antes da inserção ou os documentos de origem foram alterados, você desejará remover quaisquer documentos existentes que venham da mesma origem dos novos documentos que estão sendo indexados. Se alguns documentos de origem foram excluídos, você desejará excluir todos os documentos existentes no armazenamento de vetores e substituí-los pelos documentos reindexados.\n",
    "\n",
    "Os modos de limpeza da `API de indexação` permitem escolher o comportamento desejado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Vendo isso em ação</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">`Primeiro` vamos inicializar nosso armazenamento de vetores. Faremos uma demonstração com o `ElasticsearchStore`, pois ele atende aos pré-requisitos de suporte à `inserção` (insertion) e `exclusão` (deletion). Consulte a seção Documentos de [requisitos](https://python.langchain.com/docs/modules/data_connection/indexing?ref=blog.langchain.dev#requirements) para obter mais informações sobre os requisitos do armazenamento de vetores.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.288\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/eddygiusepe/1_Eddy_Giusepe/3_estudando_LLMs/Large_Language_Models_LLMs/venv_LLMs/lib/python3.10/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental, ragas\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install openai elasticsearch\n",
    "\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import ElasticsearchStore\n",
    "\n",
    "# collection_name = \"eddy_test_index\"\n",
    "\n",
    "# # Set env var OPENAI_API_KEY:\n",
    "# embedding = OpenAIEmbeddings()\n",
    "\n",
    "# # Execute uma instância do Elasticsearch localmente:\n",
    "# # !docker run -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.9.0\n",
    "# vector_store = ElasticsearchStore(es_url=\"http://localhost:9200\",\n",
    "#                                   index_name=collection_name,\n",
    "#                                   embedding=embedding\n",
    "#                                  )\n",
    "\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# Constrói a string de conexão PGVector a partir dos parâmetros.\n",
    "host= os.environ['DB_HOST']\n",
    "port= os.environ['DB_PORT']\n",
    "user= os.environ['DB_USER']\n",
    "password= os.environ['DB_PASSWORD']\n",
    "dbname= os.environ['DB_NAME']\n",
    "\n",
    "CONNECTION_STRING = f\"postgresql://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "COLLECTION_NAME = \"Eddy_vectordb\"\n",
    "\n",
    "\n",
    "vectorstore = PGVector.from_documents(\n",
    "    [],\n",
    "    embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">E agora vamos inicializar e criar um esquema para nosso gerenciador de registros, para o qual usaremos apenas uma tabela `SQLite`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.indexes import SQLRecordManager\n",
    "\n",
    "# namespace = f\"elasticsearch/{collection_name}\"\n",
    "\n",
    "# record_manager = SQLRecordManager(namespace, db_url=\"sqlite:///eddy_record_manager_cache.sql\")\n",
    "# record_manager.create_schema()\n",
    "\n",
    "\n",
    "from langchain.indexes import SQLRecordManager\n",
    "\n",
    "# Atualize o namespace para refletir PGVector:\n",
    "namespace = f\"pgvector/{COLLECTION_NAME}\"\n",
    "\n",
    "record_manager = SQLRecordManager(namespace,\n",
    "                                  db_url=CONNECTION_STRING\n",
    "                                 )\n",
    "\n",
    "\n",
    "# Criar schema para o Gerenciador de registros (record):\n",
    "record_manager.create_schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "\n",
    "\n",
    "doc1 = Document(page_content=\"Meu gatinho Schrödinger\", metadata={\"source\": \"kitty.txt\"})\n",
    "doc2 = Document(page_content=\"O cachorro de chama Rabito.\", metadata={\"source\": \"doggy.txt\"})\n",
    "\n",
    "\n",
    "def _clear():\n",
    "    \"\"\"Método auxiliar hacky para limpar o conteúdo. Veja a seção do modo `full` para entender por que funciona.\"\"\"\n",
    "    index([],\n",
    "          record_manager,\n",
    "          vectorstore,\n",
    "          cleanup=\"full\",\n",
    "          source_id_key=\"source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index(\n",
    "    [doc1, doc2],\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=None,\n",
    "    source_id_key=\"source\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
