{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">RAG and OpenAI’s Function-Calling for Question-Answering with Langchain</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link de estudo:\n",
    "\n",
    "* [RAG OpenAI Q&A Function Calling ang LangChain](https://dipankarmedh1.medium.com/exploring-the-power-of-rag-and-openais-function-calling-for-question-answering-d512c45c56b5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Retrieval Augmented Generation for Question-answering</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Simplificando o processo de perguntas e respostas com `RAG` e o método de chamada de função (`Function Calling`) mais recente da `OpenAI`.\n",
    "\n",
    "\n",
    "Se você está cansado de pesquisar manualmente em `documentos` ou `bancos de dados` para encontrar as informações necessárias e deseja uma maneira mais eficiente de extrair dados de fontes externas, pode estar interessado em usar o `Retrieval Augmented Generation (RAG)`.\n",
    "\n",
    "A `Geração Aumentada de Recuperação (RAG)` é uma nova abordagem para resposta a perguntas que combina os pontos fortes dos modelos de resposta a perguntas extrativos e generativos. Os modelos `RAG` <font color=\"pink\">primeiro usam um recuperador para identificar um conjunto de documentos relevantes de uma base de conhecimento.</font> <font color=\"green\">Em seguida, eles usam um gerador para criar uma nova resposta baseada nos documentos recuperados e na pergunta original.</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">RAG simplificado | O que é RAG ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `RAG`, os dados externos podem vir de qualquer lugar, como um repositório de documentos, bancos de dados ou APIs.\n",
    "\n",
    "1. O primeiro passo é converter os documentos, bem como a query do usuário, no formato para que possam ser comparados e a `pesquisa por similaridade` possa ser realizada.\n",
    "\n",
    "\n",
    "2. E para tornar os formatos comparáveis ​​para fazer a pesquisa, uma coleção de documentos (`knowledge hub`) e a query enviada pelo usuário são convertidas em representação numérica, chamada de `Embeddings`, usando modelos de linguagem de Embeddings.\n",
    "\n",
    "\n",
    "3. Os embeddings de documentos são essencialmente representações numéricas de conceitos em texto, ou podemos dizer vetores. Os embeddings podem ser armazenados em um banco de dados vetorial como `Chroma`, `Weaviate`.\n",
    "\n",
    "\n",
    "4. Em seguida, com base nos Embeddings criado a partir da query do usuário, um texto similar é identificado na coleção de documentos por uma busca por similaridade no espaço de Embeddings.\n",
    "\n",
    "\n",
    "5. Em seguida, o `prompt + entered_text` é adicionado ao contexto. Todo o prompt é agora enviado para o `LLM` e como o contexto possui dados externos relevantes junto com o prompt original, a saída do modelo é relevante e precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Q&A application using OpenAI</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos uma ideia sobre os sistemas `RAG Q&A`, podemos começar a experimentar e tentar construir um aplicativo que aproveite os conceitos `RAG` que discutimos acima. Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Extração de texto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos extraindo os dados dos documentos. Aqui vamos usar apenas documentos `PDF`.\n",
    "\n",
    "Alguns dos pacotes que podem ajudar a extrair texto de documentos são:\n",
    "\n",
    "* PyPDF2\n",
    "\n",
    "* pdf2imagem\n",
    "\n",
    "* Pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Ao aproveitar o poder da tecnologia `OCR` e de bibliotecas como `pytesseract` e `pdf2image`, esta função pode ajudar aos usuários a automatizar o processo de extração de dados de texto de documentos digitalizados.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_data_from_scanned_pdf(file_path: str) -> str:\n",
    "    data = \"\"\n",
    "    config = r'--oem 2 --psm 6'\n",
    "    images = convert_from_path(file_path)\n",
    "    for i in range(len(images)):\n",
    "        img = np.array(images[i])\n",
    "        data += pytesseract.image_to_string(img, config=config)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Splitting do text em chunks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Os dados extraídos podem ser grandes demais para serem ingeridos pelo modelo de linguagem. Portanto, o próximo passo é dividir os dados em pedaços (`chunks`) menores.\n",
    "\n",
    "\n",
    "Esta função pode ser útil para dividir uma grande sequência de dados de texto em pedaços menores e mais gerenciáveis ​​usando o objeto `“CharacterTextSplitter”` do `Langchain` e especificando o tamanho do pedaço desejado.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def get_data_chunks(data: str, chunk_size: int):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=5, separator=\"\\n\", length_function=len)\n",
    "    chunks = text_splitter.split_text(data)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Criando o knowledge hub ou Database</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Terminamos de dividir os dados em pedaços (`chunks`) menores de texto. Agora precisamos converter esses pedaços em `Embeddings` e armazená-los em um banco de dados vetorial. Podemos usar `Chroma` ou `FAISS` ou qualquer outro banco de dados.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def create_knowledge_hub(chunks: list):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    knowledge_hub = FAISS.from_texts(chunks, embeddings)\n",
    "    return knowledge_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Usando a chain de Q&A para retrieval de Informações</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Em seguida, um modelo de linguagem grande (`LLM`) é usado para fazer perguntas, recuperando a resposta relevante de um determinado dado de texto usando um pipeline de `geração aumentada de recuperação` (RAG).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def get_answer_LLM(\n",
    "        question: str,\n",
    "        data: str,\n",
    "        chunk_size: int = 1000,\n",
    "        chain_type: str = 'stuff',\n",
    "    ) -> str:    \n",
    "    if data == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    chunks = get_data_chunks(data, chunk_size=chunk_size)  # create text chunks\n",
    "    knowledge_hub = create_knowledge_hub(chunks)  # create knowledge hub\n",
    "\n",
    "\n",
    "    retriever = knowledge_hub.as_retriever(search_type=\"similarity\",\n",
    "                                           search_kwargs={\"k\": 2}\n",
    "                                          )\n",
    "    # O método 'RetrievalQA.from_chain_type' é usado para criar um pipeline de RAG. A função usa o método \n",
    "    # chain do pipeline RAG para gerar uma resposta à pergunta de entrada.\n",
    "    chain = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0.0, model_name=\"text-davinci-003\"),\n",
    "                                        chain_type=chain_type,\n",
    "                                        retriever=retriever,\n",
    "                                        return_source_documents=True,\n",
    "                                       )\n",
    "    \n",
    "    result = chain({\"query\": question})\n",
    "\n",
    "    return result['result']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Recuperação de informações usando o recurso Function Calling da OpenAI</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Respondendo algumas perguntas de documentos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escrever a função para gerar a mensagem e recuperar os argumentos como resposta do método de chamada de função (`Function Calling`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Dict, List\n",
    "import openai\n",
    "\n",
    "def message_generate(data:str) -> List[Dict[str, str]]:\n",
    "    prompt = f\"Please extract information from this given data: {data}.\" # create the prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}] \n",
    "    return messages\n",
    "\n",
    "def function_call(\n",
    "    data: str,\n",
    "    functions:List[Dict[str, any]], \n",
    "    model:str =\"gpt-3.5-turbo-0613\", \n",
    "    function_call:str=\"auto\"\n",
    ") -> Dict:\n",
    "    arguments = {}\n",
    "    message = message_generate(data) # create the message\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=message,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\"\n",
    "    ) \n",
    "    arguments = eval(response.choices[0][\"message\"][\"function_call\"][\"arguments\"]) # convert to dic\n",
    "    return arguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar a função do `OpenAI` para automatizar seu processo de `Q&A`, você precisará fornecer um argumento de mensagem com a função e o campo de conteúdo (`prompt`) como entrada. O prompt é o que direciona o modelo com as tarefas a serem realizadas e determina o tipo de informação que será recuperada.\n",
    "\n",
    "Ao usar a `função OpenAI`, é importante garantir que você forneça os argumentos necessários no formato correto. Os campos obrigatórios podem ser encontrados na documentação da função, que fornece instruções detalhadas sobre como usar a função de forma eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"pink\">Defina a função/propriedades necessárias</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora precisamos formatar as funções ou os campos a serem extraídos. Nomeando-os como funções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [  \n",
    "              {\n",
    "                  \"name\": \"get_answers\",\n",
    "                  \"description\": \"Get the answers of the questions asked\",\n",
    "                  \"parameters\": {\n",
    "                       \"type\": \"object\",\n",
    "                       \"properties\": {\n",
    "                            \"What is the date of the lease?\": {\n",
    "                                 \"type\": \"string\",\n",
    "                                 \"description\": \"Date of the lease document\",\n",
    "                             },\n",
    "                            \"What is the length of the tenancy?\":{\n",
    "                                 \"type\": \"string\",\n",
    "                                 \"description\": \"The length of the tenancy\"\n",
    "                             },\n",
    "                            \"What is the address of the property being leased?\":{\n",
    "                                 \"type\": \"string\",\n",
    "                                 \"description\": \"The adress of the property that is being leased\"\n",
    "                             },           \n",
    "                         },\n",
    "                        \"required\": [\n",
    "                          \"What is the date of the lease?\",\n",
    "                          \"What is the length of the tenancy?\",\n",
    "                          \"What is the address of the property being leased?\",\n",
    "                         ],\n",
    "                        }\n",
    "                    }\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
