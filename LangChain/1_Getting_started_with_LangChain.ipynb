{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Introdução ao LangChain — Uma ferramenta poderosa para trabalhar com modelos de linguagem grandes</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste tutorial aprenderemos a construir um aplicativo da Web usando o modelo de linguagem `OpenAI GPT3` e `SimpleSequentialChain` de `LangChain` em um front-end `Streamlit`, para este objetivo seguiremos os vídeos de Youtbe do [Avra](https://www.youtube.com/@Avra_b/playlists)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*MOR5HpojnJBJbvks_yfWBQ.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que é LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LangChain` é uma ferramenta poderosa que pode ser usada para trabalhar com Large Language Models (`LLMs`). Os LLMs são de natureza muito geral, o que significa que, embora possam executar muitas tarefas com eficácia, eles podem não ser capazes de fornecer respostas específicas a perguntas ou tarefas que exijam conhecimento ou experiência de `domínio profundo`. <font color=\"red\">Por exemplo:</font> imagine que você deseja usar um LLM para responder a perguntas sobre um campo específico, como `medicina` ou `direito`. Embora o LLM possa responder a perguntas gerais sobre o campo, pode não ser capaz de fornecer respostas mais detalhadas ou diferenciadas que exijam conhecimento especializado ou experiência.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*UTHw0sfJ6cFJYDaSE_PdlA.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Por que precisamos do LangChain?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para contornar essa limitação, o `LangChain` oferece uma abordagem útil em que o `corpus de texto` é pré-processado, dividindo-o em blocos ou resumos, incorporando-os em um `espaço vetorial` e procurando blocos semelhantes quando uma pergunta é feita. Esse padrão de pré-processamento, coleta em tempo real e interação com o `LLM` é comum e também pode ser usado em outros cenários, como código e `pesquisa semântica`. LangChain fornece uma abstração que simplifica o processo de composição dessas peças. Esse `“prompt plumbing”` (encanamento imediato) é crucial e se tornará cada vez mais importante à medida que os LLMs se tornarem mais poderosos e exigirem que mais dados sejam fornecidos no momento do `prompt`.\n",
    "\n",
    "Você pode ler mais sobre casos de uso gerais do `LangChain` em sua [documentação](https://python.langchain.com/en/latest/index.html) ou em seu [repositório GitHub](https://github.com/hwchase17/langchain). Altamente recomendado para ter uma perspectiva mais ampla sobre este pacote."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*n060CQrtGGJtxbu6ePuvLw.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atributos do LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o nome sugere, um dos atributos mais poderosos (entre muitos outros!) que o `LangChain` oferece é criar [Chains](https://python.langchain.com/en/latest/modules/chains/getting_started.html#why-do-we-need-chains). As chains (cadeias) são um recurso importante do LangChain, permitindo que os usuários combinem vários componentes para criar um aplicativo único e coerente. <font color=\"red\">Um exemplo</font> disso é a criação de uma cadeia que recebe a entrada do usuário, a formata usando um `PromptTemplate` e, em seguida, passa a resposta formatada para um LLM (`Large Language Model`) para processamento.\n",
    "\n",
    "Aqui, examinaremos principalmente os `recursos de abstração` das cadeias sequenciais. As `cadeias sequenciais` ([sequential chains](https://python.langchain.com/en/latest/modules/chains/getting_started.html#combine-chains-with-the-sequentialchain)), como o nome sugere, executam seus links em uma ordem sequencial ou passo a passo. Em outras palavras, a saída de um link é passada como entrada para o próximo link da cadeia. Isso significa que a saída do `primeiro LLM` se torna a entrada do `segundo LLM` e assim por diante, até que a saída final seja gerada. Essa abordagem permite que os usuários criem modelos mais complexos e sofisticados combinando os pontos fortes de vários LLMs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*NxvA_c44X2EM5TljQutH-g.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web App with LangChain + OpenAI + Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to install \n",
    "%pip install streamlit \n",
    "%pip install langchain\n",
    "%pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos as Bibliotecas necessárias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, começamos importando os pacotes necessários. Também importamos três classes da biblioteca `langchain`: `LLMChain`, `SimpleSequentialChain` e `PromptTemplate`. Essas classes são usadas para definir e executar nossas cadeias de modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "from langchain.chains import LLMChain, SimpleSequentialChain \n",
    "from langchain.llms import OpenAI # Importo o modelo OpenAI \n",
    "from langchain.prompts import PromptTemplate # Importo PromptTemplate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração básica do aplicativo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos o `app`, com poucas informações relevantes, usando a sintaxe simples do Streamlit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Título da nossa App\n",
    "st.title(\"✅ O que é verdade  : Usando LangChain `SimpleSequentialChain`\")\n",
    "\n",
    "# Adicione um link para o repositório Github que inspirou este aplicativo\n",
    "st.markdown(\"Inspired from [fact-checker](https://github.com/jagilley/fact-checker) by Jagiley\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widgets de entrada para interagir com o usuário front-end (API KEY, widget de pergunta, etc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O aplicativo também permite que o usuário insira sua `chave de API OpenAI`, que será usada para acessar o modelo de linguagem do `OpenAI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an API key has been provided, create an OpenAI language model instance\n",
    "if API:\n",
    "    llm = OpenAI(temperature=0.7, openai_api_key=API)\n",
    "else:\n",
    "    # If an API key hasn't been provided, display a warning message\n",
    "    st.warning(\"Enter your OPENAI API-KEY. Get your OpenAI API key from [here](https://platform.openai.com/account/api-keys).\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, precisamos fornecer um widget de entrada, que permitirá que nosso usuário insira qualquer pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a text input box for the user's question\n",
    "user_question = st.text_input(\n",
    "    \"Enter Your Question : \",\n",
    "    placeholder = \"Cyanobacteria can perform photosynthetsis , are they considered as plants?\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As CHAINS estão em funcionamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para gerar uma resposta válida para a pergunta do usuário final, passamos as perguntas por meio de alguns pipelines `SimpleSequentialChain` - assim que o botão `Tell me about it` é clicado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the final answer to the user's question using all the chains\n",
    "if st.button(\"Tell me about it\", type=\"primary\"):\n",
    "    # Chain 1: Generating a rephrased version of the user's question\n",
    "    template = \"\"\"{question}\\n\\n\"\"\"\n",
    "    prompt_template = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "    question_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Chain 2: Generating assumptions made in the statement\n",
    "    template = \"\"\"Here is a statement:\n",
    "        {statement}\n",
    "        Make a bullet point list of the assumptions you made when producing the above statement.\\n\\n\"\"\"\n",
    "    prompt_template = PromptTemplate(input_variables=[\"statement\"], template=template)\n",
    "    assumptions_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    assumptions_chain_seq = SimpleSequentialChain(\n",
    "        chains=[question_chain, assumptions_chain], verbose=True\n",
    "    )\n",
    "\n",
    "    # Chain 3: Fact checking the assumptions\n",
    "    template = \"\"\"Here is a bullet point list of assertions:\n",
    "    {assertions}\n",
    "    For each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n\"\"\"\n",
    "    prompt_template = PromptTemplate(input_variables=[\"assertions\"], template=template)\n",
    "    fact_checker_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    fact_checker_chain_seq = SimpleSequentialChain(\n",
    "        chains=[question_chain, assumptions_chain, fact_checker_chain], verbose=True\n",
    "    )\n",
    "\n",
    "    # Final Chain: Generating the final answer to the user's question based on the facts and assumptions\n",
    "    template = \"\"\"In light of the above facts, how would you answer the question '{}'\"\"\".format(\n",
    "        user_question\n",
    "    )\n",
    "    template = \"\"\"{facts}\\n\"\"\" + template\n",
    "    prompt_template = PromptTemplate(input_variables=[\"facts\"], template=template)\n",
    "    answer_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    overall_chain = SimpleSequentialChain(\n",
    "        chains=[question_chain, assumptions_chain, fact_checker_chain, answer_chain],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Running all the chains on the user's question and displaying the final answer\n",
    "    st.success(overall_chain.run(user_question))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1f61a9d800d8d7b3909976e7b91d21ea533d3244cb881687ab788269722b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
